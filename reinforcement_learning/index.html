



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://vinkrish.github.io/MachineLearning/reinforcement_learning/">
      
      
        <meta name="author" content="Vinay Krishna">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.0.2">
    
    
      
        <title>Reinforcement Learning - Intro to Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.982221ab.css">
      
      
    
    
      <script src="../assets/javascripts/modernizr.1f0bcf2b.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#reinforcement-learning" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://vinkrish.github.io/MachineLearning/" title="Intro to Machine Learning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Intro to Machine Learning
            </span>
            <span class="md-header-nav__topic">
              Reinforcement Learning
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/vinkrish/MachineLearning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    vinkrish/MachineLearning
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://vinkrish.github.io/MachineLearning/" title="Intro to Machine Learning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Intro to Machine Learning
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/vinkrish/MachineLearning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    vinkrish/MachineLearning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../terminology/" title="Terminology" class="md-nav__link">
      Terminology
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../math/" title="Math" class="md-nav__link">
      Math
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Algorithms
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Algorithms
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../logistic_regression/" title="Logistic Regression" class="md-nav__link">
      Logistic Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../neural_network/" title="Neural Network" class="md-nav__link">
      Neural Network
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../algorithms/" title="Definition" class="md-nav__link">
      Definition
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../deep_learning/" title="Deep Learning" class="md-nav__link">
      Deep Learning
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Reinforcement Learning
      </label>
    
    <a href="./" title="Reinforcement Learning" class="md-nav__link md-nav__link--active">
      Reinforcement Learning
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-setting-revisited" title="The Setting, Revisited" class="md-nav__link">
    The Setting, Revisited
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#episodic-vs-continuing-tasks" title="Episodic vs. Continuing Tasks" class="md-nav__link">
    Episodic vs. Continuing Tasks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-hypothesis" title="The Reward Hypothesis" class="md-nav__link">
    The Reward Hypothesis
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cumulative-reward" title="Cumulative Reward" class="md-nav__link">
    Cumulative Reward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discounted-return" title="Discounted Return" class="md-nav__link">
    Discounted Return
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mdps-and-one-step-dynamics" title="MDPs and One-Step Dynamics" class="md-nav__link">
    MDPs and One-Step Dynamics
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equations" title="Bellman Equations" class="md-nav__link">
    Bellman Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calculating-the-expectation" title="Calculating the Expectation" class="md-nav__link">
    Calculating the Expectation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policies" title="Policies" class="md-nav__link">
    Policies
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#state-value-functions" title="State-Value Functions" class="md-nav__link">
    State-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimality" title="Optimality" class="md-nav__link">
    Optimality
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#action-value-functions" title="Action-Value Functions" class="md-nav__link">
    Action-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policies" title="Optimal Policies" class="md-nav__link">
    Optimal Policies
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../applications/" title="Applications" class="md-nav__link">
      Applications
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../dap/" title="Data Analysis Process" class="md-nav__link">
      Data Analysis Process
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../ml_workflow/" title="Workflow with example" class="md-nav__link">
      Workflow with example
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../libraries/" title="Libraries" class="md-nav__link">
      Libraries
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../tensorflow/" title="TensorFlow" class="md-nav__link">
      TensorFlow
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../source/" title="Source" class="md-nav__link">
      Source
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-setting-revisited" title="The Setting, Revisited" class="md-nav__link">
    The Setting, Revisited
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#episodic-vs-continuing-tasks" title="Episodic vs. Continuing Tasks" class="md-nav__link">
    Episodic vs. Continuing Tasks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-hypothesis" title="The Reward Hypothesis" class="md-nav__link">
    The Reward Hypothesis
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cumulative-reward" title="Cumulative Reward" class="md-nav__link">
    Cumulative Reward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discounted-return" title="Discounted Return" class="md-nav__link">
    Discounted Return
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mdps-and-one-step-dynamics" title="MDPs and One-Step Dynamics" class="md-nav__link">
    MDPs and One-Step Dynamics
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equations" title="Bellman Equations" class="md-nav__link">
    Bellman Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calculating-the-expectation" title="Calculating the Expectation" class="md-nav__link">
    Calculating the Expectation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policies" title="Policies" class="md-nav__link">
    Policies
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#state-value-functions" title="State-Value Functions" class="md-nav__link">
    State-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimality" title="Optimality" class="md-nav__link">
    Optimality
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#action-value-functions" title="Action-Value Functions" class="md-nav__link">
    Action-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policies" title="Optimal Policies" class="md-nav__link">
    Optimal Policies
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/vinkrish/MachineLearning/edit/master/docs/reinforcement_learning.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="reinforcement-learning">Reinforcement Learning</h1>
<p><img alt="reinforce_learning" src="https://vinkrish-notes.s3-us-west-2.amazonaws.com/img/reinforce_learning.png" /></p>
<h2 id="the-setting-revisited">The Setting, Revisited</h2>
<ul>
<li>The reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment.</li>
<li>At each time step, the agent receives the environment's state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state.</li>
<li>All agents have the goal to maximize expected cumulative reward, or the expected sum of rewards attained over all time steps.</li>
</ul>
<h2 id="episodic-vs-continuing-tasks">Episodic vs. Continuing Tasks</h2>
<ul>
<li>A task is an instance of the reinforcement learning (RL) problem.</li>
<li>Continuing tasks are tasks that continue forever, without end.</li>
<li>Episodic tasks are tasks with a well-defined starting and ending point.<ul>
<li>In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.</li>
<li>Episodic tasks come to an end whenever the agent reaches a terminal state.</li>
</ul>
</li>
</ul>
<h2 id="the-reward-hypothesis">The Reward Hypothesis</h2>
<ul>
<li>Reward Hypothesis: All goals can be framed as the maximization of (expected) cumulative reward.</li>
</ul>
<h2 id="cumulative-reward">Cumulative Reward</h2>
<p>The return at time step t is $$G_t := R_{t+1} + R_{t+2} + R_{t+3} + \ldots$$
The agent selects actions with the goal of maximizing expected (discounted) return.</p>
<h2 id="discounted-return">Discounted Return</h2>
<ul>
<li>The discounted return at time step t is $$G_t := R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$</li>
<li>The discount rate γ is something that you set, to refine the goal that you have the agent.<ul>
<li>It must satisfy 0 &lt;=  γ &lt;= 1</li>
<li>If γ=0, the agent only cares about the most immediate reward.</li>
<li>If γ=1, the return is not discounted.</li>
<li>For larger values of γ, the agent cares more about the distant future. Smaller values of γ result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.</li>
</ul>
</li>
</ul>
<h2 id="mdps-and-one-step-dynamics">MDPs and One-Step Dynamics</h2>
<ul>
<li>The state space <strong>S</strong> is the set of all (nonterminal) states.</li>
<li>In episodic tasks, we use \mathcal{S}^+ to refer to the set of all states, including terminal states.</li>
<li>The action space <strong>A</strong> is the set of possible actions. (Alternatively, <strong>A(s)</strong> refers to the set of possible actions available in state <strong>s in S</strong>)</li>
<li>The one-step dynamics of the environment determine how the environment decides the state and reward at every time step. The dynamics can be defined by specifying</li>
</ul>
<p>$$p(s',r|s,a) \doteq \mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a) \text{ for each possible } s', r, s, \text{and } a$$</p>
<ul>
<li>A (finite) Markov Decision Process (MDP) is defined by:<ul>
<li>a (finite) set of states <strong>S</strong> (or S+, in the case of an episodic task)</li>
<li>a (finite) set of actions <strong>A</strong></li>
<li>a set of rewards <strong>R</strong></li>
<li>the one-step dynamics of the environment</li>
<li>the discount rate \gamma \in [0,1]</li>
</ul>
</li>
</ul>
<h2 id="bellman-equations">Bellman Equations</h2>
<p>In this gridworld example, once the agent selects an action,</p>
<ul>
<li>it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and</li>
<li>the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution).</li>
</ul>
<p>In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state.</p>
<p>Alexis mentioned that for a general MDP, we have to instead work in terms of an expectation, since it's not often the case that the immediate reward and next state can be predicted with certainty. Indeed, we saw in an earlier lesson that the reward and next state are chosen according to the one-step dynamics of the MDP. In this case, where the reward <em>r</em> and next state <em>s′</em> are drawn from a (conditional) probability distribution <em>p(s',r|s,a)</em>, the <strong>Bellman Expectation Equation</strong> (for $ v_\piv 
π $) expresses the value of any state <em>s</em> in terms of the expected immediate reward and the expected value of the next state:</p>
<p>$$v_\pi(s) = \mathbb{E}<em>\pi[R</em>{t+1} + \gamma v_\pi(S_{t+1})|S_t =s]$$</p>
<h2 id="calculating-the-expectation">Calculating the Expectation</h2>
<p>In the event that the agent's policy π is <strong>deterministic</strong>, the agent selects action <em>π(s)</em> when in state <em>s</em>, and the Bellman Expectation Equation can be rewritten as the sum over two variables (<em>s′</em> and <em>r</em>):</p>
<p>$$v_\pi(s) = \text{} \sum_{s'\in\mathcal{S}^+, r\in\mathcal{R}}p(s',r|s,\pi(s))(r+\gamma v_\pi(s'))$$</p>
<p>In this case, we multiply the sum of the reward and discounted value of the next state $(r+\gamma v_\pi(s'))$ by its corresponding probability $p(s',r|s,\pi(s))$ and sum over all possibilities to yield the expected value.</p>
<p>If the agent's policy π is <strong>stochastic</strong>, the agent selects action <em>a</em> with probability <em>π(a∣s)</em> when in state <em>s</em>, and the Bellman Expectation Equation can be rewritten as the sum over three variables <em>(s′, r, and a)</em>:</p>
<p>$$v_\pi(s) = \text{} \sum_{s'\in\mathcal{S}^+, r\in\mathcal{R},a\in\mathcal{A}(s)}\pi(a|s)p(s',r|s,a)(r+\gamma v_\pi(s'))$$</p>
<p>In this case, we multiply the sum of the reward and discounted value of the next state $(r+\gamma v_\pi(s'))$ by its corresponding probability <em>π(a∣s)p(s′,r∣s,a)</em> and sum over all possibilities to yield the expected value.</p>
<h2 id="policies">Policies</h2>
<ul>
<li>A <strong>deterministic</strong> policy is a mapping</li>
</ul>
<p>$$\pi: \mathcal{S}\to\mathcal{A}$$</p>
<p>For each state s ∈ S, it yields the action a ∈ A that the agent will choose while in state <em>s</em>.</p>
<ul>
<li>A <strong>stochastic policy</strong> is a mapping</li>
</ul>
<p>$$\pi: \mathcal{S}\times\mathcal{A}\to [0,1]$$</p>
<p>For each state s ∈ S and action a ∈ A, it yields the probability π(a∣s) that the agent chooses action <em>a__ while in state _s</em>.</p>
<h2 id="state-value-functions">State-Value Functions</h2>
<ul>
<li>The state-value function for a policy π is denoted vπ. For each state s ∈ S, it yields the expected return if the agent starts in state <em>s</em> and then uses the policy to choose its actions for all time steps. That is,</li>
</ul>
<p>$$v_\pi(s) \doteq \text{} \mathbb{E}_\pi[G_t | S_t=s]$$</p>
<p>We refer to vπ(s) as the value of state ss under policy π.</p>
<ul>
<li>The notation Eπ[⋅] is borrowed from the suggested textbook, where it is defined as the expected value of a random variable, given that the agent follows policy π.</li>
</ul>
<h2 id="optimality">Optimality</h2>
<ul>
<li>A policy π′ is defined to be better than or equal to a policy π if and only if $$v_{\pi'}(s) \geq v_\pi(s)$$ for all s ∈ S.</li>
<li>An optimal policy π∗ satisfies π∗ ≥ π for all policies π. An optimal policy is guaranteed to exist but may not be unique.</li>
<li>All optimal policies have the same state-value function v∗ called the optimal state-value function.</li>
</ul>
<h2 id="action-value-functions">Action-Value Functions</h2>
<ul>
<li>The action-value function for a policy π is denoted qπ. For each state s ∈ S and action a ∈ A, it yields the expected return if the agent starts in state <em>s</em>, takes action <em>a</em>, and then follows the policy for all future time steps. That is, </li>
</ul>
<p>$$q_\pi(s,a) \doteq \mathbb{E}_\pi[G_t|S_t=s, A_t=a]$$</p>
<p>We refer qπ(s,a) as the value of taking action <em>a</em> in state <em>s</em> under a policy π (or alternatively as the value of the state-action pair s,a).
- All optimal policies have the same action-value function q∗, called the optimal action-value function.</p>
<h2 id="optimal-policies">Optimal Policies</h2>
<ul>
<li>Once the agent determines the optimal action-value function q∗, it can quickly obtain an optimal policy π∗ by setting </li>
</ul>
<p>$$\pi_<em>(s) = \arg\max_{a\in\mathcal{A}(s)} q_</em>(s,a)$$</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../deep_learning/" title="Deep Learning" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Deep Learning
              </span>
            </div>
          </a>
        
        
          <a href="../applications/" title="Applications" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Applications
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/vinkrish" class="md-footer-social__link fa fa-github"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.d9aa80ab.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>