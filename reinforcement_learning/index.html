



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://vinkrish.github.io/MachineLearning/reinforcement_learning/">
      
      
        <meta name="author" content="Vinay Krishna">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.0.2">
    
    
      
        <title>Reinforcement Learning - Intro to Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.982221ab.css">
      
      
    
    
      <script src="../assets/javascripts/modernizr.1f0bcf2b.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#reinforcement-learning" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://vinkrish.github.io/MachineLearning/" title="Intro to Machine Learning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Intro to Machine Learning
            </span>
            <span class="md-header-nav__topic">
              Reinforcement Learning
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/vinkrish/MachineLearning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    vinkrish/MachineLearning
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://vinkrish.github.io/MachineLearning/" title="Intro to Machine Learning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Intro to Machine Learning
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/vinkrish/MachineLearning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    vinkrish/MachineLearning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../math/" title="Math" class="md-nav__link">
      Math
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../libraries/" title="Libraries" class="md-nav__link">
      Libraries
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../ml_workflow/" title="Workflow" class="md-nav__link">
      Workflow
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Algorithms
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Algorithms
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../logistic_regression/" title="Logistic Regression" class="md-nav__link">
      Logistic Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../neural_network/" title="Neural Network" class="md-nav__link">
      Neural Network
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../algorithms/" title="Definition" class="md-nav__link">
      Definition
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6" checked>
    
    <label class="md-nav__link" for="nav-6">
      Deep Learning
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        Deep Learning
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../deep_learning/" title="Intro" class="md-nav__link">
      Intro
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Reinforcement Learning
      </label>
    
    <a href="./" title="Reinforcement Learning" class="md-nav__link md-nav__link--active">
      Reinforcement Learning
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#episodic-vs-continuing-tasks" title="Episodic vs. Continuing Tasks" class="md-nav__link">
    Episodic vs. Continuing Tasks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-hypothesis" title="The Reward Hypothesis" class="md-nav__link">
    The Reward Hypothesis
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cumulative-reward" title="Cumulative Reward" class="md-nav__link">
    Cumulative Reward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discounted-return" title="Discounted Return" class="md-nav__link">
    Discounted Return
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mdps-and-one-step-dynamics" title="MDPs and One-Step Dynamics" class="md-nav__link">
    MDPs and One-Step Dynamics
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equations" title="Bellman Equations" class="md-nav__link">
    Bellman Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calculating-the-expectation" title="Calculating the Expectation" class="md-nav__link">
    Calculating the Expectation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policies" title="Policies" class="md-nav__link">
    Policies
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#state-value-functions" title="State-Value Functions" class="md-nav__link">
    State-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimality" title="Optimality" class="md-nav__link">
    Optimality
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#action-value-functions" title="Action-Value Functions" class="md-nav__link">
    Action-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policies" title="Optimal Policies" class="md-nav__link">
    Optimal Policies
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../dynamic_programming/" title="Dynamic Programming" class="md-nav__link">
      Dynamic Programming
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../monte_carlo_methods/" title="Monte Carlo Methods" class="md-nav__link">
      Monte Carlo Methods
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../rl_summary/" title="Summary" class="md-nav__link">
      Summary
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../applications/" title="Applications" class="md-nav__link">
      Applications
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7">
    
    <label class="md-nav__link" for="nav-7">
      Data Science
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        Data Science
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../data_science/" title="Intro" class="md-nav__link">
      Intro
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../dap/" title="Data Analysis Process" class="md-nav__link">
      Data Analysis Process
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../big_data/" title="Big Data" class="md-nav__link">
      Big Data
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../tensorflow/" title="TensorFlow" class="md-nav__link">
      TensorFlow
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../terminology/" title="Terminology" class="md-nav__link">
      Terminology
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../source/" title="Source" class="md-nav__link">
      Source
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#episodic-vs-continuing-tasks" title="Episodic vs. Continuing Tasks" class="md-nav__link">
    Episodic vs. Continuing Tasks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-hypothesis" title="The Reward Hypothesis" class="md-nav__link">
    The Reward Hypothesis
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cumulative-reward" title="Cumulative Reward" class="md-nav__link">
    Cumulative Reward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discounted-return" title="Discounted Return" class="md-nav__link">
    Discounted Return
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mdps-and-one-step-dynamics" title="MDPs and One-Step Dynamics" class="md-nav__link">
    MDPs and One-Step Dynamics
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equations" title="Bellman Equations" class="md-nav__link">
    Bellman Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calculating-the-expectation" title="Calculating the Expectation" class="md-nav__link">
    Calculating the Expectation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policies" title="Policies" class="md-nav__link">
    Policies
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#state-value-functions" title="State-Value Functions" class="md-nav__link">
    State-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimality" title="Optimality" class="md-nav__link">
    Optimality
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#action-value-functions" title="Action-Value Functions" class="md-nav__link">
    Action-Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policies" title="Optimal Policies" class="md-nav__link">
    Optimal Policies
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/vinkrish/MachineLearning/edit/master/docs/reinforcement_learning.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="reinforcement-learning">Reinforcement Learning</h1>
<p>Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.</p>
<ul>
<li>A <strong>policy</strong> defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.</li>
<li>A reward signal defines the goal of a reinforcement learning problem. On each time step, the environment sends a single number called the <strong>reward</strong> to the reinforcement learning agent.</li>
<li>The reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the <strong>value</strong> of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.</li>
<li><strong>Model</strong> mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave.</li>
</ul>
<p>Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners.</p>
<p><em>Reinforcement learning</em> uses the formal framework of Markov decision processes to define the interaction between a learning agent and its environment in terms of states, actions, and rewards. We will learn problem formulation with finite <em>Markov Decision Processes</em> and its main ideas including Bellman equations and value functions.</p>
<p>Three fundamental classes of methods for solving finite Markov decision problems:</p>
<ul>
<li>Dynamic programming</li>
<li>Monte Carlo methods</li>
<li>Temporal difference learning</li>
</ul>
<p>The methods also differ in several ways with respect to their efficiency and speed of convergence.</p>
<p><strong>Dynamic programming</strong> methods are well developed mathematically, but require a complete and accurate model of the environment.</p>
<p><strong>Monte Carlo</strong> methods don’t require a model and are conceptually simple, but are not well suited for step-by-step incremental computation.</p>
<p><strong>Temporal-difference</strong> methods require no model and are fully incremental, but are more complex to analyze.</p>
<p>If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem.</p>
<p><img alt="reinforce_learning" src="https://vinkrish-notes.s3-us-west-2.amazonaws.com/img/reinforce_learning.png" /></p>
<ul>
<li>The learner and decision maker is called the <strong>agent</strong>.</li>
<li>The thing it interacts with, comprising everything outside the agent, is called the <strong>environment</strong>.</li>
<li>Thus RL framework is characterized by an <em>agent</em> learning to interact with its <em>environment</em>.</li>
<li>At each time step, the agent receives the environment's <strong>state</strong> (the environment presents a situation to the agent), and the agent must choose an appropriate <strong>action</strong> in response. One time step later, the agent receives a <strong>reward</strong> (the environment indicates whether the agent has responded appropriately to the state) and a new state.</li>
<li>All agents have the goal to maximize expected <strong>cumulative reward</strong>, or the expected sum of rewards attained over all time steps.</li>
</ul>
<p>The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the <strong>Markov property</strong>.</p>
<h2 id="episodic-vs-continuing-tasks">Episodic vs. Continuing Tasks</h2>
<ul>
<li>A <strong>task</strong> is an instance of the reinforcement learning (RL) problem.</li>
<li><strong>Continuing tasks</strong> are tasks that continue forever, without end.</li>
<li><strong>Episodic tasks</strong> are tasks with a well-defined starting and ending point.<ul>
<li>In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.</li>
<li>Episodic tasks come to an end whenever the agent reaches a terminal state.</li>
</ul>
</li>
</ul>
<h2 id="the-reward-hypothesis">The Reward Hypothesis</h2>
<ul>
<li><strong>Reward Hypothesis</strong>: All goals can be framed as the maximization of (expected) cumulative reward.</li>
</ul>
<h2 id="cumulative-reward">Cumulative Reward</h2>
<p>The return at time step t is <script type="math/tex; mode=display">G_t := R_{t+1} + R_{t+2} + R_{t+3} + \ldots</script>
The agent selects actions with the goal of maximizing expected (discounted) return.</p>
<h2 id="discounted-return">Discounted Return</h2>
<ul>
<li>Discounting: The agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized.</li>
<li>The discounted return at time step t is <script type="math/tex; mode=display">G_t := R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots</script>
<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots)</script>
<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma G_{t+1}</script>
<script type="math/tex; mode=display">G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}</script>
</li>
<li>The discount rate <script type="math/tex">\gamma</script> is something that you set, to refine the goal that you have the agent.<ul>
<li>It must satisfy 0 &lt;= <script type="math/tex">\gamma</script> &lt;= 1</li>
<li>If <script type="math/tex">\gamma=0</script>, the agent only cares about the most immediate reward.</li>
<li>If <script type="math/tex">\gamma=1</script>, the return is not discounted.</li>
<li>For larger values of <script type="math/tex">\gamma</script>, the agent cares more about the distant future. Smaller values of <script type="math/tex">\gamma</script> result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.</li>
</ul>
</li>
</ul>
<h2 id="mdps-and-one-step-dynamics">MDPs and One-Step Dynamics</h2>
<ul>
<li>The <strong>state space</strong> <script type="math/tex">\mathcal{S}</script> is the set of all (nonterminal) states.</li>
<li>In episodic tasks, we use <script type="math/tex">\mathcal{S}^+</script> to refer to the set of all states, including terminal states.</li>
<li>The <strong>action space</strong> <script type="math/tex">\mathcal{A}</script> is the set of possible actions. (Alternatively, <script type="math/tex">\mathcal{A(s)}</script> refers to the set of possible actions available in state <script type="math/tex">s \in (\mathcal{S}</script>
</li>
<li>The <strong>one-step dynamics</strong> of the environment determine how the environment decides the state and reward at every time step. The dynamics can be defined by specifying</li>
</ul>
<p>
<script type="math/tex; mode=display">p(s',r|s,a) \doteq \mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a) \text{ for each possible } s', r, s, \text{and } a</script>
</p>
<ul>
<li>A <strong>(finite) Markov Decision Process (MDP)</strong> is defined by:<ul>
<li>a (finite) set of states <script type="math/tex">\mathcal{S}</script> (or <script type="math/tex">\mathcal{S}^+</script>, in the case of an episodic task)</li>
<li>a (finite) set of actions <script type="math/tex">\mathcal{A}</script>
</li>
<li>a set of rewards <script type="math/tex">\mathcal{R}</script>
</li>
<li>the one-step dynamics of the environment</li>
<li>the discount rate <script type="math/tex">\gamma \in [0,1]</script>
</li>
</ul>
</li>
</ul>
<h2 id="bellman-equations">Bellman Equations</h2>
<p>In this gridworld example, once the agent selects an action,</p>
<ul>
<li>it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and</li>
<li>the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution).</li>
</ul>
<p>In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state.</p>
<p>Alexis mentioned that for a general MDP, we have to instead work in terms of an expectation, since it's not often the case that the immediate reward and next state can be predicted with certainty. Indeed, we saw in an earlier lesson that the reward and next state are chosen according to the one-step dynamics of the MDP. In this case, where the reward <em>r</em> and next state <em>s′</em> are drawn from a (conditional) probability distribution <script type="math/tex">p(s',r|s,a)</script>, the <strong>Bellman Expectation Equation</strong> (for <script type="math/tex">v_\pi)</script> expresses the value of any state <em>s</em> in terms of the expected immediate reward and the expected value of the next state:</p>
<p>
<script type="math/tex; mode=display">v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t =s]</script>
</p>
<h2 id="calculating-the-expectation">Calculating the Expectation</h2>
<p>In the event that the agent's policy π is <strong>deterministic</strong>, the agent selects action <script type="math/tex">\pi(s)</script> when in state <script type="math/tex">s</script>, and the Bellman Expectation Equation can be rewritten as the sum over two variables (<script type="math/tex">s′ \text{ and } r</script>):</p>
<p>
<script type="math/tex; mode=display">v_\pi(s) = \text{} \sum_{s'\in\mathcal{S}^+, r\in\mathcal{R}}p(s',r|s,\pi(s))(r+\gamma v_\pi(s'))</script>
</p>
<p>In this case, we multiply the sum of the reward and discounted value of the next state <script type="math/tex">(r+\gamma v_\pi(s'))</script> by its corresponding probability <script type="math/tex">p(s',r|s,\pi(s))</script> and sum over all possibilities to yield the expected value.</p>
<p>If the agent's policy <script type="math/tex">\pi</script> is <strong>stochastic</strong>, the agent selects action <script type="math/tex">a</script> with probability <script type="math/tex">\pi(a∣s)</script> when in state <script type="math/tex">a</script>, and the Bellman Expectation Equation can be rewritten as the sum over three variables <script type="math/tex">(s′, r, a)</script>:</p>
<p>
<script type="math/tex; mode=display">v_\pi(s) = \text{} \sum_{s'\in\mathcal{S}^+, r\in\mathcal{R},a\in\mathcal{A}(s)}\pi(a|s)p(s',r|s,a)(r+\gamma v_\pi(s'))</script>
</p>
<p>In this case, we multiply the sum of the reward and discounted value of the next state <script type="math/tex">(r+\gamma v_\pi(s'))</script> by its corresponding probability <script type="math/tex">π(a∣s)p(s′,r∣s,a)</script> and sum over all possibilities to yield the expected value.</p>
<h2 id="policies">Policies</h2>
<ul>
<li>
<p>A <strong>deterministic</strong> policy is a mapping <script type="math/tex">\pi: \mathcal{S}\to\mathcal{A}</script>. For each state <script type="math/tex">s \in S</script>, it yields the action (a \in A) that the agent will choose while in state <script type="math/tex">s</script>.</p>
</li>
<li>
<p>A <strong>stochastic</strong> policy is a mapping <script type="math/tex">\pi: \mathcal{S}\times\mathcal{A}\to [0,1]</script>. For each state <script type="math/tex">s \in S</script> and action <script type="math/tex">a \in A</script>, it yields the probability <script type="math/tex">\pi(a|s)</script> that the agent chooses action <script type="math/tex">a</script> while in state <script type="math/tex">s</script>.</p>
</li>
</ul>
<h2 id="state-value-functions">State-Value Functions</h2>
<ul>
<li>The state-value function for a policy <script type="math/tex">\pi</script> is denoted <script type="math/tex">v_\pi</script>. For each state <script type="math/tex">s \in S</script>, it yields the expected return if the agent starts in state <script type="math/tex">s</script> and then uses the policy to choose its actions for all time steps. That is,</li>
</ul>
<p>
<script type="math/tex; mode=display">v_\pi(s) \doteq \mathbb{E}_\pi[G_t | S_t=s]</script>
We refer to <script type="math/tex">v_\pi(s)</script> as the value of state <script type="math/tex">s</script> under policy <script type="math/tex">\pi</script>.</p>
<ul>
<li>The notation <script type="math/tex">\mathbb{E}_\pi[\cdot]</script> denotes the expected value of a random variable given that the agent follows
policy <script type="math/tex">\pi</script>, and <script type="math/tex">t</script> is any time step.</li>
</ul>
<h2 id="optimality">Optimality</h2>
<ul>
<li>
<p>A policy <script type="math/tex">\pi′</script> is defined to be better than or equal to a policy <script type="math/tex">\pi</script> if and only if <script type="math/tex">v_{\pi'}(s) \geq v_\pi(s)</script> for all <script type="math/tex">s \in S</script>.</p>
</li>
<li>
<p>An <strong>optimal policy</strong> <script type="math/tex">\pi_*</script> satisfies <script type="math/tex">\pi_*\ge\pi</script> for all policies π. An optimal policy is guaranteed to exist but may not be unique.</p>
</li>
<li>All optimal policies have the same state-value function <script type="math/tex">v_*</script> called the <strong>optimal state-value function</strong>.</li>
</ul>
<h2 id="action-value-functions">Action-Value Functions</h2>
<ul>
<li>The <strong>action-value function</strong> for a policy <script type="math/tex">\pi</script> is denoted <script type="math/tex">q_\pi</script>. For each state <script type="math/tex">s \in S</script> and action <script type="math/tex">a \in A</script>, it yields the expected return if the agent starts in state <script type="math/tex">s</script>, takes action <script type="math/tex">a</script>, and then follows the policy for all future time steps. That is,</li>
</ul>
<p>
<script type="math/tex; mode=display">q_\pi(s,a) \doteq \mathbb{E}_\pi[G_t|S_t=s, A_t=a]</script>
</p>
<p>We refer <script type="math/tex">q_\pi(s,a)</script> as the value of taking action <script type="math/tex">a</script> in state <script type="math/tex">s</script> under a policy <script type="math/tex">\pi</script> (or alternatively as the value of the state-action pair <em>s</em>,<em>a</em>).</p>
<ul>
<li>All optimal policies have the same action-value function <script type="math/tex">q_*</script>, called the <strong>optimal action-value function</strong>.</li>
</ul>
<h2 id="optimal-policies">Optimal Policies</h2>
<ul>
<li>Once the agent determines the optimal action-value function <script type="math/tex">q_*</script>, it can quickly obtain an optimal policy <script type="math/tex">\pi_*</script> by setting:</li>
</ul>
<p>
<script type="math/tex; mode=display">\pi_*(s) = \arg\max_{a\in\mathcal{A}(s)} q_*(s,a)</script>
</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../deep_learning/" title="Intro" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Intro
              </span>
            </div>
          </a>
        
        
          <a href="../dynamic_programming/" title="Dynamic Programming" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Dynamic Programming
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/vinkrish" class="md-footer-social__link fa fa-github"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.d9aa80ab.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>