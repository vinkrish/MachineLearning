{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Machine Learning? Arthur Samuel described it as: \"The field of study that gives computers the ability to learn without being explicitly programmed.\" This is an older, informal definition. Tom Mitchell provides a more modern definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" Example : playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning Unsupervised learning Supervised Learning In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into regression and classification problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Example 1 : Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2 : (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. Unsupervised Learning Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results. Example : Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).","title":"About"},{"location":"#what-is-machine-learning","text":"Arthur Samuel described it as: \"The field of study that gives computers the ability to learn without being explicitly programmed.\" This is an older, informal definition. Tom Mitchell provides a more modern definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" Example : playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning Unsupervised learning","title":"What is Machine Learning?"},{"location":"#supervised-learning","text":"In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into regression and classification problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Example 1 : Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2 : (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.","title":"Supervised Learning"},{"location":"#unsupervised-learning","text":"Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results. Example : Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).","title":"Unsupervised Learning"},{"location":"dap/","text":"This process will help you understand, explore and use your data intelligently so that you make the most of the information you're given. Five steps: Question Wrangle Explore Draw conclusions Communicate. Question The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights. Wrangle Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with. Explore Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data. Conclusions After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately. Communicate Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations Packages Overview Numpy let's you perform mathematical functions on large multi dimensional arrays and matrices efficiently. Pandas is like a more powerful and flexible version of Excel that can handle large amounts of data. Matplotlib is a plodding library that can produce great visualizations often with very few lines of code. Scikit-learn is designed to work with NumPy, SciPy and Pandas, provides toolset for training and evaluation tasks: Data splitting Pre-processing Feature selection Model training Model tuning and offers common interface across algorithms","title":"Data Analysis Process"},{"location":"dap/#question","text":"The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights.","title":"Question"},{"location":"dap/#wrangle","text":"Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with.","title":"Wrangle"},{"location":"dap/#explore","text":"Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data.","title":"Explore"},{"location":"dap/#conclusions","text":"After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately.","title":"Conclusions"},{"location":"dap/#communicate","text":"Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations","title":"Communicate"},{"location":"dap/#packages-overview","text":"Numpy let's you perform mathematical functions on large multi dimensional arrays and matrices efficiently. Pandas is like a more powerful and flexible version of Excel that can handle large amounts of data. Matplotlib is a plodding library that can produce great visualizations often with very few lines of code. Scikit-learn is designed to work with NumPy, SciPy and Pandas, provides toolset for training and evaluation tasks: Data splitting Pre-processing Feature selection Model training Model tuning and offers common interface across algorithms","title":"Packages Overview"},{"location":"ml_workflow/","text":"An orchestrated and repeatable pattern which systematically transforms and processes information to create prediction solutions. Asking the right question Preparing data Selecting the algorithm Training the model Testing the model Solution Statement Use the Machine Learning Workflow to process and transform Pima Indian data to create a prediction model. This model must predict which peopel are likely to develop diabetes with 70% or greater accuracy Tidy Data Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column each observation is a row each type of observational unit is a table Selecting the algorithm We will use our problem knowledge to help us decide the algorithm to use. we will discuss - The role of the algorithm in machine learning process, select our initial algorithm by utilizing the requirements identified in the solution statement as a guide, and discuss at a high level the characteristics of some specific algorithms. Finally select one algorithm to be our initial algorithm, in machine learning we often cycle through the workflow. In our search to find the best solution, it is likely we will need to train and evaluate multiple algorithms. Let's review how the algorithm is involved in the process. When the training function (often named fit in scikit-learn) is called, the algorithm executes its logic and processes the training data. Using the algorithm's logic, the data in analyzed. This analysis evaluates the data with respect to mathematical model and logic associated with the algorithm. The algorithm uses the results of this analysis to adjust internal parameters to produce a model that has been trained to best fit the features in the training data and produce the associated class result. This best fit is defined by evaluating a function specific to the particular algorithm. The fit parameters are stored and the model is now said to be trained. Later, the trained model is called the prediction function (often named predict in scikit-learn). When this prediction function is called, real data is passed to the trained model. Using only the features in the data, the trained model uses its code and parameter values set during training to evaluate the data's features and predict the class result, diabetes or not for this new data. Decision factors to select our initial algorithm: We will use our solution statement and knowledge of the workflow to help guide us in the evaluation of these factors. what Learning Type they support the Result Type the algorithm predicts the Complexity of the algorithm whether the algorithm is Basic or Enhanced Each algorithm has a set of problems with which it works best. One way to divide them is to look at the type of Learning they support. Reading the statement, we see that our solution is about prediction. Prediction means supervised machine learning, so we can eliminate all algorithms that do not support it. Let's see how Result Type can help. Prediction results can be divided into two categories: Regression (Continuous values) Classification (Discrete values) Based on the Statment, the algorithm must support Binary classification. Since this is our initial algorithm, let's stick to the basic algorithms. Selecting Our Initial Algorithm Candidate Algorithms: Naive Bayes Logistic Regression Decision Tree More complex algoritms use these as building blocks. Naive Bayes Algorithm The Naive Bayes algorithm is based on Bayes' Theorem. This theorem calculates a probability of a diabetes by looking at the likelihood of diabetes based on previous data combined with probability of diabetes on nearby feature values. In other words, so how often does the person having high blood pressure correlate to diabetes? It makes the naive assumption that all of the features we pass in are independent of each other and equally impact the result. This assumption that every featuer is independent to the others allows for fast conversions and therefore requires a small amount of data to train. Logistic Regression Algorithm The Logistic Regression algorithm has a somewhat confusing name. In Statistics, Regression often implies continuous values. But Logistics Regression returns a binary result. The algorithm measures the relationship of each feature and weights them based on their impact on the result. The resultant value is mapped against a curve with two values, one and zero, which is equivalent to diabetes or no diabetes. Decision Tree Algorithm The Decision Tree algorithm can be nicely visualized. The algorithm uses a binary tree structure with each node making a decision based upon the values of the feature. At each node, the feature value causes us to go down one path or another. A lot of data may be required to find the value which defines taking one path or another. As we see decision trees have the advantage of having tools available to produce a picture of the tree. This makes it easy to follow along and visualize how the trained model works. Training the Model Letting specific data teach a machine learning algorithm to create a specific prediction model. Why retrain? Retraining will ensure that our model can take advantage of the new data to make better predictions. And also verify the algorithm can still create a high-performance model with the new data. Performance Improvement Options Adjust current algorithm Get more data or improve data Improve training Switch algorithms Overfitting The algorithm analyses the data and trains itself to create a high mathematical order model based on the data. $$y = x_1 + w_2x_2^3 + w_3x_3^8$$ These high-order terms let this equation define a precise decision boundary between the positive and negative values, but as a result, the training process has created a model that works very well on training data but poorly when asked to predict values based on data it has not trained - this is the class overfit problem and is an issue that must be handled to create machine learning models that work well not only on the training data but also on real-world data.","title":"Machine Learning Workflow"},{"location":"ml_workflow/#solution-statement","text":"Use the Machine Learning Workflow to process and transform Pima Indian data to create a prediction model. This model must predict which peopel are likely to develop diabetes with 70% or greater accuracy","title":"Solution Statement"},{"location":"ml_workflow/#tidy-data","text":"Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column each observation is a row each type of observational unit is a table","title":"Tidy Data"},{"location":"ml_workflow/#selecting-the-algorithm","text":"We will use our problem knowledge to help us decide the algorithm to use. we will discuss - The role of the algorithm in machine learning process, select our initial algorithm by utilizing the requirements identified in the solution statement as a guide, and discuss at a high level the characteristics of some specific algorithms. Finally select one algorithm to be our initial algorithm, in machine learning we often cycle through the workflow. In our search to find the best solution, it is likely we will need to train and evaluate multiple algorithms. Let's review how the algorithm is involved in the process. When the training function (often named fit in scikit-learn) is called, the algorithm executes its logic and processes the training data. Using the algorithm's logic, the data in analyzed. This analysis evaluates the data with respect to mathematical model and logic associated with the algorithm. The algorithm uses the results of this analysis to adjust internal parameters to produce a model that has been trained to best fit the features in the training data and produce the associated class result. This best fit is defined by evaluating a function specific to the particular algorithm. The fit parameters are stored and the model is now said to be trained. Later, the trained model is called the prediction function (often named predict in scikit-learn). When this prediction function is called, real data is passed to the trained model. Using only the features in the data, the trained model uses its code and parameter values set during training to evaluate the data's features and predict the class result, diabetes or not for this new data. Decision factors to select our initial algorithm: We will use our solution statement and knowledge of the workflow to help guide us in the evaluation of these factors. what Learning Type they support the Result Type the algorithm predicts the Complexity of the algorithm whether the algorithm is Basic or Enhanced Each algorithm has a set of problems with which it works best. One way to divide them is to look at the type of Learning they support. Reading the statement, we see that our solution is about prediction. Prediction means supervised machine learning, so we can eliminate all algorithms that do not support it. Let's see how Result Type can help. Prediction results can be divided into two categories: Regression (Continuous values) Classification (Discrete values) Based on the Statment, the algorithm must support Binary classification. Since this is our initial algorithm, let's stick to the basic algorithms.","title":"Selecting the algorithm"},{"location":"ml_workflow/#selecting-our-initial-algorithm","text":"Candidate Algorithms: Naive Bayes Logistic Regression Decision Tree More complex algoritms use these as building blocks.","title":"Selecting Our Initial Algorithm"},{"location":"ml_workflow/#naive-bayes-algorithm","text":"The Naive Bayes algorithm is based on Bayes' Theorem. This theorem calculates a probability of a diabetes by looking at the likelihood of diabetes based on previous data combined with probability of diabetes on nearby feature values. In other words, so how often does the person having high blood pressure correlate to diabetes? It makes the naive assumption that all of the features we pass in are independent of each other and equally impact the result. This assumption that every featuer is independent to the others allows for fast conversions and therefore requires a small amount of data to train.","title":"Naive Bayes Algorithm"},{"location":"ml_workflow/#logistic-regression-algorithm","text":"The Logistic Regression algorithm has a somewhat confusing name. In Statistics, Regression often implies continuous values. But Logistics Regression returns a binary result. The algorithm measures the relationship of each feature and weights them based on their impact on the result. The resultant value is mapped against a curve with two values, one and zero, which is equivalent to diabetes or no diabetes.","title":"Logistic Regression Algorithm"},{"location":"ml_workflow/#decision-tree-algorithm","text":"The Decision Tree algorithm can be nicely visualized. The algorithm uses a binary tree structure with each node making a decision based upon the values of the feature. At each node, the feature value causes us to go down one path or another. A lot of data may be required to find the value which defines taking one path or another. As we see decision trees have the advantage of having tools available to produce a picture of the tree. This makes it easy to follow along and visualize how the trained model works.","title":"Decision Tree Algorithm"},{"location":"ml_workflow/#training-the-model","text":"Letting specific data teach a machine learning algorithm to create a specific prediction model. Why retrain? Retraining will ensure that our model can take advantage of the new data to make better predictions. And also verify the algorithm can still create a high-performance model with the new data. Performance Improvement Options Adjust current algorithm Get more data or improve data Improve training Switch algorithms","title":"Training the Model"},{"location":"ml_workflow/#overfitting","text":"The algorithm analyses the data and trains itself to create a high mathematical order model based on the data. $$y = x_1 + w_2x_2^3 + w_3x_3^8$$ These high-order terms let this equation define a precise decision boundary between the positive and negative values, but as a result, the training process has created a model that works very well on training data but poorly when asked to predict values based on data it has not trained - this is the class overfit problem and is an issue that must be handled to create machine learning models that work well not only on the training data but also on real-world data.","title":"Overfitting"},{"location":"source/DataFrame/","text":"import pandas as pd import numpy as np # We load Google stock data in a DataFrame Google_stock = pd.read_csv('C:/Users/Vinay/Python Workspace/Nanodegree/goog-1.csv') # We print some information about Google_stock print('Google_stock is of type:', type(Google_stock)) print('Google_stock has shape:', Google_stock.shape) Google_stock is of type: <class 'pandas.core.frame.DataFrame'> Google_stock has shape: (3313, 7) Google_stock.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Open High Low Close Adj Close Volume 0 2004-08-19 49.676899 51.693783 47.669952 49.845802 49.845802 44994500 1 2004-08-20 50.178635 54.187561 49.925285 53.805050 53.805050 23005800 2 2004-08-23 55.017166 56.373344 54.172661 54.346527 54.346527 18393200 3 2004-08-24 55.260582 55.439419 51.450363 52.096165 52.096165 15361800 4 2004-08-25 52.140873 53.651051 51.604362 52.657513 52.657513 9257400 Google_stock.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Open High Low Close Adj Close Volume 0 2004-08-19 49.676899 51.693783 47.669952 49.845802 49.845802 44994500 1 2004-08-20 50.178635 54.187561 49.925285 53.805050 53.805050 23005800 Google_stock.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Open High Low Close Adj Close Volume 3308 2017-10-09 980.000000 985.424988 976.109985 977.000000 977.000000 891400 3309 2017-10-10 980.000000 981.570007 966.080017 972.599976 972.599976 968400 3310 2017-10-11 973.719971 990.710022 972.250000 989.250000 989.250000 1693300 3311 2017-10-12 987.450012 994.119995 985.000000 987.830017 987.830017 1262400 3312 2017-10-13 992.000000 997.210022 989.000000 989.679993 989.679993 1157700 Google_stock.tail(8) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Open High Low Close Adj Close Volume 3305 2017-10-04 957.000000 960.390015 950.690002 951.679993 951.679993 952400 3306 2017-10-05 955.489990 970.909973 955.179993 969.960022 969.960022 1213800 3307 2017-10-06 966.700012 979.460022 963.359985 978.890015 978.890015 1173900 3308 2017-10-09 980.000000 985.424988 976.109985 977.000000 977.000000 891400 3309 2017-10-10 980.000000 981.570007 966.080017 972.599976 972.599976 968400 3310 2017-10-11 973.719971 990.710022 972.250000 989.250000 989.250000 1693300 3311 2017-10-12 987.450012 994.119995 985.000000 987.830017 987.830017 1262400 3312 2017-10-13 992.000000 997.210022 989.000000 989.679993 989.679993 1157700 Google_stock.isnull().any() Date False Open False High False Low False Close False Adj Close False Volume False dtype: bool # We get descriptive statistics on our stock data Google_stock.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Open High Low Close Adj Close Volume count 3313.000000 3313.000000 3313.000000 3313.000000 3313.000000 3.313000e+03 mean 380.186092 383.493740 376.519309 380.072458 380.072458 8.038476e+06 std 223.818650 224.974534 222.473232 223.853780 223.853780 8.399521e+06 min 49.274517 50.541279 47.669952 49.681866 49.681866 7.900000e+03 25% 226.556473 228.394516 224.003082 226.407440 226.407440 2.584900e+06 50% 293.312286 295.433502 289.929291 293.029114 293.029114 5.281300e+06 75% 536.650024 540.000000 532.409973 536.690002 536.690002 1.065370e+07 max 992.000000 997.210022 989.000000 989.679993 989.679993 8.276810e+07 # We get descriptive statistics on a single column of our DataFrame Google_stock['Adj Close'].describe() count 3313.000000 mean 380.072458 std 223.853780 min 49.681866 25% 226.407440 50% 293.029114 75% 536.690002 max 989.679993 Name: Adj Close, dtype: float64 # We print information about our DataFrame print() print('Maximum values of each column:\\n', Google_stock.max()) print() print('Minimum Close value:', Google_stock['Close'].min()) print() print('Average value of each column:\\n', Google_stock.mean()) Maximum values of each column: Date 2017-10-13 Open 992 High 997.21 Low 989 Close 989.68 Adj Close 989.68 Volume 82768100 dtype: object Minimum Close value: 49.681866 Average value of each column: Open 3.801861e+02 High 3.834937e+02 Low 3.765193e+02 Close 3.800725e+02 Adj Close 3.800725e+02 Volume 8.038476e+06 dtype: float64 # We display the correlation between columns Google_stock.corr() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Open High Low Close Adj Close Volume Open 1.000000 0.999904 0.999845 0.999745 0.999745 -0.564258 High 0.999904 1.000000 0.999834 0.999868 0.999868 -0.562749 Low 0.999845 0.999834 1.000000 0.999899 0.999899 -0.567007 Close 0.999745 0.999868 0.999899 1.000000 1.000000 -0.564967 Adj Close 0.999745 0.999868 0.999899 1.000000 1.000000 -0.564967 Volume -0.564258 -0.562749 -0.567007 -0.564967 -0.564967 1.000000 # We load Fake Company data in a DataFrame data = pd.read_csv('C:/Users/Vinay/Python Workspace/Nanodegree/fake_company.csv') data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Name Department Age Salary 0 1990 Alice HR 25 50000 1 1990 Bob RD 30 48000 2 1990 Charlie Admin 45 55000 3 1991 Alice HR 26 52000 4 1991 Bob RD 31 50000 5 1991 Charlie Admin 46 60000 6 1992 Alice HR 27 60000 7 1992 Bob RD 32 52000 8 1992 Charlie Admin 47 62000 # We display the total amount of money spent in salaries each year data.groupby(['Year'])['Salary'].sum() Year 1990 153000 1991 162000 1992 174000 Name: Salary, dtype: int64 # We display the average salary per year data.groupby(['Year'])['Salary'].mean() Year 1990 51000 1991 54000 1992 58000 Name: Salary, dtype: int64 # We display the total salary each employee received in all the years they worked for the company data.groupby(['Name'])['Salary'].sum() Name Alice 162000 Bob 150000 Charlie 177000 Name: Salary, dtype: int64 # We display the salary distribution per department per year. data.groupby(['Year', 'Department'])['Salary'].sum() Year Department 1990 Admin 55000 HR 50000 RD 48000 1991 Admin 60000 HR 52000 RD 50000 1992 Admin 62000 HR 60000 RD 52000 Name: Salary, dtype: int64","title":"DataFrame"},{"location":"source/NumPy/","text":"import numpy as np x = np.array([1,2,3,4]) x array([1, 2, 3, 4]) print(x) print(type(x)) [1 2 3 4] <class 'numpy.ndarray'> np.zeros((2,3)) array([[0., 0., 0.], [0., 0., 0.]]) np.ones((3,3)) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) np.full((1,4), 5) array([[5, 5, 5, 5]]) np.arange(10) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) np.eye(4) np.eye(4) array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) np.diag([1,2,3,4]) array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) np.arange(3,10) array([3, 4, 5, 6, 7, 8, 9]) np.arange(2,9,3) array([2, 5, 8]) np.arange(20).reshape(4, 5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]) np.linspace(0,50,10, endpoint=False).reshape(5,2) array([[ 0., 5.], [10., 15.], [20., 25.], [30., 35.], [40., 45.]]) np.random.randint(4,15,size=(3,2)) array([[ 6, 9], [ 8, 4], [13, 6]]) Y = np.array([[1,2,3],[4,5,6],[7,8,9], [10,11,12]]) # We print Y print() print('Y = \\n', Y) print() # We print information about Y print('Y has dimensions:', Y.shape) print('Y has a total of', Y.size, 'elements') print('Y is an object of type:', type(Y)) print('The elements in Y are of type:', Y.dtype) Y = [[ 1 2 3] [ 4 5 6] [ 7 8 9] [10 11 12]] Y has dimensions: (4, 3) Y has a total of 12 elements Y is an object of type: <class 'numpy.ndarray'> The elements in Y are of type: int32 # We create a rank 1 ndarray x = np.array([1, 2, 3, 4, 5]) # We create a rank 2 ndarray Y = np.array([[1,2,3],[4,5,6],[7,8,9]]) # We print x print() print('Original x = ', x) # We delete the first and last element of x x = np.delete(x, [0,4]) # We print x with the first and last element deleted print() print('Modified x = ', x) # We print Y print() print('Original Y = \\n', Y) # We delete the first row of y w = np.delete(Y, 0, axis=0) # We delete the first and last column of y v = np.delete(Y, [0,2], axis=1) # We print w print() print('w = \\n', w) # We print v print() print('v = \\n', v) Original x = [1 2 3 4 5] Modified x = [2 3 4] Original Y = [[1 2 3] [4 5 6] [7 8 9]] w = [[4 5 6] [7 8 9]] v = [[2] [5] [8]] # We create a rank 1 ndarray x = np.array([1, 2, 5, 6, 7]) # We create a rank 2 ndarray Y = np.array([[1,2,3],[7,8,9]]) # We print x print() print('Original x = ', x) # We insert the integer 3 and 4 between 2 and 5 in x. x = np.insert(x,2,[3,4]) # We print x with the inserted elements print() print('x = ', x) # We print Y print() print('Original Y = \\n', Y) # We insert a row between the first and last row of y w = np.insert(Y,1,[4,5,6],axis=0) # We insert a column full of 5s between the first and second column of y v = np.insert(Y,1,5, axis=1) # We print w print() print('w = \\n', w) # We print v print() print('v = \\n', v)# We create a rank 1 ndarray x = np.array([1, 2, 3, 4, 5]) # We create a rank 2 ndarray Y = np.array([[1,2,3],[4,5,6]]) # We print x print() print('Original x = ', x) # We append the integer 6 to x x = np.append(x, 6) # We print x print() print('x = ', x) # We append the integer 7 and 8 to x x = np.append(x, [7,8]) # We print x print() print('x = ', x) # We print Y print() print('Original Y = \\n', Y) # We append a new row containing 7,8,9 to y v = np.append(Y, [[7,8,9]], axis=0) # We append a new column containing 9 and 10 to y q = np.append(Y,[[9],[10]], axis=1) # We print v print() print('v = \\n', v) # We print q print() print('q = \\n', q) Original x = [1 2 5 6 7] x = [1 2 3 4 5 6 7] Original Y = [[1 2 3] [7 8 9]] w = [[1 2 3] [4 5 6] [7 8 9]] v = [[1 5 2 3] [7 5 8 9]] Original x = [1 2 3 4 5] x = [1 2 3 4 5 6] x = [1 2 3 4 5 6 7 8] Original Y = [[1 2 3] [4 5 6]] v = [[1 2 3] [4 5 6] [7 8 9]] q = [[ 1 2 3 9] [ 4 5 6 10]] # We create a rank 1 ndarray x = np.array([1,2]) # We create a rank 2 ndarray Y = np.array([[3,4],[5,6]]) # We print x print() print('x = ', x) # We print Y print() print('Y = \\n', Y) # We stack x on top of Y z = np.vstack((x,Y)) # We stack x on the right of Y. We need to reshape x in order to stack it on the right of Y. w = np.hstack((Y,x.reshape(2,1))) # We print z print() print('z = \\n', z) # We print w print() print('w = \\n', w) x = [1 2] Y = [[3 4] [5 6]] z = [[1 2] [3 4] [5 6]] w = [[3 4 1] [5 6 2]] # We create a 4 x 5 ndarray that contains integers from 0 to 19 X = np.arange(20).reshape(4, 5) # We print X print() print('X = \\n', X) print() # We select all the elements that are in the 2nd through 4th rows and in the 3rd to 5th columns Z = X[1:4,2:5] # We print Z print('Z = \\n', Z) # We can select the same elements as above using method 2 W = X[1:,2:5] # We print W print() print('W = \\n', W) # We select all the elements that are in the 1st through 3rd rows and in the 3rd to 4th columns Y = X[:3,2:5] # We print Y print() print('Y = \\n', Y) # We select all the elements in the 3rd row v = X[2,:] # We print v print() print('v = ', v) # We select all the elements in the 3rd column q = X[:,2] # We print q print() print('q = ', q) # We select all the elements in the 3rd column but return a rank 2 ndarray R = X[:,2:3] # We print R print() print('R = \\n', R) X = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]] Z = [[ 7 8 9] [12 13 14] [17 18 19]] W = [[ 7 8 9] [12 13 14] [17 18 19]] Y = [[ 2 3 4] [ 7 8 9] [12 13 14]] v = [10 11 12 13 14] q = [ 2 7 12 17] R = [[ 2] [ 7] [12] [17]] # We create a 4 x 5 ndarray that contains integers from 0 to 19 X = np.arange(20).reshape(4, 5) # We print X print() print('X = \\n', X) print() # create a copy of the slice using the np.copy() function Z = np.copy(X[1:4,2:5]) # create a copy of the slice using the copy as a method W = X[1:4,2:5].copy() # We change the last element in Z to 555 Z[2,2] = 555 # We change the last element in W to 444 W[2,2] = 444 # We print X print() print('X = \\n', X) # We print Z print() print('Z = \\n', Z) # We print W print() print('W = \\n', W) X = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]] X = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]] Z = [[ 7 8 9] [ 12 13 14] [ 17 18 555]] W = [[ 7 8 9] [ 12 13 14] [ 17 18 444]] # We create a 4 x 5 ndarray that contains integers from 0 to 19 X = np.arange(20).reshape(4, 5) # We create a rank 1 ndarray that will serve as indices to select elements from X indices = np.array([1,3]) # We print X print() print('X = \\n', X) print() # We print indices print('indices = ', indices) print() # We use the indices ndarray to select the 2nd and 4th row of X Y = X[indices,:] # We use the indices ndarray to select the 2nd and 4th column of X Z = X[:, indices] # We print Y print() print('Y = \\n', Y) # We print Z print() print('Z = \\n', Z) X = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]] indices = [1 3] Y = [[ 5 6 7 8 9] [15 16 17 18 19]] Z = [[ 1 3] [ 6 8] [11 13] [16 18]] # We create a 4 x 5 ndarray that contains integers from 0 to 19 X = np.arange(25).reshape(5, 5) # We print X print() print('X = \\n', X) print() # We print the elements in the main diagonal of X print('z =', np.diag(X)) print() # We print the elements above the main diagonal of X print('y =', np.diag(X, k=1)) print() # We print the elements below the main diagonal of X print('w = ', np.diag(X, k=-1)) X = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24]] z = [ 0 6 12 18 24] y = [ 1 7 13 19] w = [ 5 11 17 23] # Create 3 x 3 ndarray with repeated values X = np.array([[1,2,3],[5,2,8],[1,2,3]]) # We print X print() print('X = \\n', X) print() # We print the unique elements of X print('The unique elements in X are:',np.unique(X)) X = [[1 2 3] [5 2 8] [1 2 3]] The unique elements in X are: [1 2 3 5 8] # We create a 5 x 5 ndarray that contains integers from 0 to 24 X = np.arange(25).reshape(5, 5) # We print X print() print('Original X = \\n', X) print() # We use Boolean indexing to select elements in X: print('The elements in X that are greater than 10:', X[X > 10]) print('The elements in X that less than or equal to 7:', X[X <= 7]) print('The elements in X that are between 10 and 17:', X[(X > 10) & (X < 17)]) # We use Boolean indexing to assign the elements that are between 10 and 17 the value of -1 X[(X > 10) & (X < 17)] = -1 # We print X print() print('X = \\n', X) print() Original X = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24]] The elements in X that are greater than 10: [11 12 13 14 15 16 17 18 19 20 21 22 23 24] The elements in X that less than or equal to 7: [0 1 2 3 4 5 6 7] The elements in X that are between 10 and 17: [11 12 13 14 15 16] X = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 -1 -1 -1 -1] [-1 -1 17 18 19] [20 21 22 23 24]] # We create a rank 1 ndarray x = np.array([1,2,3,4,5]) # We create a rank 1 ndarray y = np.array([6,7,2,8,4]) # We print x print() print('x = ', x) # We print y print() print('y = ', y) # We use set operations to compare x and y: print() print('The elements that are both in x and y:', np.intersect1d(x,y)) print('The elements that are in x that are not in y:', np.setdiff1d(x,y)) print('All the elements of x and y:',np.union1d(x,y)) x = [1 2 3 4 5] y = [6 7 2 8 4] The elements that are both in x and y: [2 4] The elements that are in x that are not in y: [1 3 5] All the elements of x and y: [1 2 3 4 5 6 7 8] # We create an unsorted rank 1 ndarray x = np.random.randint(1,11,size=(10,)) # We print x print() print('Original x = ', x) # We sort x and print the sorted array using sort as a function. print() print('Sorted x (out of place):', np.sort(x)) # When we sort out of place the original array remains intact. To see this we print x again print() print('x after sorting:', x) Original x = [ 4 10 6 1 2 3 6 7 3 5] Sorted x (out of place): [ 1 2 3 3 4 5 6 6 7 10] x after sorting: [ 4 10 6 1 2 3 6 7 3 5] # We create an unsorted rank 1 ndarray x = np.random.randint(1,11,size=(10,)) # We print x print() print('Original x = ', x) # We sort x and print the sorted array using sort as a method. x.sort() # When we sort in place the original array is changed to the sorted array. To see this we print x again print() print('x after sorting:', x) Original x = [1 2 4 1 9 9 7 8 2 1] x after sorting: [1 1 1 2 2 4 7 8 9 9] # We create an unsorted rank 2 ndarray X = np.random.randint(1,11,size=(5,5)) # We print X print() print('Original X = \\n', X) print() # We sort the columns of X and print the sorted array print() print('X with sorted columns :\\n', np.sort(X, axis = 0)) # We sort the rows of X and print the sorted array print() print('X with sorted rows :\\n', np.sort(X, axis = 1)) Original X = [[ 4 2 4 10 5] [ 9 6 10 6 4] [ 2 1 8 9 8] [ 5 9 3 4 1] [ 7 8 7 6 8]] X with sorted columns : [[ 2 1 3 4 1] [ 4 2 4 6 4] [ 5 6 7 6 5] [ 7 8 8 9 8] [ 9 9 10 10 8]] X with sorted rows : [[ 2 4 4 5 10] [ 4 6 6 9 10] [ 1 2 8 8 9] [ 1 3 4 5 9] [ 6 7 7 8 8]] y = x[x%2!=0] print(y) [ 1 3 5 7 9 11 13 15 17 19 21 23 25] # We create two rank 1 ndarrays x = np.array([1,2,3,4]) y = np.array([5.5,6.5,7.5,8.5]) # We print x print() print('x = ', x) # We print y print() print('y = ', y) print() # We perfrom basic element-wise operations using arithmetic symbols and functions print('x + y = ', x + y) print('add(x,y) = ', np.add(x,y)) print() print('x - y = ', x - y) print('subtract(x,y) = ', np.subtract(x,y)) print() print('x * y = ', x * y) print('multiply(x,y) = ', np.multiply(x,y)) print() print('x / y = ', x / y) print('divide(x,y) = ', np.divide(x,y)) x = [1 2 3 4] y = [5.5 6.5 7.5 8.5] x + y = [ 6.5 8.5 10.5 12.5] add(x,y) = [ 6.5 8.5 10.5 12.5] x - y = [-4.5 -4.5 -4.5 -4.5] subtract(x,y) = [-4.5 -4.5 -4.5 -4.5] x * y = [ 5.5 13. 22.5 34. ] multiply(x,y) = [ 5.5 13. 22.5 34. ] x / y = [0.18181818 0.30769231 0.4 0.47058824] divide(x,y) = [0.18181818 0.30769231 0.4 0.47058824] # We create two rank 2 ndarrays X = np.array([1,2,3,4]).reshape(2,2) Y = np.array([5.5,6.5,7.5,8.5]).reshape(2,2) # We print X print() print('X = \\n', X) # We print Y print() print('Y = \\n', Y) print() # We perform basic element-wise operations using arithmetic symbols and functions print('X + Y = \\n', X + Y) print() print('add(X,Y) = \\n', np.add(X,Y)) print() print('X - Y = \\n', X - Y) print() print('subtract(X,Y) = \\n', np.subtract(X,Y)) print() print('X * Y = \\n', X * Y) print() print('multiply(X,Y) = \\n', np.multiply(X,Y)) print() print('X / Y = \\n', X / Y) print() print('divide(X,Y) = \\n', np.divide(X,Y)) X = [[1 2] [3 4]] Y = [[5.5 6.5] [7.5 8.5]] X + Y = [[ 6.5 8.5] [10.5 12.5]] add(X,Y) = [[ 6.5 8.5] [10.5 12.5]] X - Y = [[-4.5 -4.5] [-4.5 -4.5]] subtract(X,Y) = [[-4.5 -4.5] [-4.5 -4.5]] X * Y = [[ 5.5 13. ] [22.5 34. ]] multiply(X,Y) = [[ 5.5 13. ] [22.5 34. ]] X / Y = [[0.18181818 0.30769231] [0.4 0.47058824]] divide(X,Y) = [[0.18181818 0.30769231] [0.4 0.47058824]] # We create a rank 1 ndarray x = np.array([1,2,3,4]) # We print x print() print('x = ', x) # We apply different mathematical functions to all elements of x print() print('EXP(x) =', np.exp(x)) print() print('SQRT(x) =',np.sqrt(x)) print() print('POW(x,2) =',np.power(x,2)) # We raise all elements to the power of 2 x = [1 2 3 4] EXP(x) = [ 2.71828183 7.3890561 20.08553692 54.59815003] SQRT(x) = [1. 1.41421356 1.73205081 2. ] POW(x,2) = [ 1 4 9 16] # We create a 2 x 2 ndarray X = np.array([[1,2], [3,4]]) # We print x print() print('X = \\n', X) print() print('Average of all elements in X:', X.mean()) print('Average of all elements in the columns of X:', X.mean(axis=0)) print('Average of all elements in the rows of X:', X.mean(axis=1)) print() print('Sum of all elements in X:', X.sum()) print('Sum of all elements in the columns of X:', X.sum(axis=0)) print('Sum of all elements in the rows of X:', X.sum(axis=1)) print() print('Standard Deviation of all elements in X:', X.std()) print('Standard Deviation of all elements in the columns of X:', X.std(axis=0)) print('Standard Deviation of all elements in the rows of X:', X.std(axis=1)) print() print('Median of all elements in X:', np.median(X)) print('Median of all elements in the columns of X:', np.median(X,axis=0)) print('Median of all elements in the rows of X:', np.median(X,axis=1)) print() print('Maximum value of all elements in X:', X.max()) print('Maximum value of all elements in the columns of X:', X.max(axis=0)) print('Maximum value of all elements in the rows of X:', X.max(axis=1)) print() print('Minimum value of all elements in X:', X.min()) print('Minimum value of all elements in the columns of X:', X.min(axis=0)) print('Minimum value of all elements in the rows of X:', X.min(axis=1)) X = [[1 2] [3 4]] Average of all elements in X: 2.5 Average of all elements in the columns of X: [2. 3.] Average of all elements in the rows of X: [1.5 3.5] Sum of all elements in X: 10 Sum of all elements in the columns of X: [4 6] Sum of all elements in the rows of X: [3 7] Standard Deviation of all elements in X: 1.118033988749895 Standard Deviation of all elements in the columns of X: [1. 1.] Standard Deviation of all elements in the rows of X: [0.5 0.5] Median of all elements in X: 2.5 Median of all elements in the columns of X: [2. 3.] Median of all elements in the rows of X: [1.5 3.5] Maximum value of all elements in X: 4 Maximum value of all elements in the columns of X: [3 4] Maximum value of all elements in the rows of X: [2 4] Minimum value of all elements in X: 1 Minimum value of all elements in the columns of X: [1 2] Minimum value of all elements in the rows of X: [1 3] # We create a 2 x 2 ndarray X = np.array([[1,2], [3,4]]) # We print x print() print('X = \\n', X) print() print('3 * X = \\n', 3 * X) print() print('3 + X = \\n', 3 + X) print() print('X - 3 = \\n', X - 3) print() print('X / 3 = \\n', X / 3) X = [[1 2] [3 4]] 3 * X = [[ 3 6] [ 9 12]] 3 + X = [[4 5] [6 7]] X - 3 = [[-2 -1] [ 0 1]] X / 3 = [[0.33333333 0.66666667] [1. 1.33333333]] # We create a rank 1 ndarray x = np.array([1,2,3]) # We create a 3 x 3 ndarray Y = np.array([[1,2,3],[4,5,6],[7,8,9]]) # We create a 3 x 1 ndarray Z = np.array([1,2,3]).reshape(3,1) # We print x print() print('x = ', x) print() # We print Y print() print('Y = \\n', Y) print() # We print Z print() print('Z = \\n', Z) print() print('x + Y = \\n', x + Y) print() print('Z + Y = \\n',Z + Y) x = [1 2 3] Y = [[1 2 3] [4 5 6] [7 8 9]] Z = [[1] [2] [3]] x + Y = [[ 2 4 6] [ 5 7 9] [ 8 10 12]] Z + Y = [[ 2 3 4] [ 6 7 8] [10 11 12]]","title":"NumPy"},{"location":"source/Pandas/","text":"import pandas as pd # We create a Pandas Series that stores a grocery list groceries = pd.Series(data = [30, 6, 'Yes', 'No'], index = ['eggs', 'apples', 'milk', 'bread']) # We display the Groceries Pandas Series print(groceries) print() # We print some information about Groceries print('Groceries has shape:', groceries.shape) print('Groceries has dimension:', groceries.ndim) print('Groceries has a total of', groceries.size, 'elements') print() # We print the index and data of Groceries print('The data in Groceries is:', groceries.values) print('The index of Groceries is:', groceries.index) print() # We check whether bananas is a food item (an index) in Groceries x = 'bananas' in groceries # We check whether bread is a food item (an index) in Groceries y = 'bread' in groceries # We print the results print('Is bananas an index label in Groceries:', x) print('Is bread an index label in Groceries:', y) eggs 30 apples 6 milk Yes bread No dtype: object Groceries has shape: (4,) Groceries has dimension: 1 Groceries has a total of 4 elements The data in Groceries is: [30 6 'Yes' 'No'] The index of Groceries is: Index(['eggs', 'apples', 'milk', 'bread'], dtype='object') Is bananas an index label in Groceries: False Is bread an index label in Groceries: True # We access elements in Groceries using index labels: # We use a single index label print('How many eggs do we need to buy:', groceries['eggs']) print() # we can access multiple index labels print('Do we need milk and bread:\\n', groceries[['milk', 'bread']]) print() # we use loc to access multiple index labels print('How many eggs and apples do we need to buy:\\n', groceries.loc[['eggs', 'apples']]) print() # We access elements in Groceries using numerical indices: # we use multiple numerical indices print('How many eggs and apples do we need to buy:\\n', groceries[[0, 1]]) print() # We use a negative numerical index print('Do we need bread:\\n', groceries[[-1]]) print() # We use a single numerical index print('How many eggs do we need to buy:', groceries[0]) print() # we use iloc to access multiple numerical indices print('Do we need milk and bread:\\n', groceries.iloc[[2, 3]]) How many eggs do we need to buy: 30 Do we need milk and bread: milk Yes bread No dtype: object How many eggs and apples do we need to buy: eggs 30 apples 6 dtype: object How many eggs and apples do we need to buy: eggs 30 apples 6 dtype: object Do we need bread: bread No dtype: object How many eggs do we need to buy: 30 Do we need milk and bread: milk Yes bread No dtype: object # We display the original grocery list print('Original Grocery List:\\n', groceries) # We change the number of eggs to 2 groceries['eggs'] = 2 # We display the changed grocery list print() print('Modified Grocery List:\\n', groceries) Original Grocery List: eggs 30 apples 6 milk Yes bread No dtype: object Modified Grocery List: eggs 2 apples 6 milk Yes bread No dtype: object # We display the original grocery list print('Original Grocery List:\\n', groceries) # We remove apples from our grocery list. The drop function removes elements out of place print() print('We remove apples (out of place):\\n', groceries.drop('apples')) # When we remove elements out of place the original Series remains intact. To see this # we display our grocery list again print() print('Grocery List after removing apples out of place:\\n', groceries) # We remove apples from our grocery list in place by setting the inplace keyword to True groceries.drop('apples', inplace = True) # When we remove elements in place the original Series its modified. To see this # we display our grocery list again print() print('Grocery List after removing apples in place:\\n', groceries) Original Grocery List: eggs 2 apples 6 milk Yes bread No dtype: object We remove apples (out of place): eggs 2 milk Yes bread No dtype: object Grocery List after removing apples out of place: eggs 2 apples 6 milk Yes bread No dtype: object Grocery List after removing apples in place: eggs 2 milk Yes bread No dtype: object # We create a Pandas Series that stores a grocery list of just fruits fruits= pd.Series(data = [10, 6, 3,], index = ['apples', 'oranges', 'bananas']) # We display the fruits Pandas Series fruits apples 10 oranges 6 bananas 3 dtype: int64 # We perform basic element-wise operations using arithmetic symbols print() print('fruits + 2:\\n', fruits + 2) # We add 2 to each item in fruits print() print('fruits - 2:\\n', fruits - 2) # We subtract 2 to each item in fruits print() print('fruits * 2:\\n', fruits * 2) # We multiply each item in fruits by 2 print() print('fruits / 2:\\n', fruits / 2) # We divide each item in fruits by 2 print() fruits + 2: apples 12 oranges 8 bananas 5 dtype: int64 fruits - 2: apples 8 oranges 4 bananas 1 dtype: int64 fruits * 2: apples 20 oranges 12 bananas 6 dtype: int64 fruits / 2: apples 5.0 oranges 3.0 bananas 1.5 dtype: float64 # We import NumPy as np to be able to use the mathematical functions import numpy as np # We print fruits for reference print('Original grocery list of fruits:\\n', fruits) # We apply different mathematical functions to all elements of fruits print() print('EXP(X) = \\n', np.exp(fruits)) print() print('SQRT(X) =\\n', np.sqrt(fruits)) print() print('POW(X,2) =\\n',np.power(fruits,2)) # We raise all elements of fruits to the power of 2 Original grocery list of fruits: apples 10 oranges 6 bananas 3 dtype: int64 EXP(X) = apples 22026.465795 oranges 403.428793 bananas 20.085537 dtype: float64 SQRT(X) = apples 3.162278 oranges 2.449490 bananas 1.732051 dtype: float64 POW(X,2) = apples 100 oranges 36 bananas 9 dtype: int64 # We print fruits for reference print('Original grocery list of fruits:\\n ', fruits) print() # We add 2 only to the bananas print('Amount of bananas + 2 = ', fruits['bananas'] + 2) print() # We subtract 2 from apples print('Amount of apples - 2 = ', fruits.iloc[0] - 2) print() # We multiply apples and oranges by 2 print('We double the amount of apples and oranges:\\n', fruits[['apples', 'oranges']] * 2) print() # We divide apples and oranges by 2 print('We half the amount of apples and oranges:\\n', fruits.loc[['apples', 'oranges']] / 2) Original grocery list of fruits: apples 10 oranges 6 bananas 3 dtype: int64 Amount of bananas + 2 = 5 Amount of apples - 2 = 8 We double the amount of apples and oranges: apples 20 oranges 12 dtype: int64 We half the amount of apples and oranges: apples 5.0 oranges 3.0 dtype: float64 # Create a Pandas Series that contains the distance of some planets from the Sun. # Use the name of the planets as the index to your Pandas Series, and the distance # from the Sun as your data. The distance from the Sun is in units of 10^6 km distance_from_sun = [149.6, 1433.5, 227.9, 108.2, 778.6] planets = ['Earth','Saturn', 'Mars','Venus', 'Jupiter'] # Create a Pandas Series using the above data, with the name of the planets as # the index and the distance from the Sun as your data. dist_planets = pd.Series(distance_from_sun, planets) print(dist_planets) # Calculate the number of minutes it takes sunlight to reach each planet. You can # do this by dividing the distance from the Sun for each planet by the speed of light. # Since in the data above the distance from the Sun is in units of 10^6 km, you can # use a value for the speed of light of c = 18, since light travels 18 x 10^6 km/minute. time_light = dist_planets / 18 # Use Boolean indexing to select only those planets for which sunlight takes less # than 40 minutes to reach them. close_planets = time_light[time_light.values < 40] print() print('close planets\\n', close_planets) Earth 149.6 Saturn 1433.5 Mars 227.9 Venus 108.2 Jupiter 778.6 dtype: float64 close planets Earth 8.311111 Mars 12.661111 Venus 6.011111 dtype: float64 # We create a dictionary of Pandas Series items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']), 'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])} # We print the type of items to see that it is a dictionary print(type(items)) <class 'dict'> # We create a Pandas DataFrame by passing it a dictionary of Pandas Series shopping_carts = pd.DataFrame(items) # We display the DataFrame shopping_carts .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bob Alice bike 245.0 500.0 book NaN 40.0 glasses NaN 110.0 pants 25.0 45.0 watch 55.0 NaN # We create a dictionary of Pandas Series without indexes data = {'Bob' : pd.Series([245, 25, 55]), 'Alice' : pd.Series([40, 110, 500, 45])} # We create a DataFrame df = pd.DataFrame(data) # We display the DataFrame df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bob Alice 0 245.0 40 1 25.0 110 2 55.0 500 3 NaN 45 # We print some information about shopping_carts print('shopping_carts has shape:', shopping_carts.shape) print('shopping_carts has dimension:', shopping_carts.ndim) print('shopping_carts has a total of:', shopping_carts.size, 'elements') print() print('The data in shopping_carts is:\\n', shopping_carts.values) print() print('The row index in shopping_carts is:', shopping_carts.index) print() print('The column index in shopping_carts is:', shopping_carts.columns) shopping_carts has shape: (5, 2) shopping_carts has dimension: 2 shopping_carts has a total of: 10 elements The data in shopping_carts is: [[245. 500.] [ nan 40.] [ nan 110.] [ 25. 45.] [ 55. nan]] The row index in shopping_carts is: Index(['bike', 'book', 'glasses', 'pants', 'watch'], dtype='object') The column index in shopping_carts is: Index(['Bob', 'Alice'], dtype='object') # We Create a DataFrame that only has Bob's data bob_shopping_cart = pd.DataFrame(items, columns=['Bob']) # We display bob_shopping_cart bob_shopping_cart .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bob bike 245 pants 25 watch 55 # We Create a DataFrame that only has selected items for both Alice and Bob sel_shopping_cart = pd.DataFrame(items, index = ['pants', 'book']) # We display sel_shopping_cart sel_shopping_cart .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Bob Alice pants 25.0 45 book NaN 40 # We Create a DataFrame that only has selected items for Alice alice_sel_shopping_cart = pd.DataFrame(items, index = ['glasses', 'bike'], columns = ['Alice']) # We display alice_sel_shopping_cart alice_sel_shopping_cart .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Alice glasses 110 bike 500 # We create a dictionary of lists (arrays) data = {'Integers' : [1,2,3], 'Floats' : [4.5, 8.2, 9.6]} # We create a DataFrame df = pd.DataFrame(data) # We display the DataFrame df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Integers Floats 0 1 4.5 1 2 8.2 2 3 9.6 # We create a DataFrame and provide the row index df = pd.DataFrame(data, index = ['label 1', 'label 2', 'label 3']) # We display the DataFrame df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Integers Floats label 1 1 4.5 label 2 2 8.2 label 3 3 9.6 # We create a list of Python dictionaries items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] # We create a DataFrame store_items = pd.DataFrame(items2) # We display the DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches 0 20 NaN 30 35 1 15 50.0 5 10 # We create a DataFrame and provide the row index store_items = pd.DataFrame(items2, index = ['store 1', 'store 2']) # We display the DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches store 1 20 NaN 30 35 store 2 15 50.0 5 10 # We print the store_items DataFrame print(store_items) # We access rows, columns and elements using labels print() print('How many bikes are in each store:\\n', store_items[['bikes']]) print() print('How many bikes and pants are in each store:\\n', store_items[['bikes', 'pants']]) print() print('What items are in Store 1:\\n', store_items.loc[['store 1']]) print() print('How many bikes are in Store 2:', store_items['bikes']['store 2']) bikes glasses pants watches store 1 20 NaN 30 35 store 2 15 50.0 5 10 How many bikes are in each store: bikes store 1 20 store 2 15 How many bikes and pants are in each store: bikes pants store 1 20 30 store 2 15 5 What items are in Store 1: bikes glasses pants watches store 1 20 NaN 30 35 How many bikes are in Store 2: 15 # We add a new column named shirts to our store_items DataFrame indicating the number of # shirts in stock at each store. We will put 15 shirts in store 1 and 2 shirts in store 2 store_items['shirts'] = [15,2] # We display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches shirts store 1 20 NaN 30 35 15 store 2 15 50.0 5 10 2 # We make a new column called suits by adding the number of shirts and pants store_items['suits'] = store_items['pants'] + store_items['shirts'] # We display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches shirts suits store 1 20 NaN 30 35 15 45 store 2 15 50.0 5 10 2 7 # We create a dictionary from a list of Python dictionaries that will number of items at the new store new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}] # We create new DataFrame with the new_items and provide and index labeled store 3 new_store = pd.DataFrame(new_items, index = ['store 3']) # We display the items at the new store new_store .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches store 3 20 4 30 35 # We append store 3 to our store_items DataFrame store_items = store_items.append(new_store, sort=False) # We display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches shirts suits store 1 20 NaN 30 35 15.0 45.0 store 2 15 50.0 5 10 2.0 7.0 store 3 20 4.0 30 35 NaN NaN # We add a new column using data from particular rows in the watches column store_items['new watches'] = store_items['watches'][1:] # We display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches shirts suits new watches store 1 20 NaN 30 35 15.0 45.0 NaN store 2 15 50.0 5 10 2.0 7.0 10.0 store 3 20 4.0 30 35 NaN NaN 35.0 # We insert a new column with label shoes right before the column with numerical index 4 store_items.insert(4, 'shoes', [8,5,0]) # we display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches shoes shirts suits new watches store 1 20 NaN 30 35 8 15.0 45.0 NaN store 2 15 50.0 5 10 5 2.0 7.0 10.0 store 3 20 4.0 30 35 0 NaN NaN 35.0 # We remove the new watches column store_items.pop('new watches') # we display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants watches shoes shirts suits store 1 20 NaN 30 35 8 15.0 45.0 store 2 15 50.0 5 10 5 2.0 7.0 store 3 20 4.0 30 35 0 NaN NaN # We remove the watches and shoes columns store_items = store_items.drop(['watches', 'shoes'], axis = 1) # we display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts suits store 1 20 NaN 30 15.0 45.0 store 2 15 50.0 5 2.0 7.0 store 3 20 4.0 30 NaN NaN # We remove the store 2 and store 1 rows store_items = store_items.drop(['store 2', 'store 1'], axis = 0) # we display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts suits store 3 20 4.0 30 NaN NaN # We change the column label bikes to hats store_items = store_items.rename(columns = {'bikes': 'hats'}) # we display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } hats glasses pants shirts suits store 3 20 4.0 30 NaN NaN # We change the row label from store 3 to last store store_items = store_items.rename(index = {'store 3': 'last store'}) # we display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } hats glasses pants shirts suits last store 20 4.0 30 NaN NaN # We change the row index to be the data in the pants column store_items = store_items.set_index('pants') # we display the modified DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } hats glasses shirts suits pants 30 20 4.0 NaN NaN # We create a list of Python dictionaries items2 = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes':8, 'suits':45}, {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5, 'shirts': 2, 'shoes':5, 'suits':7}, {'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes':10}] # We create a DataFrame and provide the row index store_items = pd.DataFrame(items2, index = ['store 1', 'store 2', 'store 3']) # We display the DataFrame store_items .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20 NaN 30 15.0 8 45.0 35 store 2 15 50.0 5 2.0 5 7.0 10 store 3 20 4.0 30 NaN 10 NaN 35 # We count the number of NaN values in store_items x = store_items.isnull().sum().sum() # We print x print('Number of NaN values in our DataFrame:', x) Number of NaN values in our DataFrame: 3 store_items.isnull() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 False True False False False False False store 2 False False False False False False False store 3 False False False True False True False store_items.isnull().sum() bikes 0 glasses 1 pants 0 shirts 1 shoes 0 suits 1 watches 0 dtype: int64 # We print the number of non-NaN values in our DataFrame print() print('Number of non-NaN values in the columns of our DataFrame:\\n', store_items.count()) Number of non-NaN values in the columns of our DataFrame: bikes 3 glasses 2 pants 3 shirts 2 shoes 3 suits 2 watches 3 dtype: int64 # We drop any rows with NaN values store_items.dropna(axis = 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 2 15 50.0 5 2.0 5 7.0 10 # We drop any columns with NaN values store_items.dropna(axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes pants shoes watches store 1 20 30 8 35 store 2 15 5 5 10 store 3 20 30 10 35 # We replace all NaN values with 0 store_items.fillna(0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20 0.0 30 15.0 8 45.0 35 store 2 15 50.0 5 2.0 5 7.0 10 store 3 20 4.0 30 0.0 10 0.0 35 # We replace NaN values with the previous value in the column store_items.fillna(method = 'ffill', axis = 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20 NaN 30 15.0 8 45.0 35 store 2 15 50.0 5 2.0 5 7.0 10 store 3 20 4.0 30 2.0 10 7.0 35 # We replace NaN values with the previous value in the row store_items.fillna(method = 'ffill', axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20.0 20.0 30.0 15.0 8.0 45.0 35.0 store 2 15.0 50.0 5.0 2.0 5.0 7.0 10.0 store 3 20.0 4.0 30.0 30.0 10.0 10.0 35.0 # We replace NaN values with the next value in the column store_items.fillna(method = 'backfill', axis = 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20 50.0 30 15.0 8 45.0 35 store 2 15 50.0 5 2.0 5 7.0 10 store 3 20 4.0 30 NaN 10 NaN 35 # We replace NaN values with the next value in the row store_items.fillna(method = 'backfill', axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20.0 30.0 30.0 15.0 8.0 45.0 35.0 store 2 15.0 50.0 5.0 2.0 5.0 7.0 10.0 store 3 20.0 4.0 30.0 10.0 10.0 35.0 35.0 # We replace NaN values by using linear interpolation using column values store_items.interpolate(method = 'linear', axis = 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20 NaN 30 15.0 8 45.0 35 store 2 15 50.0 5 2.0 5 7.0 10 store 3 20 4.0 30 2.0 10 7.0 35 # We replace NaN values by using linear interpolation using row values store_items.interpolate(method = 'linear', axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bikes glasses pants shirts shoes suits watches store 1 20.0 25.0 30.0 15.0 8.0 45.0 35.0 store 2 15.0 50.0 5.0 2.0 5.0 7.0 10.0 store 3 20.0 4.0 30.0 20.0 10.0 22.5 35.0 # Since we will be working with ratings, we will set the precision of our # dataframes to one decimal place. pd.set_option('precision', 1) # Create a Pandas DataFrame that contains the ratings some users have given to a # series of books. The ratings given are in the range from 1 to 5, with 5 being # the best score. The names of the books, the authors, and the ratings of each user # are given below: books = pd.Series(data = ['Great Expectations', 'Of Mice and Men', 'Romeo and Juliet', 'The Time Machine', 'Alice in Wonderland' ]) authors = pd.Series(data = ['Charles Dickens', 'John Steinbeck', 'William Shakespeare', ' H. G. Wells', 'Lewis Carroll' ]) user_1 = pd.Series(data = [3.2, np.nan ,2.5]) user_2 = pd.Series(data = [5., 1.3, 4.0, 3.8]) user_3 = pd.Series(data = [2.0, 2.3, np.nan, 4]) user_4 = pd.Series(data = [4, 3.5, 4, 5, 4.2]) # Users that have np.nan values means that the user has not yet rated that book. # Use the data above to create a Pandas DataFrame that has the following column # labels: 'Author', 'Book Title', 'User 1', 'User 2', 'User 3', 'User 4'. Let Pandas # automatically assign numerical row indices to the DataFrame. # Create a dictionary with the data given above dat = {'Author': authors, 'Book Title': books, 'User 1': user_1, 'User 2': user_2, 'User 3': user_3, 'User 4': user_4} # Use the dictionary to create a Pandas DataFrame book_ratings = pd.DataFrame(dat) # If you created the dictionary correctly you should have a Pandas DataFrame # that has column labels: 'Author', 'Book Title', 'User 1', 'User 2', 'User 3', # 'User 4' and row indices 0 through 4. # Now replace all the NaN values in your DataFrame with the average rating in # each column. Replace the NaN values in place. HINT: you can use the fillna() # function with the keyword inplace = True, to do this. Write your code below: book_ratings.fillna(book_ratings.mean(), inplace=True) print(book_ratings) Author Book Title User 1 User 2 User 3 User 4 0 Charles Dickens Great Expectations 3.2 5.0 2.0 4.0 1 John Steinbeck Of Mice and Men 2.9 1.3 2.3 3.5 2 William Shakespeare Romeo and Juliet 2.5 4.0 2.8 4.0 3 H. G. Wells The Time Machine 2.9 3.8 4.0 5.0 4 Lewis Carroll Alice in Wonderland 2.9 3.5 2.8 4.2","title":"Pandas"},{"location":"source/PracticeProblem/","text":"import pandas as pd filename = 'chicago.csv' # load data file into a dataframe df = pd.read_csv('data/chicago.csv') df['Start Time'].head() 0 2017-05-29 18:36:27 1 2017-06-12 19:00:33 2 2017-02-13 17:02:02 3 2017-04-24 18:39:45 4 2017-01-26 15:36:07 Name: Start Time, dtype: object # convert the Start Time column to datetime df['Start Time'] = pd.to_datetime(df['Start Time']) df['Start Time'].dt.date.head() 0 2017-05-29 1 2017-06-12 2 2017-02-13 3 2017-04-24 4 2017-01-26 Name: Start Time, dtype: object df['Start Time'].dt.time.head() 0 18:36:27 1 19:00:33 2 17:02:02 3 18:39:45 4 15:36:07 Name: Start Time, dtype: object # extract hour from the Start Time column to create an hour column df['hour'] = df['Start Time'].dt.hour df['hour'].head() 0 18 1 19 2 17 3 18 4 15 Name: hour, dtype: int64 # find the most popular hour print(df['hour'].mode()) popular_hour = df['hour'].mode()[0] print(popular_hour) 0 17 dtype: int64 17 # find the most common hour (from 0 to 23) popular_hour = df.groupby(['hour']).size().reset_index(name='count') print() print(popular_hour) print() print(popular_hour['count'].idxmax()) print() print(popular_hour.loc[popular_hour['count'].idxmax()]) print() print('Most Frequent Start Hour:', popular_hour.loc[popular_hour['count'].idxmax()].hour) hour count 0 0 4 1 5 2 2 6 14 3 7 23 4 8 22 5 9 20 6 10 16 7 11 19 8 12 31 9 13 26 10 14 21 11 15 25 12 16 29 13 17 53 14 18 36 15 19 26 16 20 11 17 21 11 18 22 7 19 23 4 13 hour 17 count 53 Name: 13, dtype: int64 Most Frequent Start Hour: 17 CITY_DATA = { 'chicago': 'chicago.csv', 'new york city': 'new_york_city.csv', 'washington': 'washington.csv' } def load_data(city, month, day): # load data file into a dataframe df = pd.read_csv('data/' + CITY_DATA[city]) # convert the Start Time column to datetime df['Start Time'] = pd.to_datetime(df['Start Time']) # extract month and day of week from Start Time to create new columns df['month'] = df['Start Time'].dt.month df['day_of_week'] = df['Start Time'].dt.weekday_name # filter by month if applicable if month != 'all': # use the index of the months list to get the corresponding int months = ['january', 'february', 'march', 'april', 'may', 'june'] month = months.index(month) + 1 # filter by month to create the new dataframe df = df[df['month'] == month] # filter by day of week if applicable if day != 'all': # filter by day of week to create the new dataframe df = df[df['day_of_week'] == day.title()] return df load_data('chicago', 'march', 'friday') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Start Time End Time Trip Duration Start Station End Station User Type Gender Birth Year month day_of_week 40 2017-03-24 13:06:37 2017-03-24 13:10:44 247 Broadway & Berwyn Ave Clark St & Berwyn Ave Subscriber Female 1961.0 3 Friday 59 2017-03-03 07:55:48 2017-03-03 07:57:41 113 Clark St & Chicago Ave Wells St & Huron St Subscriber Male 1981.0 3 Friday 68 2017-03-17 12:14:50 2017-03-17 12:22:38 468 Dearborn Pkwy & Delaware Pl State St & Randolph St Subscriber Female 1984.0 3 Friday 75 2017-03-10 13:40:54 2017-03-10 13:45:09 255 Clark St & Lake St Rush St & Hubbard St Subscriber Female 1983.0 3 Friday 83 2017-03-24 14:15:43 2017-03-24 14:27:04 681 Sheridan Rd & Lawrence Ave Broadway & Thorndale Ave Subscriber Male 1984.0 3 Friday 126 2017-03-24 12:39:19 2017-03-24 12:52:11 772 Michigan Ave & Oak St Cannon Dr & Fullerton Ave Subscriber Male 1993.0 3 Friday 224 2017-03-31 19:11:12 2017-03-31 19:18:53 461 Damen Ave & Cortland St Damen Ave & Pierce Ave Subscriber Male 1989.0 3 Friday 247 2017-03-10 08:21:05 2017-03-10 08:23:28 143 Damen Ave & Division St Ashland Ave & Division St Subscriber Male 1991.0 3 Friday 290 2017-03-24 10:55:53 2017-03-24 11:01:27 334 Wacker Dr & Washington St LaSalle St & Jackson Blvd Subscriber Male 1961.0 3 Friday 343 2017-03-17 17:51:31 2017-03-17 18:00:16 525 Milwaukee Ave & Grand Ave State St & Pearson St Subscriber Male 1989.0 3 Friday 348 2017-03-31 07:47:14 2017-03-31 07:55:38 504 Canal St & Madison St Wabash Ave & Adams St Subscriber Male 1953.0 3 Friday","title":"Practice Problem"},{"location":"source/assessing/","text":"Assessing and Building Intuition Once you have your data loaded into dataframes, Pandas makes a quick investigation of the data really easy. Let's explore some helpful methods for assessing and building intuition about a dataset. We can use the cancer data from before to help us. import pandas as pd df = pd.read_csv('cancer_data.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN 5 rows \u00d7 33 columns # this returns a tuple of the dimensions of the dataframe df.shape (569, 33) # this returns the datatypes of the columns df.dtypes id int64 diagnosis object radius_mean float64 texture_mean float64 perimeter_mean float64 area_mean float64 smoothness_mean float64 compactness_mean float64 concavity_mean float64 concave points_mean float64 symmetry_mean float64 fractal_dimension_mean float64 radius_se float64 texture_se float64 perimeter_se float64 area_se float64 smoothness_se float64 compactness_se float64 concavity_se float64 concave points_se float64 symmetry_se float64 fractal_dimension_se float64 radius_worst float64 texture_worst float64 perimeter_worst float64 area_worst float64 smoothness_worst float64 compactness_worst float64 concavity_worst float64 concave points_worst float64 symmetry_worst float64 fractal_dimension_worst float64 Unnamed: 32 float64 dtype: object # although the datatype for diagnosis appears to be object, further # investigation shows it's a string type(df['diagnosis'][0]) str Pandas actually stores [pointers](https://en.wikipedia.org/wiki/Pointer_(computer_programming) to strings in dataframes and series, which is why object instead of str appears as the datatype. Understanding this is not essential for data analysis - just know that strings will appear as objects in Pandas. # this displays a concise summary of the dataframe, # including the number of non-null values in each column df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB # this returns the number of unique values in each column df.nunique() id 569 diagnosis 2 radius_mean 456 texture_mean 479 perimeter_mean 522 area_mean 539 smoothness_mean 474 compactness_mean 537 concavity_mean 537 concave points_mean 542 symmetry_mean 432 fractal_dimension_mean 499 radius_se 540 texture_se 519 perimeter_se 533 area_se 528 smoothness_se 547 compactness_se 541 concavity_se 533 concave points_se 507 symmetry_se 498 fractal_dimension_se 545 radius_worst 457 texture_worst 511 perimeter_worst 514 area_worst 544 smoothness_worst 411 compactness_worst 529 concavity_worst 539 concave points_worst 492 symmetry_worst 500 fractal_dimension_worst 535 Unnamed: 32 0 dtype: int64 # this returns useful descriptive statistics for each column of data df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 count 5.690000e+02 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 ... 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 0.0 mean 3.037183e+07 14.127292 19.289649 91.969033 654.889104 0.096360 0.104341 0.088799 0.048919 0.181162 ... 25.677223 107.261213 880.583128 0.132369 0.254265 0.272188 0.114606 0.290076 0.083946 NaN std 1.250206e+08 3.524049 4.301036 24.298981 351.914129 0.014064 0.052813 0.079720 0.038803 0.027414 ... 6.146258 33.602542 569.356993 0.022832 0.157336 0.208624 0.065732 0.061867 0.018061 NaN min 8.670000e+03 6.981000 9.710000 43.790000 143.500000 0.052630 0.019380 0.000000 0.000000 0.106000 ... 12.020000 50.410000 185.200000 0.071170 0.027290 0.000000 0.000000 0.156500 0.055040 NaN 25% 8.692180e+05 11.700000 16.170000 75.170000 420.300000 0.086370 0.064920 0.029560 0.020310 0.161900 ... 21.080000 84.110000 515.300000 0.116600 0.147200 0.114500 0.064930 0.250400 0.071460 NaN 50% 9.060240e+05 13.370000 18.840000 86.240000 551.100000 0.095870 0.092630 0.061540 0.033500 0.179200 ... 25.410000 97.660000 686.500000 0.131300 0.211900 0.226700 0.099930 0.282200 0.080040 NaN 75% 8.813129e+06 15.780000 21.800000 104.100000 782.700000 0.105300 0.130400 0.130700 0.074000 0.195700 ... 29.720000 125.400000 1084.000000 0.146000 0.339100 0.382900 0.161400 0.317900 0.092080 NaN max 9.113205e+08 28.110000 39.280000 188.500000 2501.000000 0.163400 0.345400 0.426800 0.201200 0.304000 ... 49.540000 251.200000 4254.000000 0.222600 1.058000 1.252000 0.291000 0.663800 0.207500 NaN 8 rows \u00d7 32 columns # this returns the first few lines in our dataframe # by default, it returns the first five df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN 5 rows \u00d7 33 columns # although, you can specify however many rows you'd like returned df.head(12) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.30010 0.14710 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.26540 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.08690 0.07017 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.18600 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.19740 0.12790 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.24300 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.24140 0.10520 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.25750 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.19800 0.10430 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.16250 0.2364 0.07678 NaN 5 843786 M 12.45 15.70 82.57 477.1 0.12780 0.17000 0.15780 0.08089 ... 23.75 103.40 741.6 0.1791 0.5249 0.5355 0.17410 0.3985 0.12440 NaN 6 844359 M 18.25 19.98 119.60 1040.0 0.09463 0.10900 0.11270 0.07400 ... 27.66 153.20 1606.0 0.1442 0.2576 0.3784 0.19320 0.3063 0.08368 NaN 7 84458202 M 13.71 20.83 90.20 577.9 0.11890 0.16450 0.09366 0.05985 ... 28.14 110.60 897.0 0.1654 0.3682 0.2678 0.15560 0.3196 0.11510 NaN 8 844981 M 13.00 21.82 87.50 519.8 0.12730 0.19320 0.18590 0.09353 ... 30.73 106.20 739.3 0.1703 0.5401 0.5390 0.20600 0.4378 0.10720 NaN 9 84501001 M 12.46 24.04 83.97 475.9 0.11860 0.23960 0.22730 0.08543 ... 40.68 97.65 711.4 0.1853 1.0580 1.1050 0.22100 0.4366 0.20750 NaN 10 845636 M 16.02 23.24 102.70 797.8 0.08206 0.06669 0.03299 0.03323 ... 33.88 123.80 1150.0 0.1181 0.1551 0.1459 0.09975 0.2948 0.08452 NaN 11 84610002 M 15.78 17.89 103.60 781.0 0.09710 0.12920 0.09954 0.06606 ... 27.28 136.50 1299.0 0.1396 0.5609 0.3965 0.18100 0.3792 0.10480 NaN 12 rows \u00d7 33 columns # same thing applies to `.tail()` which returns the last few rows df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 567 927241 M 20.60 29.33 140.10 1265.0 0.11780 0.27700 0.3514 0.152 ... 39.42 184.60 1821.0 0.16500 0.86810 0.9387 0.265 0.4087 0.12400 NaN 568 92751 B 7.76 24.54 47.92 181.0 0.05263 0.04362 0.0000 0.000 ... 30.37 59.16 268.6 0.08996 0.06444 0.0000 0.000 0.2871 0.07039 NaN 2 rows \u00d7 33 columns Indexing and Selecting Data in Pandas Let's separate this dataframe into three new dataframes - one for each metric (mean, standard error, and maximum). To get the data for each dataframe, we need to select the id and diagnosis columns, as well as the ten columns for that metric. # View the index number and label for each column for i, v in enumerate(df.columns): print(i, v) 0 id 1 diagnosis 2 radius_mean 3 texture_mean 4 perimeter_mean 5 area_mean 6 smoothness_mean 7 compactness_mean 8 concavity_mean 9 concave points_mean 10 symmetry_mean 11 fractal_dimension_mean 12 radius_se 13 texture_se 14 perimeter_se 15 area_se 16 smoothness_se 17 compactness_se 18 concavity_se 19 concave points_se 20 symmetry_se 21 fractal_dimension_se 22 radius_worst 23 texture_worst 24 perimeter_worst 25 area_worst 26 smoothness_worst 27 compactness_worst 28 concavity_worst 29 concave points_worst 30 symmetry_worst 31 fractal_dimension_worst 32 Unnamed: 32 We can select data using loc and iloc , which you can read more about here . loc uses labels of rows or columns to select data, while iloc uses the index numbers. We'll use these to index the dataframe below. # select all the columns from 'id' to the last mean column df_means = df.loc[:,'id':'fractal_dimension_mean'] df_means.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 # repeat the step above using index numbers df_means = df.iloc[:,:11] df_means.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 Let's save the dataframe of means for later. df_means.to_csv('cancer_data_means.csv', index=False) Selecting Multiple Ranges in Pandas Selecting the columns for the mean dataframe was pretty straightforward - the columns we needed to select were all together ( id , diagnosis , and the mean columns). Now we run into a little issue when we try to do the same for the standard errors or maximum values. id and diagnosis are separated from the rest of the columns we need! We can't specify all of these in one range. First, try creating the standard error dataframe on your own to see why doing this with just loc and iloc is an issue. Then, use this stackoverflow link to learn how to select multiple ranges in Pandas and try it below. By the way, to figure this out myself, I just found this link by googling \"how to select multiple ranges df.iloc\" Hint: You may have to import a new package! # import import numpy as np # create the standard errors dataframe df_SE = df.iloc[:, np.r_[:2, 12:22]] # view the first few rows to confirm this was successful df_SE.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se 0 842302 M 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 1 842517 M 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 2 84300903 M 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 3 84348301 M 0.4956 1.1560 3.445 27.23 0.009110 0.07458 0.05661 0.01867 0.05963 0.009208 4 84358402 M 0.7572 0.7813 5.438 94.44 0.011490 0.02461 0.05688 0.01885 0.01756 0.005115","title":"Assessing"},{"location":"source/assessing/#assessing-and-building-intuition","text":"Once you have your data loaded into dataframes, Pandas makes a quick investigation of the data really easy. Let's explore some helpful methods for assessing and building intuition about a dataset. We can use the cancer data from before to help us. import pandas as pd df = pd.read_csv('cancer_data.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN 5 rows \u00d7 33 columns # this returns a tuple of the dimensions of the dataframe df.shape (569, 33) # this returns the datatypes of the columns df.dtypes id int64 diagnosis object radius_mean float64 texture_mean float64 perimeter_mean float64 area_mean float64 smoothness_mean float64 compactness_mean float64 concavity_mean float64 concave points_mean float64 symmetry_mean float64 fractal_dimension_mean float64 radius_se float64 texture_se float64 perimeter_se float64 area_se float64 smoothness_se float64 compactness_se float64 concavity_se float64 concave points_se float64 symmetry_se float64 fractal_dimension_se float64 radius_worst float64 texture_worst float64 perimeter_worst float64 area_worst float64 smoothness_worst float64 compactness_worst float64 concavity_worst float64 concave points_worst float64 symmetry_worst float64 fractal_dimension_worst float64 Unnamed: 32 float64 dtype: object # although the datatype for diagnosis appears to be object, further # investigation shows it's a string type(df['diagnosis'][0]) str Pandas actually stores [pointers](https://en.wikipedia.org/wiki/Pointer_(computer_programming) to strings in dataframes and series, which is why object instead of str appears as the datatype. Understanding this is not essential for data analysis - just know that strings will appear as objects in Pandas. # this displays a concise summary of the dataframe, # including the number of non-null values in each column df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB # this returns the number of unique values in each column df.nunique() id 569 diagnosis 2 radius_mean 456 texture_mean 479 perimeter_mean 522 area_mean 539 smoothness_mean 474 compactness_mean 537 concavity_mean 537 concave points_mean 542 symmetry_mean 432 fractal_dimension_mean 499 radius_se 540 texture_se 519 perimeter_se 533 area_se 528 smoothness_se 547 compactness_se 541 concavity_se 533 concave points_se 507 symmetry_se 498 fractal_dimension_se 545 radius_worst 457 texture_worst 511 perimeter_worst 514 area_worst 544 smoothness_worst 411 compactness_worst 529 concavity_worst 539 concave points_worst 492 symmetry_worst 500 fractal_dimension_worst 535 Unnamed: 32 0 dtype: int64 # this returns useful descriptive statistics for each column of data df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 count 5.690000e+02 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 ... 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 0.0 mean 3.037183e+07 14.127292 19.289649 91.969033 654.889104 0.096360 0.104341 0.088799 0.048919 0.181162 ... 25.677223 107.261213 880.583128 0.132369 0.254265 0.272188 0.114606 0.290076 0.083946 NaN std 1.250206e+08 3.524049 4.301036 24.298981 351.914129 0.014064 0.052813 0.079720 0.038803 0.027414 ... 6.146258 33.602542 569.356993 0.022832 0.157336 0.208624 0.065732 0.061867 0.018061 NaN min 8.670000e+03 6.981000 9.710000 43.790000 143.500000 0.052630 0.019380 0.000000 0.000000 0.106000 ... 12.020000 50.410000 185.200000 0.071170 0.027290 0.000000 0.000000 0.156500 0.055040 NaN 25% 8.692180e+05 11.700000 16.170000 75.170000 420.300000 0.086370 0.064920 0.029560 0.020310 0.161900 ... 21.080000 84.110000 515.300000 0.116600 0.147200 0.114500 0.064930 0.250400 0.071460 NaN 50% 9.060240e+05 13.370000 18.840000 86.240000 551.100000 0.095870 0.092630 0.061540 0.033500 0.179200 ... 25.410000 97.660000 686.500000 0.131300 0.211900 0.226700 0.099930 0.282200 0.080040 NaN 75% 8.813129e+06 15.780000 21.800000 104.100000 782.700000 0.105300 0.130400 0.130700 0.074000 0.195700 ... 29.720000 125.400000 1084.000000 0.146000 0.339100 0.382900 0.161400 0.317900 0.092080 NaN max 9.113205e+08 28.110000 39.280000 188.500000 2501.000000 0.163400 0.345400 0.426800 0.201200 0.304000 ... 49.540000 251.200000 4254.000000 0.222600 1.058000 1.252000 0.291000 0.663800 0.207500 NaN 8 rows \u00d7 32 columns # this returns the first few lines in our dataframe # by default, it returns the first five df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN 5 rows \u00d7 33 columns # although, you can specify however many rows you'd like returned df.head(12) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.30010 0.14710 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.26540 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.08690 0.07017 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.18600 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.19740 0.12790 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.24300 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.24140 0.10520 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.25750 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.19800 0.10430 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.16250 0.2364 0.07678 NaN 5 843786 M 12.45 15.70 82.57 477.1 0.12780 0.17000 0.15780 0.08089 ... 23.75 103.40 741.6 0.1791 0.5249 0.5355 0.17410 0.3985 0.12440 NaN 6 844359 M 18.25 19.98 119.60 1040.0 0.09463 0.10900 0.11270 0.07400 ... 27.66 153.20 1606.0 0.1442 0.2576 0.3784 0.19320 0.3063 0.08368 NaN 7 84458202 M 13.71 20.83 90.20 577.9 0.11890 0.16450 0.09366 0.05985 ... 28.14 110.60 897.0 0.1654 0.3682 0.2678 0.15560 0.3196 0.11510 NaN 8 844981 M 13.00 21.82 87.50 519.8 0.12730 0.19320 0.18590 0.09353 ... 30.73 106.20 739.3 0.1703 0.5401 0.5390 0.20600 0.4378 0.10720 NaN 9 84501001 M 12.46 24.04 83.97 475.9 0.11860 0.23960 0.22730 0.08543 ... 40.68 97.65 711.4 0.1853 1.0580 1.1050 0.22100 0.4366 0.20750 NaN 10 845636 M 16.02 23.24 102.70 797.8 0.08206 0.06669 0.03299 0.03323 ... 33.88 123.80 1150.0 0.1181 0.1551 0.1459 0.09975 0.2948 0.08452 NaN 11 84610002 M 15.78 17.89 103.60 781.0 0.09710 0.12920 0.09954 0.06606 ... 27.28 136.50 1299.0 0.1396 0.5609 0.3965 0.18100 0.3792 0.10480 NaN 12 rows \u00d7 33 columns # same thing applies to `.tail()` which returns the last few rows df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 567 927241 M 20.60 29.33 140.10 1265.0 0.11780 0.27700 0.3514 0.152 ... 39.42 184.60 1821.0 0.16500 0.86810 0.9387 0.265 0.4087 0.12400 NaN 568 92751 B 7.76 24.54 47.92 181.0 0.05263 0.04362 0.0000 0.000 ... 30.37 59.16 268.6 0.08996 0.06444 0.0000 0.000 0.2871 0.07039 NaN 2 rows \u00d7 33 columns","title":"Assessing and Building Intuition"},{"location":"source/assessing/#indexing-and-selecting-data-in-pandas","text":"Let's separate this dataframe into three new dataframes - one for each metric (mean, standard error, and maximum). To get the data for each dataframe, we need to select the id and diagnosis columns, as well as the ten columns for that metric. # View the index number and label for each column for i, v in enumerate(df.columns): print(i, v) 0 id 1 diagnosis 2 radius_mean 3 texture_mean 4 perimeter_mean 5 area_mean 6 smoothness_mean 7 compactness_mean 8 concavity_mean 9 concave points_mean 10 symmetry_mean 11 fractal_dimension_mean 12 radius_se 13 texture_se 14 perimeter_se 15 area_se 16 smoothness_se 17 compactness_se 18 concavity_se 19 concave points_se 20 symmetry_se 21 fractal_dimension_se 22 radius_worst 23 texture_worst 24 perimeter_worst 25 area_worst 26 smoothness_worst 27 compactness_worst 28 concavity_worst 29 concave points_worst 30 symmetry_worst 31 fractal_dimension_worst 32 Unnamed: 32 We can select data using loc and iloc , which you can read more about here . loc uses labels of rows or columns to select data, while iloc uses the index numbers. We'll use these to index the dataframe below. # select all the columns from 'id' to the last mean column df_means = df.loc[:,'id':'fractal_dimension_mean'] df_means.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 # repeat the step above using index numbers df_means = df.iloc[:,:11] df_means.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 Let's save the dataframe of means for later. df_means.to_csv('cancer_data_means.csv', index=False)","title":"Indexing and Selecting Data in Pandas"},{"location":"source/assessing/#selecting-multiple-ranges-in-pandas","text":"Selecting the columns for the mean dataframe was pretty straightforward - the columns we needed to select were all together ( id , diagnosis , and the mean columns). Now we run into a little issue when we try to do the same for the standard errors or maximum values. id and diagnosis are separated from the rest of the columns we need! We can't specify all of these in one range. First, try creating the standard error dataframe on your own to see why doing this with just loc and iloc is an issue. Then, use this stackoverflow link to learn how to select multiple ranges in Pandas and try it below. By the way, to figure this out myself, I just found this link by googling \"how to select multiple ranges df.iloc\" Hint: You may have to import a new package! # import import numpy as np # create the standard errors dataframe df_SE = df.iloc[:, np.r_[:2, 12:22]] # view the first few rows to confirm this was successful df_SE.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se 0 842302 M 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 1 842517 M 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 2 84300903 M 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 3 84348301 M 0.4956 1.1560 3.445 27.23 0.009110 0.07458 0.05661 0.01867 0.05963 0.009208 4 84358402 M 0.7572 0.7813 5.438 94.44 0.011490 0.02461 0.05688 0.01885 0.01756 0.005115","title":"Selecting Multiple Ranges in Pandas"},{"location":"source/cleaning/","text":"Cleaning Example Let's explore ways of fixing some common issues in data. Here's a toy dataset I created for this lesson. It has eleven instances of user-product interactions online, recording whether the user liked the product, how long they viewed the product, whether it was on a website or through a mobile app, and what time they started viewing the product. Can you spot any potential issues in this data? import pandas as pd df = pd.read_csv('product_view_data.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False NaN mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 4 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True NaN web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False NaN web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 11 entries, 0 to 10 Data columns (total 6 columns): user_id 11 non-null int64 product_id 11 non-null int64 liked 11 non-null bool view_duration 8 non-null float64 source 11 non-null object timestamp 11 non-null object dtypes: bool(1), float64(1), int64(2), object(2) memory usage: 531.0+ bytes Let's address three issues in this dataset that are quite common in the real world Missing data (in the view_duration columns) Duplicates (rows 3 and 4) Incorrect Datatypes ( timestamp is represented as a string) Filling null values In the dataframe above, you can see null values represented as NaN , which stands for not a number. From the output of df.info() you can see that there are 8 non-null values, which leaves 3 null values of the 11 entries. Missing data is an issue that should be handled differently depending on several factors, such as the reason those values are missing and whether the occurrences seem random. One way of handling them is [imputing](https://en.wikipedia.org/wiki/Imputation_(statistics) them with the mean. You can do this quickly and efficiently with a convenient function from Pandas. # get the mean of the column with missing data mean = df['view_duration'].mean() print(mean) 0.9383504902905304 # replace NaN values with the mean df['view_duration'].fillna(mean) 0 1.048242 1 1.688173 2 0.938350 3 0.194162 4 0.194162 5 0.493194 6 0.938350 7 0.675041 8 0.938350 9 0.101743 10 3.112086 Name: view_duration, dtype: float64 Let's look at the dataframe now - did this fix the problem? df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False NaN mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 4 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True NaN web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False NaN web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 Instead of making changes to the original column, it just returned a new column with the changes, which we didn't store anywhere. To keep the changes, make sure to assign it to the original like this: df['view_duration'] = df['view_duration'].fillna(mean) Alternatively, you can use an extra parameter as shown in the cell below. # replace NaN values and make changes in place df['view_duration'].fillna(mean, inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False 0.938350 mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 4 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True 0.938350 web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False 0.938350 web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 Success! Dropping Duplicates There are multiple reasons you may end up with duplicated data, like combined data sources or human error. Here's a simple scenario where two rows (3 and 4) are identical. This toy dataset is small enough for us to count visually. For bigger datasets, you can use this function to see which rows are duplicates. # By default, this marks duplicates as True excluding the first instance, # and it considers a row to be a duplicate only if the values in all # columns match. You can change both of these with its parameters. df.duplicated() 0 False 1 False 2 False 3 False 4 True 5 False 6 False 7 False 8 False 9 False 10 False dtype: bool # For larger datasets, it would probably be more helpful to get a count # of duplicates in the dataset like this sum(df.duplicated()) 1 # You can drop duplicated data with this function. Remember to use # assigned it to the original dataframe or use inplace to keep changes! df.drop_duplicates(inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False 0.938350 mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True 0.938350 web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False 0.938350 web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 Awesome! You can see we've dropped row 4 - the row marked as a duplicate. This was a simple situation where the entire row was identical. You could imagine duplicated data scenarios that are a bit more complicated. For example, let's say we had data on patients from a hospital. What happens when you come across two rows with the same patient id but different data on medical exam results? Do you combine them? Only keep the latest one? This is a situation you'd have to investigate more. For this scenario, you'd likely identify duplicates only based on the column recording the patient's id. You can use the subset paramater in duplicated() and drop_duplicates() to do this. Converting to datetime Incorrect datatypes is also a problem data analysts frequently come across. In this case, the timestamps are represented as strings instead of datetimes. This isn't critical, but datetimes are much more convenient to work with if you want to extract specific information from them or filter them more easily. # This shows the datatype of timestamp is not yet datetime df.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 10 entries, 0 to 10 Data columns (total 6 columns): user_id 10 non-null int64 product_id 10 non-null int64 liked 10 non-null bool view_duration 10 non-null float64 source 10 non-null object timestamp 10 non-null object dtypes: bool(1), float64(1), int64(2), object(2) memory usage: 490.0+ bytes # Let's use this awesome function to convert this column to datetime df['timestamp'] = pd.to_datetime(df['timestamp']) # Now we can see timestamp is represented as a datetime df.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 10 entries, 0 to 10 Data columns (total 6 columns): user_id 10 non-null int64 product_id 10 non-null int64 liked 10 non-null bool view_duration 10 non-null float64 source 10 non-null object timestamp 10 non-null datetime64[ns] dtypes: bool(1), datetime64[ns](1), float64(1), int64(2), object(1) memory usage: 490.0+ bytes Note that even if you save this to a csv file after making this change, it will be read as a string by default the next time you open it. You'll still have to convert it after opening the csv file, or use parameters like parse_dates in the read_csv() function. to_datetime() provides parameters for more options if the strings you have to parse are formatted unconventionally.","title":"Cleaning"},{"location":"source/cleaning/#cleaning-example","text":"Let's explore ways of fixing some common issues in data. Here's a toy dataset I created for this lesson. It has eleven instances of user-product interactions online, recording whether the user liked the product, how long they viewed the product, whether it was on a website or through a mobile app, and what time they started viewing the product. Can you spot any potential issues in this data? import pandas as pd df = pd.read_csv('product_view_data.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False NaN mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 4 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True NaN web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False NaN web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 11 entries, 0 to 10 Data columns (total 6 columns): user_id 11 non-null int64 product_id 11 non-null int64 liked 11 non-null bool view_duration 8 non-null float64 source 11 non-null object timestamp 11 non-null object dtypes: bool(1), float64(1), int64(2), object(2) memory usage: 531.0+ bytes","title":"Cleaning Example"},{"location":"source/cleaning/#lets-address-three-issues-in-this-dataset-that-are-quite-common-in-the-real-world","text":"Missing data (in the view_duration columns) Duplicates (rows 3 and 4) Incorrect Datatypes ( timestamp is represented as a string)","title":"Let's address three issues in this dataset that are quite common in the real world"},{"location":"source/cleaning/#filling-null-values","text":"In the dataframe above, you can see null values represented as NaN , which stands for not a number. From the output of df.info() you can see that there are 8 non-null values, which leaves 3 null values of the 11 entries. Missing data is an issue that should be handled differently depending on several factors, such as the reason those values are missing and whether the occurrences seem random. One way of handling them is [imputing](https://en.wikipedia.org/wiki/Imputation_(statistics) them with the mean. You can do this quickly and efficiently with a convenient function from Pandas. # get the mean of the column with missing data mean = df['view_duration'].mean() print(mean) 0.9383504902905304 # replace NaN values with the mean df['view_duration'].fillna(mean) 0 1.048242 1 1.688173 2 0.938350 3 0.194162 4 0.194162 5 0.493194 6 0.938350 7 0.675041 8 0.938350 9 0.101743 10 3.112086 Name: view_duration, dtype: float64 Let's look at the dataframe now - did this fix the problem? df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False NaN mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 4 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True NaN web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False NaN web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 Instead of making changes to the original column, it just returned a new column with the changes, which we didn't store anywhere. To keep the changes, make sure to assign it to the original like this: df['view_duration'] = df['view_duration'].fillna(mean) Alternatively, you can use an extra parameter as shown in the cell below. # replace NaN values and make changes in place df['view_duration'].fillna(mean, inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False 0.938350 mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 4 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True 0.938350 web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False 0.938350 web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 Success!","title":"Filling null values"},{"location":"source/cleaning/#dropping-duplicates","text":"There are multiple reasons you may end up with duplicated data, like combined data sources or human error. Here's a simple scenario where two rows (3 and 4) are identical. This toy dataset is small enough for us to count visually. For bigger datasets, you can use this function to see which rows are duplicates. # By default, this marks duplicates as True excluding the first instance, # and it considers a row to be a duplicate only if the values in all # columns match. You can change both of these with its parameters. df.duplicated() 0 False 1 False 2 False 3 False 4 True 5 False 6 False 7 False 8 False 9 False 10 False dtype: bool # For larger datasets, it would probably be more helpful to get a count # of duplicates in the dataset like this sum(df.duplicated()) 1 # You can drop duplicated data with this function. Remember to use # assigned it to the original dataframe or use inplace to keep changes! df.drop_duplicates(inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id product_id liked view_duration source timestamp 0 3987 997021 True 1.048242 web 2017-09-23 00:18:29.056895 1 6300 865003 True 1.688173 web 2017-09-21 02:20:22.022096 2 6451 712951 False 0.938350 mobile 2017-09-07 11:57:50.044683 3 7782 283235 True 0.194162 mobile 2017-09-17 03:48:20.019677 5 5700 587019 False 0.493194 web 2017-09-07 00:25:07.019097 6 3400 505123 True 0.938350 web 2017-09-07 13:53:21.008403 7 8403 459916 False 0.675041 mobile 2017-09-25 21:54:00.028323 8 8965 943363 False 0.938350 web 2017-09-17 15:12:21.059489 9 9693 787546 True 0.101743 web 2017-09-26 12:34:46.012559 10 4107 811855 False 3.112086 web 2017-09-01 10:50:07.042593 Awesome! You can see we've dropped row 4 - the row marked as a duplicate. This was a simple situation where the entire row was identical. You could imagine duplicated data scenarios that are a bit more complicated. For example, let's say we had data on patients from a hospital. What happens when you come across two rows with the same patient id but different data on medical exam results? Do you combine them? Only keep the latest one? This is a situation you'd have to investigate more. For this scenario, you'd likely identify duplicates only based on the column recording the patient's id. You can use the subset paramater in duplicated() and drop_duplicates() to do this.","title":"Dropping Duplicates"},{"location":"source/cleaning/#converting-to-datetime","text":"Incorrect datatypes is also a problem data analysts frequently come across. In this case, the timestamps are represented as strings instead of datetimes. This isn't critical, but datetimes are much more convenient to work with if you want to extract specific information from them or filter them more easily. # This shows the datatype of timestamp is not yet datetime df.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 10 entries, 0 to 10 Data columns (total 6 columns): user_id 10 non-null int64 product_id 10 non-null int64 liked 10 non-null bool view_duration 10 non-null float64 source 10 non-null object timestamp 10 non-null object dtypes: bool(1), float64(1), int64(2), object(2) memory usage: 490.0+ bytes # Let's use this awesome function to convert this column to datetime df['timestamp'] = pd.to_datetime(df['timestamp']) # Now we can see timestamp is represented as a datetime df.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 10 entries, 0 to 10 Data columns (total 6 columns): user_id 10 non-null int64 product_id 10 non-null int64 liked 10 non-null bool view_duration 10 non-null float64 source 10 non-null object timestamp 10 non-null datetime64[ns] dtypes: bool(1), datetime64[ns](1), float64(1), int64(2), object(1) memory usage: 490.0+ bytes Note that even if you save this to a csv file after making this change, it will be read as a string by default the next time you open it. You'll still have to convert it after opening the csv file, or use parameters like parse_dates in the read_csv() function. to_datetime() provides parameters for more options if the strings you have to parse are formatted unconventionally.","title":"Converting to datetime"},{"location":"source/communicate/","text":"Communicating Results Let's see how we can communicate findings with visualizations with the census income data. import pandas as pd %matplotlib inline df_census = pd.read_csv('census_income_data.csv') Let's create two dataframes to separate people who make above and below 50K. df_a = df_census[df_census['income'] == ' >50K'] df_b = df_census[df_census['income'] == ' <=50K'] We can use bar graphs to compare the education levels reached in both groups. ind = df_a['education'].value_counts().index df_a['education'].value_counts()[ind].plot(kind='bar'); df_b['education'].value_counts()[ind].plot(kind='bar'); Notice the same index was used to keep the labels of the bar charts in the same order. Next, let's plot pie charts to compare what workclasses dominate in each group. ind = df_a['workclass'].value_counts().index df_a['workclass'].value_counts()[ind].plot(kind='pie', figsize=(8, 8)); df_b['workclass'].value_counts()[ind].plot(kind='pie', figsize=(8, 8)); Next, let's use histograms to plot the distribution of ages for each group. df_a['age'].hist(); df_b['age'].hist(); df_a['age'].describe() count 7841.000000 mean 44.249841 std 10.519028 min 19.000000 25% 36.000000 50% 44.000000 75% 51.000000 max 90.000000 Name: age, dtype: float64 df_b['age'].describe() count 24720.000000 mean 36.783738 std 14.020088 min 17.000000 25% 25.000000 50% 34.000000 75% 46.000000 max 90.000000 Name: age, dtype: float64","title":"Communicate"},{"location":"source/communicate/#communicating-results","text":"Let's see how we can communicate findings with visualizations with the census income data. import pandas as pd %matplotlib inline df_census = pd.read_csv('census_income_data.csv') Let's create two dataframes to separate people who make above and below 50K. df_a = df_census[df_census['income'] == ' >50K'] df_b = df_census[df_census['income'] == ' <=50K'] We can use bar graphs to compare the education levels reached in both groups. ind = df_a['education'].value_counts().index df_a['education'].value_counts()[ind].plot(kind='bar'); df_b['education'].value_counts()[ind].plot(kind='bar'); Notice the same index was used to keep the labels of the bar charts in the same order. Next, let's plot pie charts to compare what workclasses dominate in each group. ind = df_a['workclass'].value_counts().index df_a['workclass'].value_counts()[ind].plot(kind='pie', figsize=(8, 8)); df_b['workclass'].value_counts()[ind].plot(kind='pie', figsize=(8, 8)); Next, let's use histograms to plot the distribution of ages for each group. df_a['age'].hist(); df_b['age'].hist(); df_a['age'].describe() count 7841.000000 mean 44.249841 std 10.519028 min 19.000000 25% 36.000000 50% 44.000000 75% 51.000000 max 90.000000 Name: age, dtype: float64 df_b['age'].describe() count 24720.000000 mean 36.783738 std 14.020088 min 17.000000 25% 25.000000 50% 34.000000 75% 46.000000 max 90.000000 Name: age, dtype: float64","title":"Communicating Results"},{"location":"source/conclusions/","text":"Drawing Conclusions Example Let's address a question we posed with this cancer data earlier in the lesson - does the size of a tumor affect its malignancy? We can use descriptive statistics and visualizations to help us. import pandas as pd df = pd.read_csv('cancer_data_edited.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius texture perimeter area smoothness compactness concavity concave_points symmetry fractal_dimension 0 842302 M 17.99 19.293431 122.80 1001.0 0.118400 0.27760 0.3001 0.14710 0.2419 0.07871 1 842517 M 20.57 17.770000 132.90 1326.0 0.084740 0.07864 0.0869 0.07017 0.1812 0.05667 2 84300903 M 19.69 21.250000 130.00 1203.0 0.109600 0.15990 0.1974 0.12790 0.2069 0.05999 3 84348301 M 11.42 20.380000 77.58 386.1 0.096087 0.28390 0.2414 0.10520 0.2597 0.09744 4 84358402 M 20.29 14.340000 135.10 1297.0 0.100300 0.13280 0.1980 0.10430 0.1809 0.05883 Selecting Data with Masks In order to do this analysis, we'd ideally compare sizes of tumors that are benign and malignant. We can use masks to select all rows in the dataframe that were diagnosed as malignant. # Create new dataframe with only malignant tumors df_m = df[df['diagnosis'] == 'M'] df_m.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius texture perimeter area smoothness compactness concavity concave_points symmetry fractal_dimension 0 842302 M 17.99 19.293431 122.80 1001.0 0.118400 0.27760 0.3001 0.14710 0.2419 0.07871 1 842517 M 20.57 17.770000 132.90 1326.0 0.084740 0.07864 0.0869 0.07017 0.1812 0.05667 2 84300903 M 19.69 21.250000 130.00 1203.0 0.109600 0.15990 0.1974 0.12790 0.2069 0.05999 3 84348301 M 11.42 20.380000 77.58 386.1 0.096087 0.28390 0.2414 0.10520 0.2597 0.09744 4 84358402 M 20.29 14.340000 135.10 1297.0 0.100300 0.13280 0.1980 0.10430 0.1809 0.05883 Let's break down how we got df_m . df['diagnosis'] == 'M' returns a Pandas Series of booleans indicating whether the value in the diagnosis columns is equal to M . mask = df['diagnosis'] == 'M' print(mask) 0 True 1 True 2 True 3 True 4 True 5 True 6 True 7 True 8 True 9 True 10 True 11 True 12 True 13 True 14 True 15 True 16 True 17 True 18 True 19 False 20 False 21 False 22 True 23 True 24 True 25 True 26 True 27 True 28 True 29 True ... 534 False 535 False 536 False 537 False 538 False 539 False 540 False 541 False 542 False 543 False 544 False 545 False 546 False 547 False 548 False 549 False 550 False 551 False 552 False 553 False 554 False 555 False 556 False 557 True 558 True 559 True 560 True 561 True 562 True 563 False Name: diagnosis, Length: 564, dtype: bool And indexing the dataframe with this mask will return all rows where the value in mask is True (ie. where diagnosis == 'M' ). df_m = df[mask] df_m .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius texture perimeter area smoothness compactness concavity concave_points symmetry fractal_dimension 0 842302 M 17.99 19.293431 122.80 1001.0 0.118400 0.27760 0.30010 0.14710 0.241900 0.07871 1 842517 M 20.57 17.770000 132.90 1326.0 0.084740 0.07864 0.08690 0.07017 0.181200 0.05667 2 84300903 M 19.69 21.250000 130.00 1203.0 0.109600 0.15990 0.19740 0.12790 0.206900 0.05999 3 84348301 M 11.42 20.380000 77.58 386.1 0.096087 0.28390 0.24140 0.10520 0.259700 0.09744 4 84358402 M 20.29 14.340000 135.10 1297.0 0.100300 0.13280 0.19800 0.10430 0.180900 0.05883 5 843786 M 12.45 15.700000 82.57 477.1 0.127800 0.17000 0.15780 0.08089 0.208700 0.07613 6 844359 M 18.25 19.980000 119.60 1040.0 0.094630 0.10900 0.11270 0.07400 0.181091 0.05742 7 84458202 M 13.71 20.830000 90.20 577.9 0.118900 0.16450 0.09366 0.05985 0.219600 0.07451 8 844981 M 13.00 21.820000 87.50 519.8 0.127300 0.19320 0.18590 0.09353 0.235000 0.07389 9 84501001 M 12.46 24.040000 83.97 475.9 0.118600 0.23960 0.22730 0.08543 0.203000 0.08243 10 845636 M 16.02 23.240000 102.70 797.8 0.082060 0.06669 0.03299 0.03323 0.152800 0.05697 11 84610002 M 15.78 17.890000 103.60 781.0 0.097100 0.12920 0.09954 0.06606 0.184200 0.06082 12 846226 M 19.17 24.800000 132.40 1123.0 0.097400 0.24580 0.20650 0.11180 0.239700 0.07800 13 846381 M 15.85 19.293431 103.70 782.7 0.084010 0.10020 0.09938 0.05364 0.184700 0.05338 14 84667401 M 13.73 22.610000 93.60 578.3 0.113100 0.22930 0.21280 0.08025 0.206900 0.07682 15 84799002 M 14.54 27.540000 96.73 658.8 0.113900 0.15950 0.16390 0.07364 0.181091 0.07077 16 848406 M 14.68 20.130000 94.74 684.5 0.098670 0.07200 0.07395 0.05259 0.158600 0.05922 17 84862001 M 16.13 19.293431 108.10 798.8 0.117000 0.20220 0.17220 0.10280 0.216400 0.07356 18 849014 M 19.81 22.150000 130.00 1260.0 0.098310 0.10270 0.14790 0.09498 0.158200 0.05395 22 8511133 M 15.34 14.260000 102.50 704.4 0.107300 0.21350 0.20770 0.09756 0.252100 0.07032 23 851509 M 21.16 23.040000 137.20 1404.0 0.094280 0.10220 0.10970 0.08632 0.176900 0.05278 24 852552 M 16.65 21.380000 110.00 904.6 0.112100 0.14570 0.15250 0.09170 0.199500 0.06330 25 852631 M 17.14 16.400000 116.00 912.7 0.096087 0.22760 0.22290 0.14010 0.304000 0.07413 26 852763 M 14.58 21.530000 97.41 644.8 0.105400 0.18680 0.14250 0.08783 0.225200 0.06924 27 852781 M 18.61 20.250000 122.10 1094.0 0.094400 0.10660 0.14900 0.07731 0.169700 0.05699 28 852973 M 15.30 25.270000 102.40 732.4 0.108200 0.16970 0.16830 0.08751 0.192600 0.06540 29 853201 M 17.57 15.050000 115.00 955.1 0.096087 0.11570 0.09875 0.07953 0.173900 0.06149 30 853401 M 18.63 25.110000 124.80 1088.0 0.096087 0.18870 0.23190 0.12440 0.218300 0.06197 31 853612 M 11.84 18.700000 77.93 440.6 0.110900 0.15160 0.12180 0.05182 0.230100 0.07799 32 85382601 M 17.02 19.293431 112.80 899.3 0.119700 0.14960 0.24170 0.12030 0.224800 0.06382 ... ... ... ... ... ... ... ... ... ... ... ... ... 438 909445 M 17.27 25.420000 112.40 928.8 0.083310 0.11090 0.12040 0.05736 0.146700 0.05407 441 9110127 M 18.03 16.850000 117.50 990.0 0.089470 0.12320 0.10900 0.06254 0.172000 0.05780 443 9110732 M 17.75 28.030000 117.30 981.6 0.099970 0.13140 0.16980 0.08293 0.171300 0.05916 446 911157302 M 21.10 20.520000 138.10 1384.0 0.096840 0.11750 0.15720 0.11550 0.155400 0.05661 448 9111805 M 19.59 25.000000 127.70 1191.0 0.103200 0.09871 0.16550 0.09063 0.166300 0.05391 457 911296201 M 17.08 27.150000 111.20 930.9 0.098980 0.11100 0.10070 0.06431 0.179300 0.06281 458 911296202 M 27.42 26.270000 186.90 2501.0 0.108400 0.19880 0.36350 0.16890 0.206100 0.05623 465 9113538 M 17.60 23.330000 119.00 980.5 0.092890 0.20040 0.21360 0.10020 0.169600 0.07369 476 911916 M 16.25 19.510000 109.80 815.8 0.102600 0.18930 0.22360 0.09194 0.215100 0.06578 484 913505 M 19.44 18.820000 128.10 1167.0 0.108900 0.14480 0.22560 0.11940 0.182300 0.06115 488 914062 M 18.01 20.560000 118.40 1007.0 0.100100 0.12890 0.11700 0.07762 0.211600 0.06077 494 914769 M 18.49 17.520000 121.30 1068.0 0.101200 0.13170 0.14910 0.09183 0.183200 0.06697 495 91485 M 20.59 21.240000 137.80 1320.0 0.096087 0.16440 0.21880 0.11210 0.184800 0.06222 497 91504 M 13.82 24.490000 92.33 595.9 0.116200 0.16810 0.13570 0.06759 0.227500 0.07237 499 915143 M 23.09 19.830000 152.10 1682.0 0.093420 0.12750 0.16760 0.10030 0.150500 0.05484 505 915460 M 15.46 23.950000 103.80 731.3 0.118300 0.18700 0.20300 0.08520 0.180700 0.07083 508 915691 M 13.40 20.520000 88.64 556.7 0.110600 0.14690 0.14450 0.08172 0.211600 0.07325 510 91594602 M 15.05 19.070000 97.26 701.9 0.092150 0.08597 0.07486 0.04335 0.156100 0.05915 512 916799 M 18.31 20.580000 120.80 1052.0 0.106800 0.12480 0.15690 0.09451 0.186000 0.05941 513 916838 M 19.89 20.260000 130.50 1214.0 0.103700 0.13100 0.14110 0.09431 0.180200 0.06188 517 91762702 M 24.63 21.600000 165.50 1841.0 0.103000 0.21060 0.23100 0.14710 0.199100 0.06739 529 91930402 M 20.47 20.670000 134.70 1299.0 0.096087 0.13130 0.15230 0.10150 0.216600 0.05419 531 919555 M 20.55 20.860000 137.80 1308.0 0.104600 0.17390 0.20850 0.13220 0.212700 0.06251 532 91979701 M 14.27 22.550000 93.77 629.8 0.103800 0.11540 0.14630 0.06139 0.192600 0.05982 557 925622 M 15.22 30.620000 103.40 716.9 0.104800 0.20870 0.25500 0.09429 0.181091 0.07152 558 926125 M 20.92 25.090000 143.00 1347.0 0.109900 0.22360 0.31740 0.14740 0.214900 0.06879 559 926424 M 21.56 22.390000 142.00 1479.0 0.111000 0.11590 0.24390 0.13890 0.172600 0.05623 560 926682 M 20.13 28.250000 131.20 1261.0 0.097800 0.10340 0.14400 0.09791 0.175200 0.05533 561 926954 M 16.60 28.080000 108.30 858.1 0.084550 0.10230 0.09251 0.05302 0.159000 0.05648 562 927241 M 20.60 29.330000 140.10 1265.0 0.117800 0.27700 0.35140 0.15200 0.239700 0.07016 210 rows \u00d7 12 columns Now that we have all the malignant tumors together in a dataframe, let's see summary statistics about the area feature, which offers a good metric for size. # Display summary statistics for area of malignant tumors df_m['area'].describe() count 210.000000 mean 976.582857 std 365.494289 min 361.600000 25% 706.850000 50% 932.000000 75% 1200.750000 max 2501.000000 Name: area, dtype: float64 Let's do the same for all the benign tumors. # Create new dataframe with only benign tumors df_b = df[df['diagnosis'] == 'B'] # Display summary statistics for area of benign tumors df_b['area'].describe() count 354.000000 mean 462.712429 std 134.769158 min 143.500000 25% 374.975000 50% 458.150000 75% 551.550000 max 992.100000 Name: area, dtype: float64 print('The mean area of malignant tumors is {0:.4f} while that of benign \\ tumors is {1:.4f}.'.format(df_m['area'].mean(), df_b['area'].mean())) The mean area of malignant tumors is 976.5829 while that of benign tumors is 462.7124. Although summary statistics like the mean are helpful, it would be nice to be able to compare the distributions of the areas of malignant and benign tumors visually. Let's see a simple example of using matplotlib to create histograms for both distributions on the same plot. (We'll learn how to use matplotlib in the next lesson.) import matplotlib.pyplot as plt %matplotlib inline # Plot histogram of benign and malignant tumor areas on the same axes fig, ax = plt.subplots(figsize=(8, 6)) ax.hist(df_b['area'], alpha=0.5, label='benign') ax.hist(df_m['area'], alpha=0.5, label='malignant') ax.set_title('Distributions of Benign and Malignant Tumor Areas') ax.set_xlabel('Area') ax.set_ylabel('Count') ax.legend(loc='upper right') plt.show() The visual above suggests that there is a difference between the distribution of areas for benign and malignant tumors. We don't yet have the tools to conclude that these distributions are different or whether the size definitely affects a tumor's malignancy. However, we can observe from summary statistics and these histograms that malignant tumors are generally larger in size than benign tumors.","title":"Conclusion"},{"location":"source/conclusions/#drawing-conclusions-example","text":"Let's address a question we posed with this cancer data earlier in the lesson - does the size of a tumor affect its malignancy? We can use descriptive statistics and visualizations to help us. import pandas as pd df = pd.read_csv('cancer_data_edited.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius texture perimeter area smoothness compactness concavity concave_points symmetry fractal_dimension 0 842302 M 17.99 19.293431 122.80 1001.0 0.118400 0.27760 0.3001 0.14710 0.2419 0.07871 1 842517 M 20.57 17.770000 132.90 1326.0 0.084740 0.07864 0.0869 0.07017 0.1812 0.05667 2 84300903 M 19.69 21.250000 130.00 1203.0 0.109600 0.15990 0.1974 0.12790 0.2069 0.05999 3 84348301 M 11.42 20.380000 77.58 386.1 0.096087 0.28390 0.2414 0.10520 0.2597 0.09744 4 84358402 M 20.29 14.340000 135.10 1297.0 0.100300 0.13280 0.1980 0.10430 0.1809 0.05883","title":"Drawing Conclusions Example"},{"location":"source/conclusions/#selecting-data-with-masks","text":"In order to do this analysis, we'd ideally compare sizes of tumors that are benign and malignant. We can use masks to select all rows in the dataframe that were diagnosed as malignant. # Create new dataframe with only malignant tumors df_m = df[df['diagnosis'] == 'M'] df_m.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius texture perimeter area smoothness compactness concavity concave_points symmetry fractal_dimension 0 842302 M 17.99 19.293431 122.80 1001.0 0.118400 0.27760 0.3001 0.14710 0.2419 0.07871 1 842517 M 20.57 17.770000 132.90 1326.0 0.084740 0.07864 0.0869 0.07017 0.1812 0.05667 2 84300903 M 19.69 21.250000 130.00 1203.0 0.109600 0.15990 0.1974 0.12790 0.2069 0.05999 3 84348301 M 11.42 20.380000 77.58 386.1 0.096087 0.28390 0.2414 0.10520 0.2597 0.09744 4 84358402 M 20.29 14.340000 135.10 1297.0 0.100300 0.13280 0.1980 0.10430 0.1809 0.05883 Let's break down how we got df_m . df['diagnosis'] == 'M' returns a Pandas Series of booleans indicating whether the value in the diagnosis columns is equal to M . mask = df['diagnosis'] == 'M' print(mask) 0 True 1 True 2 True 3 True 4 True 5 True 6 True 7 True 8 True 9 True 10 True 11 True 12 True 13 True 14 True 15 True 16 True 17 True 18 True 19 False 20 False 21 False 22 True 23 True 24 True 25 True 26 True 27 True 28 True 29 True ... 534 False 535 False 536 False 537 False 538 False 539 False 540 False 541 False 542 False 543 False 544 False 545 False 546 False 547 False 548 False 549 False 550 False 551 False 552 False 553 False 554 False 555 False 556 False 557 True 558 True 559 True 560 True 561 True 562 True 563 False Name: diagnosis, Length: 564, dtype: bool And indexing the dataframe with this mask will return all rows where the value in mask is True (ie. where diagnosis == 'M' ). df_m = df[mask] df_m .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius texture perimeter area smoothness compactness concavity concave_points symmetry fractal_dimension 0 842302 M 17.99 19.293431 122.80 1001.0 0.118400 0.27760 0.30010 0.14710 0.241900 0.07871 1 842517 M 20.57 17.770000 132.90 1326.0 0.084740 0.07864 0.08690 0.07017 0.181200 0.05667 2 84300903 M 19.69 21.250000 130.00 1203.0 0.109600 0.15990 0.19740 0.12790 0.206900 0.05999 3 84348301 M 11.42 20.380000 77.58 386.1 0.096087 0.28390 0.24140 0.10520 0.259700 0.09744 4 84358402 M 20.29 14.340000 135.10 1297.0 0.100300 0.13280 0.19800 0.10430 0.180900 0.05883 5 843786 M 12.45 15.700000 82.57 477.1 0.127800 0.17000 0.15780 0.08089 0.208700 0.07613 6 844359 M 18.25 19.980000 119.60 1040.0 0.094630 0.10900 0.11270 0.07400 0.181091 0.05742 7 84458202 M 13.71 20.830000 90.20 577.9 0.118900 0.16450 0.09366 0.05985 0.219600 0.07451 8 844981 M 13.00 21.820000 87.50 519.8 0.127300 0.19320 0.18590 0.09353 0.235000 0.07389 9 84501001 M 12.46 24.040000 83.97 475.9 0.118600 0.23960 0.22730 0.08543 0.203000 0.08243 10 845636 M 16.02 23.240000 102.70 797.8 0.082060 0.06669 0.03299 0.03323 0.152800 0.05697 11 84610002 M 15.78 17.890000 103.60 781.0 0.097100 0.12920 0.09954 0.06606 0.184200 0.06082 12 846226 M 19.17 24.800000 132.40 1123.0 0.097400 0.24580 0.20650 0.11180 0.239700 0.07800 13 846381 M 15.85 19.293431 103.70 782.7 0.084010 0.10020 0.09938 0.05364 0.184700 0.05338 14 84667401 M 13.73 22.610000 93.60 578.3 0.113100 0.22930 0.21280 0.08025 0.206900 0.07682 15 84799002 M 14.54 27.540000 96.73 658.8 0.113900 0.15950 0.16390 0.07364 0.181091 0.07077 16 848406 M 14.68 20.130000 94.74 684.5 0.098670 0.07200 0.07395 0.05259 0.158600 0.05922 17 84862001 M 16.13 19.293431 108.10 798.8 0.117000 0.20220 0.17220 0.10280 0.216400 0.07356 18 849014 M 19.81 22.150000 130.00 1260.0 0.098310 0.10270 0.14790 0.09498 0.158200 0.05395 22 8511133 M 15.34 14.260000 102.50 704.4 0.107300 0.21350 0.20770 0.09756 0.252100 0.07032 23 851509 M 21.16 23.040000 137.20 1404.0 0.094280 0.10220 0.10970 0.08632 0.176900 0.05278 24 852552 M 16.65 21.380000 110.00 904.6 0.112100 0.14570 0.15250 0.09170 0.199500 0.06330 25 852631 M 17.14 16.400000 116.00 912.7 0.096087 0.22760 0.22290 0.14010 0.304000 0.07413 26 852763 M 14.58 21.530000 97.41 644.8 0.105400 0.18680 0.14250 0.08783 0.225200 0.06924 27 852781 M 18.61 20.250000 122.10 1094.0 0.094400 0.10660 0.14900 0.07731 0.169700 0.05699 28 852973 M 15.30 25.270000 102.40 732.4 0.108200 0.16970 0.16830 0.08751 0.192600 0.06540 29 853201 M 17.57 15.050000 115.00 955.1 0.096087 0.11570 0.09875 0.07953 0.173900 0.06149 30 853401 M 18.63 25.110000 124.80 1088.0 0.096087 0.18870 0.23190 0.12440 0.218300 0.06197 31 853612 M 11.84 18.700000 77.93 440.6 0.110900 0.15160 0.12180 0.05182 0.230100 0.07799 32 85382601 M 17.02 19.293431 112.80 899.3 0.119700 0.14960 0.24170 0.12030 0.224800 0.06382 ... ... ... ... ... ... ... ... ... ... ... ... ... 438 909445 M 17.27 25.420000 112.40 928.8 0.083310 0.11090 0.12040 0.05736 0.146700 0.05407 441 9110127 M 18.03 16.850000 117.50 990.0 0.089470 0.12320 0.10900 0.06254 0.172000 0.05780 443 9110732 M 17.75 28.030000 117.30 981.6 0.099970 0.13140 0.16980 0.08293 0.171300 0.05916 446 911157302 M 21.10 20.520000 138.10 1384.0 0.096840 0.11750 0.15720 0.11550 0.155400 0.05661 448 9111805 M 19.59 25.000000 127.70 1191.0 0.103200 0.09871 0.16550 0.09063 0.166300 0.05391 457 911296201 M 17.08 27.150000 111.20 930.9 0.098980 0.11100 0.10070 0.06431 0.179300 0.06281 458 911296202 M 27.42 26.270000 186.90 2501.0 0.108400 0.19880 0.36350 0.16890 0.206100 0.05623 465 9113538 M 17.60 23.330000 119.00 980.5 0.092890 0.20040 0.21360 0.10020 0.169600 0.07369 476 911916 M 16.25 19.510000 109.80 815.8 0.102600 0.18930 0.22360 0.09194 0.215100 0.06578 484 913505 M 19.44 18.820000 128.10 1167.0 0.108900 0.14480 0.22560 0.11940 0.182300 0.06115 488 914062 M 18.01 20.560000 118.40 1007.0 0.100100 0.12890 0.11700 0.07762 0.211600 0.06077 494 914769 M 18.49 17.520000 121.30 1068.0 0.101200 0.13170 0.14910 0.09183 0.183200 0.06697 495 91485 M 20.59 21.240000 137.80 1320.0 0.096087 0.16440 0.21880 0.11210 0.184800 0.06222 497 91504 M 13.82 24.490000 92.33 595.9 0.116200 0.16810 0.13570 0.06759 0.227500 0.07237 499 915143 M 23.09 19.830000 152.10 1682.0 0.093420 0.12750 0.16760 0.10030 0.150500 0.05484 505 915460 M 15.46 23.950000 103.80 731.3 0.118300 0.18700 0.20300 0.08520 0.180700 0.07083 508 915691 M 13.40 20.520000 88.64 556.7 0.110600 0.14690 0.14450 0.08172 0.211600 0.07325 510 91594602 M 15.05 19.070000 97.26 701.9 0.092150 0.08597 0.07486 0.04335 0.156100 0.05915 512 916799 M 18.31 20.580000 120.80 1052.0 0.106800 0.12480 0.15690 0.09451 0.186000 0.05941 513 916838 M 19.89 20.260000 130.50 1214.0 0.103700 0.13100 0.14110 0.09431 0.180200 0.06188 517 91762702 M 24.63 21.600000 165.50 1841.0 0.103000 0.21060 0.23100 0.14710 0.199100 0.06739 529 91930402 M 20.47 20.670000 134.70 1299.0 0.096087 0.13130 0.15230 0.10150 0.216600 0.05419 531 919555 M 20.55 20.860000 137.80 1308.0 0.104600 0.17390 0.20850 0.13220 0.212700 0.06251 532 91979701 M 14.27 22.550000 93.77 629.8 0.103800 0.11540 0.14630 0.06139 0.192600 0.05982 557 925622 M 15.22 30.620000 103.40 716.9 0.104800 0.20870 0.25500 0.09429 0.181091 0.07152 558 926125 M 20.92 25.090000 143.00 1347.0 0.109900 0.22360 0.31740 0.14740 0.214900 0.06879 559 926424 M 21.56 22.390000 142.00 1479.0 0.111000 0.11590 0.24390 0.13890 0.172600 0.05623 560 926682 M 20.13 28.250000 131.20 1261.0 0.097800 0.10340 0.14400 0.09791 0.175200 0.05533 561 926954 M 16.60 28.080000 108.30 858.1 0.084550 0.10230 0.09251 0.05302 0.159000 0.05648 562 927241 M 20.60 29.330000 140.10 1265.0 0.117800 0.27700 0.35140 0.15200 0.239700 0.07016 210 rows \u00d7 12 columns Now that we have all the malignant tumors together in a dataframe, let's see summary statistics about the area feature, which offers a good metric for size. # Display summary statistics for area of malignant tumors df_m['area'].describe() count 210.000000 mean 976.582857 std 365.494289 min 361.600000 25% 706.850000 50% 932.000000 75% 1200.750000 max 2501.000000 Name: area, dtype: float64 Let's do the same for all the benign tumors. # Create new dataframe with only benign tumors df_b = df[df['diagnosis'] == 'B'] # Display summary statistics for area of benign tumors df_b['area'].describe() count 354.000000 mean 462.712429 std 134.769158 min 143.500000 25% 374.975000 50% 458.150000 75% 551.550000 max 992.100000 Name: area, dtype: float64 print('The mean area of malignant tumors is {0:.4f} while that of benign \\ tumors is {1:.4f}.'.format(df_m['area'].mean(), df_b['area'].mean())) The mean area of malignant tumors is 976.5829 while that of benign tumors is 462.7124. Although summary statistics like the mean are helpful, it would be nice to be able to compare the distributions of the areas of malignant and benign tumors visually. Let's see a simple example of using matplotlib to create histograms for both distributions on the same plot. (We'll learn how to use matplotlib in the next lesson.) import matplotlib.pyplot as plt %matplotlib inline # Plot histogram of benign and malignant tumor areas on the same axes fig, ax = plt.subplots(figsize=(8, 6)) ax.hist(df_b['area'], alpha=0.5, label='benign') ax.hist(df_m['area'], alpha=0.5, label='malignant') ax.set_title('Distributions of Benign and Malignant Tumor Areas') ax.set_xlabel('Area') ax.set_ylabel('Count') ax.legend(loc='upper right') plt.show() The visual above suggests that there is a difference between the distribution of areas for benign and malignant tumors. We don't yet have the tools to conclude that these distributions are different or whether the size definitely affects a tumor's malignancy. However, we can observe from summary statistics and these histograms that malignant tumors are generally larger in size than benign tumors.","title":"Selecting Data with Masks"},{"location":"source/plotting_with_pandas/","text":"Plotting with Pandas Here are some of the most common and useful plots you can create with Pandas. Note that the plot methods on a Series or DataFrame are just simple wrappers around matplotlib functions. This is why you might see these them used interchangeably. Pandas is nice for quick insights, but you'll need to use matplotlib to really dive into details and customize your visualizations. We'll get into this more later on. Let's first use census income data to practice plotting histograms, bar charts, and pie charts. import pandas as pd # This allows us to view visualizations in Jupyter notebook - useful!! %matplotlib inline # View summary of census income data df_census = pd.read_csv('census_income_data.csv') df_census.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 32561 entries, 0 to 32560 Data columns (total 15 columns): age 32561 non-null int64 workclass 30725 non-null object fnlwgt 32561 non-null int64 education 32561 non-null object education-num 32561 non-null int64 marital-status 32561 non-null object occupation 30718 non-null object relationship 32561 non-null object race 32561 non-null object sex 32561 non-null object capital-gain 32561 non-null int64 capital-loss 32561 non-null int64 hours-per-week 32561 non-null int64 native-country 31978 non-null object income 32561 non-null object dtypes: int64(6), object(9) memory usage: 3.7+ MB # This is quick way to view histograms for all numeric columns df_census.hist() array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC0ECB70>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BBCBC278>], [<matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC3E35C0>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC40AB38>], [<matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC43C0B8>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC464630>]], dtype=object) # That was way too crowded, let's make our figure size bigger # Also, we can use a semicolon to suppress unwanted output df_census.hist(figsize=(8,8)); # We can also get a histogram for a single column like this df_census['age'].hist(); # We can also plot a histogram using this more general function df_census['age'].plot(kind='hist'); Next, let's plot a bar chart. For this, we need counts for each distinct value (or bar). # This function aggregates counts for each unique value in a column print(df_census['education'].value_counts()); HS-grad 10501 Some-college 7291 Bachelors 5355 Masters 1723 Assoc-voc 1382 11th 1175 Assoc-acdm 1067 10th 933 7th-8th 646 Prof-school 576 9th 514 12th 433 Doctorate 413 5th-6th 333 1st-4th 168 Preschool 51 Name: education, dtype: int64 # We can use value counts to plot our bar chart df_census['education'].value_counts().plot(kind='bar'); # Value counts are also required for pie charts df_census['workclass'].value_counts().plot(kind='pie', figsize=(8, 8)); Now, let's use cancer data to practice plotting scatter plots and box plots. df_cancer = pd.read_csv('cancer_data_edited.csv') df_cancer.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 564 entries, 0 to 563 Data columns (total 12 columns): id 564 non-null int64 diagnosis 564 non-null object radius 564 non-null float64 texture 564 non-null float64 perimeter 564 non-null float64 area 564 non-null float64 smoothness 564 non-null float64 compactness 564 non-null float64 concavity 564 non-null float64 concave_points 564 non-null float64 symmetry 564 non-null float64 fractal_dimension 564 non-null float64 dtypes: float64(10), int64(1), object(1) memory usage: 53.0+ KB This next function is really cool for getting quick insight into the relationships among numeric variables with scatterplots. It also displays a histogram for each variable. # Create scatter matrix, make figure size big enough to display clearly pd.plotting.scatter_matrix(df_cancer, figsize=(15, 15)); # Create a single scatter plot like this df_cancer.plot(x='compactness', y='concavity', kind='scatter'); # Create a box plot like this df_cancer['concave_points'].plot(kind='box');","title":"Plotting"},{"location":"source/plotting_with_pandas/#plotting-with-pandas","text":"Here are some of the most common and useful plots you can create with Pandas. Note that the plot methods on a Series or DataFrame are just simple wrappers around matplotlib functions. This is why you might see these them used interchangeably. Pandas is nice for quick insights, but you'll need to use matplotlib to really dive into details and customize your visualizations. We'll get into this more later on. Let's first use census income data to practice plotting histograms, bar charts, and pie charts. import pandas as pd # This allows us to view visualizations in Jupyter notebook - useful!! %matplotlib inline # View summary of census income data df_census = pd.read_csv('census_income_data.csv') df_census.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 32561 entries, 0 to 32560 Data columns (total 15 columns): age 32561 non-null int64 workclass 30725 non-null object fnlwgt 32561 non-null int64 education 32561 non-null object education-num 32561 non-null int64 marital-status 32561 non-null object occupation 30718 non-null object relationship 32561 non-null object race 32561 non-null object sex 32561 non-null object capital-gain 32561 non-null int64 capital-loss 32561 non-null int64 hours-per-week 32561 non-null int64 native-country 31978 non-null object income 32561 non-null object dtypes: int64(6), object(9) memory usage: 3.7+ MB # This is quick way to view histograms for all numeric columns df_census.hist() array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC0ECB70>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BBCBC278>], [<matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC3E35C0>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC40AB38>], [<matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC43C0B8>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001E3BC464630>]], dtype=object) # That was way too crowded, let's make our figure size bigger # Also, we can use a semicolon to suppress unwanted output df_census.hist(figsize=(8,8)); # We can also get a histogram for a single column like this df_census['age'].hist(); # We can also plot a histogram using this more general function df_census['age'].plot(kind='hist'); Next, let's plot a bar chart. For this, we need counts for each distinct value (or bar). # This function aggregates counts for each unique value in a column print(df_census['education'].value_counts()); HS-grad 10501 Some-college 7291 Bachelors 5355 Masters 1723 Assoc-voc 1382 11th 1175 Assoc-acdm 1067 10th 933 7th-8th 646 Prof-school 576 9th 514 12th 433 Doctorate 413 5th-6th 333 1st-4th 168 Preschool 51 Name: education, dtype: int64 # We can use value counts to plot our bar chart df_census['education'].value_counts().plot(kind='bar'); # Value counts are also required for pie charts df_census['workclass'].value_counts().plot(kind='pie', figsize=(8, 8)); Now, let's use cancer data to practice plotting scatter plots and box plots. df_cancer = pd.read_csv('cancer_data_edited.csv') df_cancer.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 564 entries, 0 to 563 Data columns (total 12 columns): id 564 non-null int64 diagnosis 564 non-null object radius 564 non-null float64 texture 564 non-null float64 perimeter 564 non-null float64 area 564 non-null float64 smoothness 564 non-null float64 compactness 564 non-null float64 concavity 564 non-null float64 concave_points 564 non-null float64 symmetry 564 non-null float64 fractal_dimension 564 non-null float64 dtypes: float64(10), int64(1), object(1) memory usage: 53.0+ KB This next function is really cool for getting quick insight into the relationships among numeric variables with scatterplots. It also displays a histogram for each variable. # Create scatter matrix, make figure size big enough to display clearly pd.plotting.scatter_matrix(df_cancer, figsize=(15, 15)); # Create a single scatter plot like this df_cancer.plot(x='compactness', y='concavity', kind='scatter'); # Create a box plot like this df_cancer['concave_points'].plot(kind='box');","title":"Plotting with Pandas"},{"location":"week_1/cost_function/","text":"","title":"Cost Function"},{"location":"week_1/cost_function_intuition_1/","text":"","title":"Cost Function Intuition 1"},{"location":"week_1/cost_function_intuition_2/","text":"","title":"Cost Function Intuition 2"},{"location":"week_1/gd_for_linear_regression/","text":"","title":"Gradient Descent For Linear Regression"},{"location":"week_1/gradient_descent/","text":"","title":"Gradient Descent"},{"location":"week_1/gradient_descent_intuition/","text":"","title":"Gradient Descent Intuition"},{"location":"week_1/model_representation/","text":"","title":"Model Representation"}]}