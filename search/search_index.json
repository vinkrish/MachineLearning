{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Machine Learning? Arthur Samuel described it as: \"The field of study that gives computers the ability to learn without being explicitly programmed.\" This is an older, informal definition. Tom Mitchell provides a more modern definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" ML systems learn how to combine input to produce useful predictions on never-before-seen data. Example : playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning Unsupervised learning Supervised Learning In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into Regression and Classification problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. Commonly used algorithms: Linear Regression Logistics Regression Polynomial Regression Stepwise Regression Ridge Regression Lasso Regression ElasticNet Regression Support Vector Regression (SVR) In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Commonly used algorithms: Linear Classifier (Logistic Regression & Naive Bayes) Support Vector Machines Decision Trees Random Forest Neural Networks Nearest Neighbor Example 1 : (a) Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. (b) We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2 : (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. Unsupervised Learning Unsupervised learning allows us to approach problems with little or no idea what our results should look like. With unsupervised learning there is no feedback based on the prediction results. We can derive structure: From data where we don't necessarily know the effect of the variables. By clustering the data based on relationships among the variables in the data. Example : Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Commonly used algorithms: K-means Clustering Mean-Shift Clustering Density-Based Spatial Clustering of Applications(DBSCAN) Ex-Hierarchical Dimensionality Reduction (Principal Component Analysis)","title":"About"},{"location":"#what-is-machine-learning","text":"Arthur Samuel described it as: \"The field of study that gives computers the ability to learn without being explicitly programmed.\" This is an older, informal definition. Tom Mitchell provides a more modern definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" ML systems learn how to combine input to produce useful predictions on never-before-seen data. Example : playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning Unsupervised learning","title":"What is Machine Learning?"},{"location":"#supervised-learning","text":"In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into Regression and Classification problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. Commonly used algorithms: Linear Regression Logistics Regression Polynomial Regression Stepwise Regression Ridge Regression Lasso Regression ElasticNet Regression Support Vector Regression (SVR) In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Commonly used algorithms: Linear Classifier (Logistic Regression & Naive Bayes) Support Vector Machines Decision Trees Random Forest Neural Networks Nearest Neighbor Example 1 : (a) Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. (b) We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2 : (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.","title":"Supervised Learning"},{"location":"#unsupervised-learning","text":"Unsupervised learning allows us to approach problems with little or no idea what our results should look like. With unsupervised learning there is no feedback based on the prediction results. We can derive structure: From data where we don't necessarily know the effect of the variables. By clustering the data based on relationships among the variables in the data. Example : Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Commonly used algorithms: K-means Clustering Mean-Shift Clustering Density-Based Spatial Clustering of Applications(DBSCAN) Ex-Hierarchical Dimensionality Reduction (Principal Component Analysis)","title":"Unsupervised Learning"},{"location":"algorithms/","text":"Linear regression is a method for finding the straight line or hyperplane that best fits a set of points.","title":"Algorithms"},{"location":"dap/","text":"This process will help you understand, explore and use your data intelligently so that you make the most of the information you're given. Five steps: Question Wrangle Explore Draw conclusions Communicate. Question The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights. Wrangle Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with. Explore Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data. Conclusions After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately. Communicate Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations","title":"Data Analysis Process"},{"location":"dap/#question","text":"The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights.","title":"Question"},{"location":"dap/#wrangle","text":"Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with.","title":"Wrangle"},{"location":"dap/#explore","text":"Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data.","title":"Explore"},{"location":"dap/#conclusions","text":"After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately.","title":"Conclusions"},{"location":"dap/#communicate","text":"Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations","title":"Communicate"},{"location":"math/","text":"Mean deviation refers to how far away every data point is from the average of those data points. Squared mean deviation simply squares this result. This is the square of the distance of every data point from the mean, and now we can finally calculate the variance of these data points. $$variance=\\frac{\\sum(x_i-\\overline{x})^2}{n}$$ Variance describe the concentration of data points around the mean, It is a measure of spread/variability of data. It is calculated as sum of the squares of the distances of every individual data point from the mean divided by the total number of data points. The estimate of the variance can be improved by tweaking the denominator of this function. This tweak is called the Bessel's Correction . So instead of using N for all of the data points, we'll simply use N-1 as the denominator. $$variance=\\frac{\\sum(x_i-\\overline{x})^2}{n-1}$$ Mean and variance succinctly summarize a set of numbers along with Standard deviation , which once again measures how much the numbers jump around. $$SD = \\sqrt{variance}$$ Mean Square Error (MSE) MSE is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples: $$MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2$$ Standard Error(SE) of a statistic is the standard deviation of its sampling distribution or an estimate of that standard deviation. If the parameter or the statistic is the mean, it is called the standard error of the mean (SEM). $\\sigma$=Standard Deviation of population, n=size of sample $$SE = \\frac{\\sigma}{\\sqrt{n}}$$ Z score (Standard Score) Given an observed value x, the Z score finds the number of Standard deviations x is away from the mean. $$Z = \\frac{x-\\mu}{\\sigma}$$ T distribution The t-Test is best to use when we do not know the population standard deviation. Instead we use the sample standard deviation. The number of degrees of freedom(df) is the number of values that are free to vary (n-1). $$S=\\frac{\\sum(x_i-\\overline{x})^2}{n-1}$$ $$t = \\frac{\\overline{x}-\\mu}{SE}$$ T-tests are also great for testing two sample means (i.e. paired t-tests), we modify the formula to become: $$SD = \\sqrt{S_1^2 + S_2^2}$$ $$SE = \\sqrt{\\frac{S_1^2}{n1} + \\frac{S_2^2}{n2}}$$ $$t = \\frac{(\\overline{x}_2-\\overline{x}_1)-(\\mu_2-\\mu_1)}{\\frac{\\sqrt(s_1^2+s_2^2)}{n}}$$ Cohen's d It measures the effect size of the strength of a phenomenon. Cohen\u2019s d gives us the distance between means in standardized units. $\\overline{x} = \\overline{x_1} - \\overline{x_2}$ $$d = \\frac{\\overline{x}-\\mu}{S}$$ Margin of Error (ME) $$ME = t_{critical} * SE$$ Confidence Interval (CI) $$CI = \\overline{x} \\pm ME$$ Evaluation Metrics Positive Class In binary classification, the two possible classes are labeled as positive and negative. The positive outcome is the thing we're testing for. (Admittedly, we're simultaneously testing for both outcomes, but play along.) For example, the positive class in a medical test might be \"tumor.\" The positive class in an email classifier might be \"spam\". Negative Class In binary classification, one class is termed positive and the other is termed negative. The positive class is the thing we're looking for and the negative class is the other possibility. For example, the negative class in a medical test might be \"not tumor.\" The negative class in an email classifier might be \"not spam\". True Positive (TP) An example in which the model correctly predicted the positive class. For example, the model inferred that a particular email message was spam, and that email message really was spam. True Negative (TN) An example in which the model correctly predicted the negative class. For example, the model inferred that a particular email message was not spam, and that email message really was not spam. true positive rate (TPR) Synonym for recall. That is: True positive rate is the y-axis in an ROC curve. $$\\text{True Positive Rate} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}}$$ False Negative (FN) An example in which the model mistakenly predicted the negative class. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam. False Positive (FP) An example in which the model mistakenly predicted the positive class. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam. false positive rate (FPR) The x-axis in an ROC curve. The false positive rate is defined as follows: $$\\text{False Positive Rate} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}$$ ROC (receiver operating characteristic) Curve: A curve of true positive rate vs. false positive rate at different classification thresholds. Accuracy The fraction of predictions that a classification model got right. In multi-class classification, accuracy is defined as follows: $$\\text{Accuracy} = \\frac{\\text{Correct Predictions}} {\\text{Total Number Of Examples}}$$ In binary classification, accuracy has the following definition: $$\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Number Of Examples}}$$ Precision A metric for classification models. Precision identifies the frequency with which a model was correct when predicting the positive class. That is: $$\\text{Precision} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Positives}}$$ Recall A metric for classification models that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is: $$\\text{Recall} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}}$$ Packages Overview Numpy let's you perform mathematical functions on large multi dimensional arrays and matrices efficiently. Pandas is like a more powerful and flexible version of Excel that can handle large amounts of data. Matplotlib is a plotting library that can produce great visualizations often with very few lines of code. Scikit-learn is designed to work with NumPy, SciPy and Pandas, provides toolset for training and evaluation tasks: Data splitting Pre-processing Feature selection Model training Model tuning and offers common interface across algorithms","title":"Math"},{"location":"math/#z-score-standard-score","text":"Given an observed value x, the Z score finds the number of Standard deviations x is away from the mean. $$Z = \\frac{x-\\mu}{\\sigma}$$","title":"Z score (Standard Score)"},{"location":"math/#t-distribution","text":"The t-Test is best to use when we do not know the population standard deviation. Instead we use the sample standard deviation. The number of degrees of freedom(df) is the number of values that are free to vary (n-1). $$S=\\frac{\\sum(x_i-\\overline{x})^2}{n-1}$$ $$t = \\frac{\\overline{x}-\\mu}{SE}$$ T-tests are also great for testing two sample means (i.e. paired t-tests), we modify the formula to become: $$SD = \\sqrt{S_1^2 + S_2^2}$$ $$SE = \\sqrt{\\frac{S_1^2}{n1} + \\frac{S_2^2}{n2}}$$ $$t = \\frac{(\\overline{x}_2-\\overline{x}_1)-(\\mu_2-\\mu_1)}{\\frac{\\sqrt(s_1^2+s_2^2)}{n}}$$","title":"T distribution"},{"location":"math/#cohens-d","text":"It measures the effect size of the strength of a phenomenon. Cohen\u2019s d gives us the distance between means in standardized units. $\\overline{x} = \\overline{x_1} - \\overline{x_2}$ $$d = \\frac{\\overline{x}-\\mu}{S}$$","title":"Cohen's d"},{"location":"math/#margin-of-error-me","text":"$$ME = t_{critical} * SE$$","title":"Margin of Error (ME)"},{"location":"math/#confidence-interval-ci","text":"$$CI = \\overline{x} \\pm ME$$","title":"Confidence Interval (CI)"},{"location":"math/#evaluation-metrics","text":"Positive Class In binary classification, the two possible classes are labeled as positive and negative. The positive outcome is the thing we're testing for. (Admittedly, we're simultaneously testing for both outcomes, but play along.) For example, the positive class in a medical test might be \"tumor.\" The positive class in an email classifier might be \"spam\". Negative Class In binary classification, one class is termed positive and the other is termed negative. The positive class is the thing we're looking for and the negative class is the other possibility. For example, the negative class in a medical test might be \"not tumor.\" The negative class in an email classifier might be \"not spam\". True Positive (TP) An example in which the model correctly predicted the positive class. For example, the model inferred that a particular email message was spam, and that email message really was spam. True Negative (TN) An example in which the model correctly predicted the negative class. For example, the model inferred that a particular email message was not spam, and that email message really was not spam. true positive rate (TPR) Synonym for recall. That is: True positive rate is the y-axis in an ROC curve. $$\\text{True Positive Rate} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}}$$ False Negative (FN) An example in which the model mistakenly predicted the negative class. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam. False Positive (FP) An example in which the model mistakenly predicted the positive class. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam. false positive rate (FPR) The x-axis in an ROC curve. The false positive rate is defined as follows: $$\\text{False Positive Rate} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}$$ ROC (receiver operating characteristic) Curve: A curve of true positive rate vs. false positive rate at different classification thresholds. Accuracy The fraction of predictions that a classification model got right. In multi-class classification, accuracy is defined as follows: $$\\text{Accuracy} = \\frac{\\text{Correct Predictions}} {\\text{Total Number Of Examples}}$$ In binary classification, accuracy has the following definition: $$\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Number Of Examples}}$$ Precision A metric for classification models. Precision identifies the frequency with which a model was correct when predicting the positive class. That is: $$\\text{Precision} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Positives}}$$ Recall A metric for classification models that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is: $$\\text{Recall} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}}$$","title":"Evaluation Metrics"},{"location":"math/#packages-overview","text":"Numpy let's you perform mathematical functions on large multi dimensional arrays and matrices efficiently. Pandas is like a more powerful and flexible version of Excel that can handle large amounts of data. Matplotlib is a plotting library that can produce great visualizations often with very few lines of code. Scikit-learn is designed to work with NumPy, SciPy and Pandas, provides toolset for training and evaluation tasks: Data splitting Pre-processing Feature selection Model training Model tuning and offers common interface across algorithms","title":"Packages Overview"},{"location":"ml_workflow/","text":"An orchestrated and repeatable pattern which systematically transforms and processes information to create prediction solutions. Asking the right question Preparing data Selecting the algorithm Training the model Testing the model Solution Statement Use the Machine Learning Workflow to process and transform Pima Indian data to create a prediction model. This model must predict which peopel are likely to develop diabetes with 70% or greater accuracy Tidy Data Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column each observation is a row each type of observational unit is a table Selecting the algorithm We will use our problem knowledge to help us decide the algorithm to use. we will discuss - The role of the algorithm in machine learning process, select our initial algorithm by utilizing the requirements identified in the solution statement as a guide, and discuss at a high level the characteristics of some specific algorithms. Finally select one algorithm to be our initial algorithm, in machine learning we often cycle through the workflow. In our search to find the best solution, it is likely we will need to train and evaluate multiple algorithms. Let's review how the algorithm is involved in the process. When the training function (often named fit in scikit-learn) is called, the algorithm executes its logic and processes the training data. Using the algorithm's logic, the data in analyzed. This analysis evaluates the data with respect to mathematical model and logic associated with the algorithm. The algorithm uses the results of this analysis to adjust internal parameters to produce a model that has been trained to best fit the features in the training data and produce the associated class result. This best fit is defined by evaluating a function specific to the particular algorithm. The fit parameters are stored and the model is now said to be trained. Later, the trained model is called the prediction function (often named predict in scikit-learn). When this prediction function is called, real data is passed to the trained model. Using only the features in the data, the trained model uses its code and parameter values set during training to evaluate the data's features and predict the class result, diabetes or not for this new data. Decision factors to select our initial algorithm: We will use our solution statement and knowledge of the workflow to help guide us in the evaluation of these factors. what Learning Type they support the Result Type the algorithm predicts the Complexity of the algorithm whether the algorithm is Basic or Enhanced Each algorithm has a set of problems with which it works best. One way to divide them is to look at the type of Learning they support. Reading the statement, we see that our solution is about prediction. Prediction means supervised machine learning, so we can eliminate all algorithms that do not support it. Let's see how Result Type can help. Prediction results can be divided into two categories: Regression (Continuous values) Classification (Discrete values) Based on the Statment, the algorithm must support Binary classification. Since this is our initial algorithm, let's stick to the basic algorithms. Selecting Our Initial Algorithm Candidate Algorithms: Naive Bayes Logistic Regression Decision Tree More complex algoritms use these as building blocks. Naive Bayes Algorithm The Naive Bayes algorithm is based on Bayes' Theorem. This theorem calculates a probability of a diabetes by looking at the likelihood of diabetes based on previous data combined with probability of diabetes on nearby feature values. In other words, so how often does the person having high blood pressure correlate to diabetes? It makes the naive assumption that all of the features we pass in are independent of each other and equally impact the result. This assumption that every featuer is independent to the others allows for fast conversions and therefore requires a small amount of data to train. Logistic Regression Algorithm The Logistic Regression algorithm has a somewhat confusing name. In Statistics, Regression often implies continuous values. But Logistics Regression returns a binary result. The algorithm measures the relationship of each feature and weights them based on their impact on the result. The resultant value is mapped against a curve with two values, one and zero, which is equivalent to diabetes or no diabetes. Decision Tree Algorithm The Decision Tree algorithm can be nicely visualized. The algorithm uses a binary tree structure with each node making a decision based upon the values of the feature. At each node, the feature value causes us to go down one path or another. A lot of data may be required to find the value which defines taking one path or another. As we see decision trees have the advantage of having tools available to produce a picture of the tree. This makes it easy to follow along and visualize how the trained model works. Training the Model Letting specific data teach a machine learning algorithm to create a specific prediction model. Why retrain? Retraining will ensure that our model can take advantage of the new data to make better predictions. And also verify the algorithm can still create a high-performance model with the new data. We will compare our model prediction with actual prediction or actual labels that are associated with training data and use this as feeback to tweak our model parameters, this is the loss function or cost function and its primary purpose is to improve our model parameters and build stronger model. Performance Improvement Options Adjust current algorithm Get more data or improve data Improve training Switch algorithms","title":"Machine Learning Workflow"},{"location":"ml_workflow/#solution-statement","text":"Use the Machine Learning Workflow to process and transform Pima Indian data to create a prediction model. This model must predict which peopel are likely to develop diabetes with 70% or greater accuracy","title":"Solution Statement"},{"location":"ml_workflow/#tidy-data","text":"Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column each observation is a row each type of observational unit is a table","title":"Tidy Data"},{"location":"ml_workflow/#selecting-the-algorithm","text":"We will use our problem knowledge to help us decide the algorithm to use. we will discuss - The role of the algorithm in machine learning process, select our initial algorithm by utilizing the requirements identified in the solution statement as a guide, and discuss at a high level the characteristics of some specific algorithms. Finally select one algorithm to be our initial algorithm, in machine learning we often cycle through the workflow. In our search to find the best solution, it is likely we will need to train and evaluate multiple algorithms. Let's review how the algorithm is involved in the process. When the training function (often named fit in scikit-learn) is called, the algorithm executes its logic and processes the training data. Using the algorithm's logic, the data in analyzed. This analysis evaluates the data with respect to mathematical model and logic associated with the algorithm. The algorithm uses the results of this analysis to adjust internal parameters to produce a model that has been trained to best fit the features in the training data and produce the associated class result. This best fit is defined by evaluating a function specific to the particular algorithm. The fit parameters are stored and the model is now said to be trained. Later, the trained model is called the prediction function (often named predict in scikit-learn). When this prediction function is called, real data is passed to the trained model. Using only the features in the data, the trained model uses its code and parameter values set during training to evaluate the data's features and predict the class result, diabetes or not for this new data. Decision factors to select our initial algorithm: We will use our solution statement and knowledge of the workflow to help guide us in the evaluation of these factors. what Learning Type they support the Result Type the algorithm predicts the Complexity of the algorithm whether the algorithm is Basic or Enhanced Each algorithm has a set of problems with which it works best. One way to divide them is to look at the type of Learning they support. Reading the statement, we see that our solution is about prediction. Prediction means supervised machine learning, so we can eliminate all algorithms that do not support it. Let's see how Result Type can help. Prediction results can be divided into two categories: Regression (Continuous values) Classification (Discrete values) Based on the Statment, the algorithm must support Binary classification. Since this is our initial algorithm, let's stick to the basic algorithms.","title":"Selecting the algorithm"},{"location":"ml_workflow/#selecting-our-initial-algorithm","text":"Candidate Algorithms: Naive Bayes Logistic Regression Decision Tree More complex algoritms use these as building blocks.","title":"Selecting Our Initial Algorithm"},{"location":"ml_workflow/#naive-bayes-algorithm","text":"The Naive Bayes algorithm is based on Bayes' Theorem. This theorem calculates a probability of a diabetes by looking at the likelihood of diabetes based on previous data combined with probability of diabetes on nearby feature values. In other words, so how often does the person having high blood pressure correlate to diabetes? It makes the naive assumption that all of the features we pass in are independent of each other and equally impact the result. This assumption that every featuer is independent to the others allows for fast conversions and therefore requires a small amount of data to train.","title":"Naive Bayes Algorithm"},{"location":"ml_workflow/#logistic-regression-algorithm","text":"The Logistic Regression algorithm has a somewhat confusing name. In Statistics, Regression often implies continuous values. But Logistics Regression returns a binary result. The algorithm measures the relationship of each feature and weights them based on their impact on the result. The resultant value is mapped against a curve with two values, one and zero, which is equivalent to diabetes or no diabetes.","title":"Logistic Regression Algorithm"},{"location":"ml_workflow/#decision-tree-algorithm","text":"The Decision Tree algorithm can be nicely visualized. The algorithm uses a binary tree structure with each node making a decision based upon the values of the feature. At each node, the feature value causes us to go down one path or another. A lot of data may be required to find the value which defines taking one path or another. As we see decision trees have the advantage of having tools available to produce a picture of the tree. This makes it easy to follow along and visualize how the trained model works.","title":"Decision Tree Algorithm"},{"location":"ml_workflow/#training-the-model","text":"Letting specific data teach a machine learning algorithm to create a specific prediction model. Why retrain? Retraining will ensure that our model can take advantage of the new data to make better predictions. And also verify the algorithm can still create a high-performance model with the new data. We will compare our model prediction with actual prediction or actual labels that are associated with training data and use this as feeback to tweak our model parameters, this is the loss function or cost function and its primary purpose is to improve our model parameters and build stronger model. Performance Improvement Options Adjust current algorithm Get more data or improve data Improve training Switch algorithms","title":"Training the Model"},{"location":"source/","text":"DS & Model Report Playground NumPy Pandas DataFrame Stroop Effect Practice Problem Data Analysis Process Assessing Cleaning Plotting Conclusion Communicate Case Study Wine Rating Case Study Fuel Economy Fuel Economy - Conclusion Fuel Economy - Visuals Fuel Economy - Merging Data Investigate Dataset Titanic Decision Tree Titanic Survival Exploration Naive Bayes Email Spam or Ham Random Forest & Logistic Regression Predict Diabetes Categorical and Numeric Data Text Feature Extraction Image Feature Extraction Lasso & Ridge Regression Vehicle Price SVR Mileage SVM Document Classification Image Classification Gradient Boost Regression / Decision Tree Automobile Price Mean Shift Clustering Titanic Survivor PCA Wine Rating","title":"Source"},{"location":"tensorflow/","text":"TensorFlow consists of the following two components: a graph protocol buffer a runtime that executes the (distributed) graph These two components are analogous to Python code and the Python interpreter. Just as the Python interpreter is implemented on multiple hardware platforms to run Python code, TensorFlow can run the graph on multiple hardware platforms, including CPU, GPU, and TPU A Quick Look at the tf.estimator API import tensorflow as tf # Set up a linear classifier. classifier = tf.estimator.LinearClassifier(feature_columns) # Train the model on some example data. classifier.train(input_fn=train_input_fn, steps=2000) # Use it to predict. predictions = classifier.predict(input_fn=predict_input_fn) Hyperparameters Steps , which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model's weights once. Batch Size , which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1. $$total\\,number\\,of\\,trained\\,examples = batch\\,size * steps$$","title":"TensorFlow"},{"location":"tensorflow/#a-quick-look-at-the-tfestimator-api","text":"import tensorflow as tf # Set up a linear classifier. classifier = tf.estimator.LinearClassifier(feature_columns) # Train the model on some example data. classifier.train(input_fn=train_input_fn, steps=2000) # Use it to predict. predictions = classifier.predict(input_fn=predict_input_fn)","title":"A Quick Look at the tf.estimator API"},{"location":"tensorflow/#hyperparameters","text":"Steps , which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model's weights once. Batch Size , which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1. $$total\\,number\\,of\\,trained\\,examples = batch\\,size * steps$$","title":"Hyperparameters"},{"location":"terminology/","text":"Model The representation of what an ML system has learned from the training data. Labels A label is the thing we're predicting\u2014the y variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything. Features A feature is an input variable\u2014the x variable in simple linear regression. $$x_1,x_2,x_3,....x_N$$ synthetic feature: A feature not present among the input features, but created from one or more of them. Kinds of synthetic features include: Bucketing a continuous feature into range bins. Multiplying (or dividing) one feature value by other feature value(s) or by itself. Creating a feature cross. Features created by normalizing or scaling alone are not considered synthetic features. Scaling A commonly used practice in feature engineering to tame a feature's range of values to match the range of other features in the dataset. For example, suppose that you want all floating-point features in the dataset to have a range of 0 to 1. Given a particular feature's range of 0 to 500, you could scale that feature by dividing each value by 500. Weight A coefficient for a feature in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model. Data Set training set: The subset of the dataset used to train a model. test set: The subset of the dataset that you use to test your model after the model has gone through initial vetting by the validation set. validation set: A subset of the dataset\u2014disjoint from the training set\u2014used in validation. temporal data: Data recorded at different points in time. For example, winter coat sales recorded for each day of the year would be temporal data. stationarity: A property of data in a dataset, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December. static model: A model that is trained offline. Examples An example is a particular instance of data, x. We break examples into two categories: labeled examples unlabeled examples A labeled example includes both feature(s) and the label. That is: {features, label}: (x, y) Use labeled examples to train the model. An unlabeled example contains features but not the label. That is: {features, ?}: (x, ?) Once we've trained our model with labeled examples, we use that model to predict the label on unlabeled examples. Models A model defines the relationship between features and label (and are defined by internal parameters, which are learned). For example, a spam detection model might associate certain features strongly with \"spam\". Let's highlight two phases of a model's life: Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. Inference means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y'). For example, during inference, you can predict medianHouseValue for new unlabeled examples. Loss Loss is the penalty for a bad prediction or how far a model's predictions are from its label. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. L1 Loss Loss function based on the absolute value of the difference between the values that a model is predicting and the actual values of the labels. L1 loss is less sensitive to outliers than L2 loss. L2 Loss This function calculates the squares of the difference between a model's predicted value for a labeled example and the actual value of the label. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than L1 loss. (used in linear regression) $$ \\begin{align} L_2 Loss & = (observation - prediction)^2 \\\\ & = (y - y')^2 \\\\ & = \\sum_{(x,y)\\in D}(y-prediction(x))^2 \\end{align} $$ $$\\sum \\text{:We're summing over all examples in the training set.}$$ $$D \\text{: is a data set containing many labeled examples, which are (x,y) pairs.}$$ $$ \\text{ Sometimes useful to average over all examples, } \\text{so divide out by} \\frac{1}{|D|}.$$ Selection Bias Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. The following forms of selection bias exist: coverage bias: The population represented in the dataset does not match the population that the ML model is making predictions about. sampling bias: Data is not collected randomly from the target group. non-response bias (also called participation bias): Users from certain groups opt-out of surveys at different rates than users from other groups. Gradient Gradient The vector of partial derivatives with respect to all of the independent variables. In machine learning, the gradient is the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent. $$(y - y')^2$$ The derivative of above with respect to the weights and biases tells us how loss changes for a given example Gradient Descent A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss. Exploding Gradient Problem The tendency for gradients in a deep neural networks (especially recurrent neural networks) to become surprisingly steep (high). Steep gradients result in very large updates to the weights of each node in a deep neural network. Models suffering from the exploding gradient problem become difficult or impossible to train. Gradient clipping can mitigate this problem. Gradient Clipping A commonly used mechanism to mitigate the exploding gradient problem by artificially limiting (clipping) the maximum value of gradients when using gradient descent to train a model. Convergence Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence. Underfitting Producing a model with poor predictive ability because the model hasn't captured the complexity of the training data. Many problems can cause underfitting, including: Training on the wrong set of features. Training for too few epochs or at too low a learning rate. Training with too high a regularization rate. Providing too few hidden layers in a deep neural network. Overfitting The algorithm analyses the data and trains itself to create a high mathematical order model based on the data. $$y = x_1 + w_2x_2^3 + w_3x_3^8$$ These high-order terms let this equation define a precise decision boundary between the positive and negative values, but as a result, the training process has created a model that works very well on training data but poorly when asked to predict values based on data it has not trained - this is the class overfit problem and is an issue that must be handled to create machine learning models that work well not only on the training data but also on real-world data. Regularization , Cross validation , Ensemble learning of which dropout is a part, are all ways to mitigate overfitting. We add an additional parameter where if the model coefficients get too complex we add a penalty to the objective function. This is the technique that we use in regression. Regularization: The penalty on a model's complexity. Different kinds of regularization include: L1 regularization L2 regularization Dropout regularization L1 is a type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with L2 regularization. L2 is a type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models. Dropout is a form of regularization useful in training neural networks. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. Regularization rate A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate's influence: $$\\text{minimize(loss function + }\\lambda\\text{(regularization function))}$$ Raising the regularization rate reduces overfitting but may make the model less accurate. Hyperplane A boundary that separates a space into two subspaces. For example, a line is a hyperplane in two dimensions and a plane is a hyperplane in three dimensions. More typically in machine learning, a hyperplane is the boundary separating a high-dimensional space. Kernel Support Vector Machines use hyperplanes to separate positive classes from negative classes, often in a very high-dimensional space. Parameter A variable of a model that the ML system trains on its own. A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data. Often model parameters are estimated using an optimization algorithm, which is a type of efficient search through possible parameter values. They are required by the model when making predictions. They values define the skill of the model on your problem. They are estimated or learned from data. They are often not set manually by the practitioner. They are often saved as part of the learned model. Parameters are key to machine learning algorithms. They are the part of the model that is learned from historical training data. Statistics: In statistics, you may assume a distribution for a variable, such as a Gaussian distribution. Two parameters of the Gaussian distribution are the mean (mu) and the standard deviation (sigma). This holds in machine learning, where these parameters may be estimated from data and used as part of a predictive model. Programming: In programming, you may pass a parameter to a function. In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data. Some examples of model parameters include: The weights in an artificial neural network. The support vectors in a support vector machine. The coefficients in a linear regression or logistic regression. Hyperparameter Hyperparameters are the configuration settings used to tune how the model is trained and it is external to the model, whose value cannot be estimated from data. They are often used in processes to help estimate model parameters. They are often specified by the practitioner. They can often be set using heuristics. They are often tuned for a given predictive modeling problem. We cannot know the best value for a hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error. When a machine learning algorithm is tuned for a specific problem, such as when you are using a grid search or a random search, then you are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions. Hyperparameters are often referred to as model parameters which can make things confusing. A good rule of thumb to overcome this confusion is as follows: If you have to specify a model parameter manually then it is probably a model hyperparameter. Some examples of model hyperparameters include: The learning rate for training a neural network. The C and sigma hyperparameters for support vector machines. The k in k-nearest neighbors. Grid Search is an approach to hyperparameter tuning that will methodically build and evaluate model for each combination of algorithm parameters specified in a grid. Learning Rate A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step. Time Series Analysis A subfield of machine learning and statistics that analyzes temporal data. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data. Keras A popular Python machine learning API. Keras runs on several deep learning frameworks, including TensorFlow, where it is made available as tf.keras.","title":"Terminology"},{"location":"terminology/#model","text":"The representation of what an ML system has learned from the training data.","title":"Model"},{"location":"terminology/#labels","text":"A label is the thing we're predicting\u2014the y variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything.","title":"Labels"},{"location":"terminology/#features","text":"A feature is an input variable\u2014the x variable in simple linear regression. $$x_1,x_2,x_3,....x_N$$ synthetic feature: A feature not present among the input features, but created from one or more of them. Kinds of synthetic features include: Bucketing a continuous feature into range bins. Multiplying (or dividing) one feature value by other feature value(s) or by itself. Creating a feature cross. Features created by normalizing or scaling alone are not considered synthetic features.","title":"Features"},{"location":"terminology/#scaling","text":"A commonly used practice in feature engineering to tame a feature's range of values to match the range of other features in the dataset. For example, suppose that you want all floating-point features in the dataset to have a range of 0 to 1. Given a particular feature's range of 0 to 500, you could scale that feature by dividing each value by 500.","title":"Scaling"},{"location":"terminology/#weight","text":"A coefficient for a feature in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model.","title":"Weight"},{"location":"terminology/#data-set","text":"training set: The subset of the dataset used to train a model. test set: The subset of the dataset that you use to test your model after the model has gone through initial vetting by the validation set. validation set: A subset of the dataset\u2014disjoint from the training set\u2014used in validation. temporal data: Data recorded at different points in time. For example, winter coat sales recorded for each day of the year would be temporal data. stationarity: A property of data in a dataset, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December. static model: A model that is trained offline.","title":"Data Set"},{"location":"terminology/#examples","text":"An example is a particular instance of data, x. We break examples into two categories: labeled examples unlabeled examples A labeled example includes both feature(s) and the label. That is: {features, label}: (x, y) Use labeled examples to train the model. An unlabeled example contains features but not the label. That is: {features, ?}: (x, ?) Once we've trained our model with labeled examples, we use that model to predict the label on unlabeled examples.","title":"Examples"},{"location":"terminology/#models","text":"A model defines the relationship between features and label (and are defined by internal parameters, which are learned). For example, a spam detection model might associate certain features strongly with \"spam\". Let's highlight two phases of a model's life: Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. Inference means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y'). For example, during inference, you can predict medianHouseValue for new unlabeled examples.","title":"Models"},{"location":"terminology/#loss","text":"Loss is the penalty for a bad prediction or how far a model's predictions are from its label. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. L1 Loss Loss function based on the absolute value of the difference between the values that a model is predicting and the actual values of the labels. L1 loss is less sensitive to outliers than L2 loss. L2 Loss This function calculates the squares of the difference between a model's predicted value for a labeled example and the actual value of the label. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than L1 loss. (used in linear regression) $$ \\begin{align} L_2 Loss & = (observation - prediction)^2 \\\\ & = (y - y')^2 \\\\ & = \\sum_{(x,y)\\in D}(y-prediction(x))^2 \\end{align} $$ $$\\sum \\text{:We're summing over all examples in the training set.}$$ $$D \\text{: is a data set containing many labeled examples, which are (x,y) pairs.}$$ $$ \\text{ Sometimes useful to average over all examples, } \\text{so divide out by} \\frac{1}{|D|}.$$","title":"Loss"},{"location":"terminology/#selection-bias","text":"Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. The following forms of selection bias exist: coverage bias: The population represented in the dataset does not match the population that the ML model is making predictions about. sampling bias: Data is not collected randomly from the target group. non-response bias (also called participation bias): Users from certain groups opt-out of surveys at different rates than users from other groups.","title":"Selection Bias"},{"location":"terminology/#gradient","text":"Gradient The vector of partial derivatives with respect to all of the independent variables. In machine learning, the gradient is the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent. $$(y - y')^2$$ The derivative of above with respect to the weights and biases tells us how loss changes for a given example Gradient Descent A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss. Exploding Gradient Problem The tendency for gradients in a deep neural networks (especially recurrent neural networks) to become surprisingly steep (high). Steep gradients result in very large updates to the weights of each node in a deep neural network. Models suffering from the exploding gradient problem become difficult or impossible to train. Gradient clipping can mitigate this problem. Gradient Clipping A commonly used mechanism to mitigate the exploding gradient problem by artificially limiting (clipping) the maximum value of gradients when using gradient descent to train a model.","title":"Gradient"},{"location":"terminology/#convergence","text":"Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.","title":"Convergence"},{"location":"terminology/#underfitting","text":"Producing a model with poor predictive ability because the model hasn't captured the complexity of the training data. Many problems can cause underfitting, including: Training on the wrong set of features. Training for too few epochs or at too low a learning rate. Training with too high a regularization rate. Providing too few hidden layers in a deep neural network.","title":"Underfitting"},{"location":"terminology/#overfitting","text":"The algorithm analyses the data and trains itself to create a high mathematical order model based on the data. $$y = x_1 + w_2x_2^3 + w_3x_3^8$$ These high-order terms let this equation define a precise decision boundary between the positive and negative values, but as a result, the training process has created a model that works very well on training data but poorly when asked to predict values based on data it has not trained - this is the class overfit problem and is an issue that must be handled to create machine learning models that work well not only on the training data but also on real-world data. Regularization , Cross validation , Ensemble learning of which dropout is a part, are all ways to mitigate overfitting. We add an additional parameter where if the model coefficients get too complex we add a penalty to the objective function. This is the technique that we use in regression. Regularization: The penalty on a model's complexity. Different kinds of regularization include: L1 regularization L2 regularization Dropout regularization L1 is a type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with L2 regularization. L2 is a type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models. Dropout is a form of regularization useful in training neural networks. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. Regularization rate A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate's influence: $$\\text{minimize(loss function + }\\lambda\\text{(regularization function))}$$ Raising the regularization rate reduces overfitting but may make the model less accurate.","title":"Overfitting"},{"location":"terminology/#hyperplane","text":"A boundary that separates a space into two subspaces. For example, a line is a hyperplane in two dimensions and a plane is a hyperplane in three dimensions. More typically in machine learning, a hyperplane is the boundary separating a high-dimensional space. Kernel Support Vector Machines use hyperplanes to separate positive classes from negative classes, often in a very high-dimensional space.","title":"Hyperplane"},{"location":"terminology/#parameter","text":"A variable of a model that the ML system trains on its own. A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data. Often model parameters are estimated using an optimization algorithm, which is a type of efficient search through possible parameter values. They are required by the model when making predictions. They values define the skill of the model on your problem. They are estimated or learned from data. They are often not set manually by the practitioner. They are often saved as part of the learned model. Parameters are key to machine learning algorithms. They are the part of the model that is learned from historical training data. Statistics: In statistics, you may assume a distribution for a variable, such as a Gaussian distribution. Two parameters of the Gaussian distribution are the mean (mu) and the standard deviation (sigma). This holds in machine learning, where these parameters may be estimated from data and used as part of a predictive model. Programming: In programming, you may pass a parameter to a function. In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data. Some examples of model parameters include: The weights in an artificial neural network. The support vectors in a support vector machine. The coefficients in a linear regression or logistic regression.","title":"Parameter"},{"location":"terminology/#hyperparameter","text":"Hyperparameters are the configuration settings used to tune how the model is trained and it is external to the model, whose value cannot be estimated from data. They are often used in processes to help estimate model parameters. They are often specified by the practitioner. They can often be set using heuristics. They are often tuned for a given predictive modeling problem. We cannot know the best value for a hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error. When a machine learning algorithm is tuned for a specific problem, such as when you are using a grid search or a random search, then you are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions. Hyperparameters are often referred to as model parameters which can make things confusing. A good rule of thumb to overcome this confusion is as follows: If you have to specify a model parameter manually then it is probably a model hyperparameter. Some examples of model hyperparameters include: The learning rate for training a neural network. The C and sigma hyperparameters for support vector machines. The k in k-nearest neighbors. Grid Search is an approach to hyperparameter tuning that will methodically build and evaluate model for each combination of algorithm parameters specified in a grid. Learning Rate A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.","title":"Hyperparameter"},{"location":"terminology/#time-series-analysis","text":"A subfield of machine learning and statistics that analyzes temporal data. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data.","title":"Time Series Analysis"},{"location":"terminology/#keras","text":"A popular Python machine learning API. Keras runs on several deep learning frameworks, including TensorFlow, where it is made available as tf.keras.","title":"Keras"},{"location":"week_1/cost_function/","text":"","title":"Cost function"},{"location":"week_1/cost_function_intuition_1/","text":"","title":"Cost function intuition 1"},{"location":"week_1/cost_function_intuition_2/","text":"","title":"Cost function intuition 2"},{"location":"week_1/gd_for_linear_regression/","text":"","title":"Gd for linear regression"},{"location":"week_1/gradient_descent/","text":"","title":"Gradient descent"},{"location":"week_1/gradient_descent_intuition/","text":"","title":"Gradient descent intuition"},{"location":"week_1/model_representation/","text":"","title":"Model representation"}]}