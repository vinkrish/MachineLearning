{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Machine Learning? Arthur Samuel described it as: \"The field of study that gives computers the ability to learn without being explicitly programmed.\" This is an older, informal definition. Tom Mitchell provides a more modern definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" ML systems learn how to combine input to produce useful predictions on never-before-seen data. Example : playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning Unsupervised learning Supervised Learning In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. We try to extrapolate labels for new data given labelled data we already have Supervised learning problems are categorized into Regression and Classification problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. Commonly used algorithms: Linear Regression Logistics Regression Polynomial Regression Stepwise Regression Ridge Regression Lasso Regression ElasticNet Regression Support Vector Regression (SVR) In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Commonly used algorithms: Linear Classifier (Logistic Regression & Naive Bayes) Support Vector Machines Decision Trees Random Forest Neural Networks Nearest Neighbor Example 1 : (a) Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. (b) We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2 : (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. Unsupervised Learning Unsupervised learning allows us to approach problems with little or no idea what our results should look like. With unsupervised learning there is no feedback based on the prediction results. We try to classify data into groups and extract new information hidden in the data We can derive structure: From data where we don't necessarily know the effect of the variables. By clustering the data based on relationships among the variables in the data. Example : Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Commonly used algorithms: K-means Clustering Mean-Shift Clustering Density-Based Spatial Clustering of Applications(DBSCAN) Ex-Hierarchical Dimensionality Reduction (Principal Component Analysis) Ensemble learning In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone (eg: Random Forest).","title":"About"},{"location":"#what-is-machine-learning","text":"Arthur Samuel described it as: \"The field of study that gives computers the ability to learn without being explicitly programmed.\" This is an older, informal definition. Tom Mitchell provides a more modern definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" ML systems learn how to combine input to produce useful predictions on never-before-seen data. Example : playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning Unsupervised learning","title":"What is Machine Learning?"},{"location":"#supervised-learning","text":"In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. We try to extrapolate labels for new data given labelled data we already have Supervised learning problems are categorized into Regression and Classification problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. Commonly used algorithms: Linear Regression Logistics Regression Polynomial Regression Stepwise Regression Ridge Regression Lasso Regression ElasticNet Regression Support Vector Regression (SVR) In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Commonly used algorithms: Linear Classifier (Logistic Regression & Naive Bayes) Support Vector Machines Decision Trees Random Forest Neural Networks Nearest Neighbor Example 1 : (a) Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. (b) We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories. Example 2 : (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.","title":"Supervised Learning"},{"location":"#unsupervised-learning","text":"Unsupervised learning allows us to approach problems with little or no idea what our results should look like. With unsupervised learning there is no feedback based on the prediction results. We try to classify data into groups and extract new information hidden in the data We can derive structure: From data where we don't necessarily know the effect of the variables. By clustering the data based on relationships among the variables in the data. Example : Clustering : Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering : The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Commonly used algorithms: K-means Clustering Mean-Shift Clustering Density-Based Spatial Clustering of Applications(DBSCAN) Ex-Hierarchical Dimensionality Reduction (Principal Component Analysis)","title":"Unsupervised Learning"},{"location":"#ensemble-learning","text":"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone (eg: Random Forest).","title":"Ensemble learning"},{"location":"algorithms/","text":"Supervised Learning Linear regression is a method for finding the straight line or hyperplane that best fits a set of points. Logistic Regression is a powerful statistical way of modeling a binomial outcome with one or more explanatory variables. It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Examples: Credit Scoring Measuring the success rates of marketing campaigns Predicting the revenues of a certain product Is there going to be an earthquake on a particular day? Naive Bayes is based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. Examples: To mark an email as spam or not spam Classify a news article about technology, politics, or sports Check a piece of text expressing positive emotions, or negative emotions? Used for face recognition software Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side. Example: Finding mileage Document classification Image classification Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Example: Automobile price prediction (Gradient Boosting Regression - which uses several weak decision trees) Random Forest is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Example: Predicting Diabetes Unsupervised Learning k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. Density-based spatial clustering of applications with noise (DBSCAN) It is a density-based clustering non-parametric algorithm; given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components These are a versatile algorithms that can be used for any type of grouping. Some examples of use cases are: Behavioral segmentation: Segment by purchase history Segment by activities on application, website, or platform Define personas based on interests Create profiles based on activity monitoring Inventory categorization: Group inventory by sales activity Group inventory by manufacturing metrics Sorting sensor measurements: Detect activity types in motion sensors Group images Separate audio Identify groups in health monitoring Detecting bots or anomalies: Separate valid activity groups from bots Group valid activity to clean up outlier detection Something Else Reinforcement Learning , where we try to create a model that learns the rules of an environment to best maximize its return or reward.","title":"Definition"},{"location":"algorithms/#supervised-learning","text":"Linear regression is a method for finding the straight line or hyperplane that best fits a set of points. Logistic Regression is a powerful statistical way of modeling a binomial outcome with one or more explanatory variables. It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Examples: Credit Scoring Measuring the success rates of marketing campaigns Predicting the revenues of a certain product Is there going to be an earthquake on a particular day? Naive Bayes is based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. Examples: To mark an email as spam or not spam Classify a news article about technology, politics, or sports Check a piece of text expressing positive emotions, or negative emotions? Used for face recognition software Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side. Example: Finding mileage Document classification Image classification Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Example: Automobile price prediction (Gradient Boosting Regression - which uses several weak decision trees) Random Forest is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Example: Predicting Diabetes","title":"Supervised Learning"},{"location":"algorithms/#unsupervised-learning","text":"k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. Density-based spatial clustering of applications with noise (DBSCAN) It is a density-based clustering non-parametric algorithm; given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components These are a versatile algorithms that can be used for any type of grouping. Some examples of use cases are: Behavioral segmentation: Segment by purchase history Segment by activities on application, website, or platform Define personas based on interests Create profiles based on activity monitoring Inventory categorization: Group inventory by sales activity Group inventory by manufacturing metrics Sorting sensor measurements: Detect activity types in motion sensors Group images Separate audio Identify groups in health monitoring Detecting bots or anomalies: Separate valid activity groups from bots Group valid activity to clean up outlier detection","title":"Unsupervised Learning"},{"location":"algorithms/#something-else","text":"Reinforcement Learning , where we try to create a model that learns the rules of an environment to best maximize its return or reward.","title":"Something Else"},{"location":"applications/","text":"Machine Learning creates a function that maps an input to an output. With deep learning, we can use a wide variety of data as input into this function, including tabular data, text, images, audio, and video. In addition, deep learning can map these inputs to a variety of outputs, including once again, tabular data, text, images, audio, and video. We're going to cover a few of the key deep learning applications that currently exist today, and some future applications that are just around the corner. Tables Let's start simple by looking at applications that use tabular data as both input and output. Tabular data are categorical and numeric values stored in columns and rows of the table. There are roughly four general categories of tasks that we can perform on tabular data with deep learning. Classification is where we attempt to make a decision or a prediction involving two or more categories or outcomes. For example, deciding whether to accept or reject a loan based on data from a customer's financial history. Regression is where we attempt to predict a numeric outcome based on one or more input variables. For example, trying to predict the sale price of a house based on the features of the house compared to the sale price of similar houses. Clustering is where we attempt to group similar objects together based on similarities in their data. For example, grouping customers into marketing segments based on their income, age, gender, number of children, etc. Anomaly Detection is where we find observations in the data that are different from the normal data. For example, detecting an unusual spike in the number of negative comments about a product that we've just released. While all four of these tasks can be performed on tabular data using deep neural networks, oftentimes deep learning is overkill for these types of tasks. This is typically the case when the tabular dataset is small, or the function we are attempting to model is relatively simple. These types of problems are generally solved more effectively using traditional machine learning tools, for example,decision tree classifiers, support vector machines, k-means clustering, or shallow, rather than deep neural networks. However, there are cases where the tabular datasets are large enough, and the function that we are attempting to model is complex enough that a deep neural network makes sense. For example, Stanford University announced that it had created a deep neural network to predict whether patients admitted to a hospital will die within the next year or not. The deep neural network was able to predict patient mortality with approximately 90% accuracy. This training algorithm used over 13,000 columns and over 220,000 rows of tabular data as input in order to reach this level of prediction accuracy. Text Now let's learn about deep learning applications for textual data; that is, bodies of text contained in documents. With deep learning, we can map the text and the document as an input to a variety of outputs. Examples : we can perform document classification . This is where we attempt to predict which category a document belongs to. i.e, predicting whether the topic of a news article is technology, sports, or entertainment. Natural language processing has also received tremendous benefits from deep learning as well. For example, deep neural networks can now understand the grammar, syntax, and meaning of sentences. Sentiment analysis is where we take a body of text as input, and determine the author's emotions behind the words. In the example below, we can see words and phrases that either have a positive sentiment in blue, neutral sentiment in gray, or a negative sentiment in red. We can also use deep learning to generate text as an output from a variety of inputs as well. For example, in the last module, we saw how recurrent neural networks can be used to predict the next letter someone is about to type based on the previous letters that they've already typed. However, we can also use recurrent neural networks to predict which word someone is likely to type based on all of the other words that they've previously typed. In addition, we can also use recurrent neural networks for language translation . This is where we translate words and phrases from one language into another language. You may have noticed how much better software based language translation has gotten in recent years. This is all thanks to advances in deep recurrent neural networks. We can train a recurrent neural network to generate headlines and summaries for bodies of text based on the content of the document. The computer generated headlines you see here are almost indistinguishable from the headlines that a human would create to summarize a story. Images With deep learning, we can map images as an input to a variety of outputs. For example: we can perform tasks like object recognition where we detect and provide labels for objects in an image. Gender classification, where we determine the gender of a subject in an image. Age regression, where we predict how old a person is in a photo. Emotion detection, where we predict the emotional state portrayed on a person's face. We can also perform more advanced image analysis tasks by combining a convolutional neural network with a deconvolutional neural network, an architecture known as a convolutional encoder decoder . This allows us to perform tasks like pixel wise image segmentation, where we extract pixels corresponding to specific objects in an image. For example, all of the pixels of the car in the center of this image have been color coded yellow. We can also combine a CNN and an RNN to perform image captioning . Image captioning is where we generate a description of what is happening in an image. As you can see from these three examples, the deep learning algorithm is able to provide a caption to each image that is almost indistinguishable from the caption a human would provide. However, we can also use deep learning to generate images as an output using a variety of different types of input. For example, super resolution convolutional neural networks have been applied to images to attempt to enhance their image resolution. As we can see, given an 8x8 array of pixels, the CNN is able to produce a 32x32 image that closely approximates the true image. We can also use generative adversarial networks to clean up the noise in an image as well. In fact, entire sections of this image can be missing, and the GAN will fill in the missing pixels, given the information surrounding the missing section. We can also use generative adversarial networks to modify existing images with new predictions. For example, we can take an image of yours truly and use a GAN to generate a prediction of what I'll look like in 20 years. In fact, this deep learning application is so good that it tricked the age detection algorithms into thinking that I was exactly 20 years older. We can also use deep learning to generate entirely new images from scratch. And how about these fake celebrities? Each of these images was created by a generative adversarial network. They may feel like real celebrities since the GAN was trained using more than 200,000 high-resolution images of real celebrities; however, not a single one of these synthetic celebrities is a real person. (Working) Finally, we're rapidly nearing a point in time where we can synthesize new images based on a description of what we want the image to look like. For example, all of these images were generated by a generative adversarial network using the descriptions above. It won't be long before we can just tell a computer what kind of image we need for our document or presentation, and it will synthesize an entirely new image to match our description. Audio With deep learning, we can map audio as an input to a wide variety of outputs. For example, we can use combinations of CNNs and RNNs to classify the source of a sound. This allows computers to recognize the song that you're listening to based on the music it's hearing. It also allows computers to recognize animals, vehicles, and environmental noises by sound alone. In addition, we can also use combinations of CNNs and RNNs to perform speech to text translation at near human level accuracy. If you've noticed how much better your smart phone has gotten at interpreting your commands in the past few years, once again, you can thank deep learning for the recent improvements. We can also use combinations of CNN and RNN encoder/decoder networks to perform real-time speech translation. For example, I can speak a sentence in one language, and in near real-time, the listener will hear what I said in their native language. But we can also use deep learning to generate audio as an output, given a variety of inputs. For example, using a type of deep neural network architecture known as a dilated causal convolutional neural network, we can have deep learning speech synthesis that sounds almost indistinguishable from actual human speech. Finally, here's an experimental application that's currently in development by Princeton University and Adobe. It allows you to feed in your audio recordings, it converts your audio into a transcript, and then it allows you to edit the text just like a text editor. Then after you've made your changes to the text, it synthesizes new audio that sounds almost identical to your own voice. While this specific application might be a few years away yet. Video With deep learning, we can map video as an input to a variety of types of outputs. For example, Tasks like object detection, where we classify an object as it's moving from frame to frame; facial recognition, where we recognize a person's face in a moving video; and outcome prediction, where we analyze a few frames of vide to attempt to predict what will happen next. Plus, this is also leading to entirely new applications like real-time sign language translation and computer-based lip reading. In fact, real-time sign language translation is rapidly approaching human level proficiency, and Google's LipNet is currently better than the average human lip reader. However, even more impressive is that we can now use deep learning to generate video as an output from a variety of inputs. For example, using a combination of convolutional encoder/decoder networks, we can now teach a computer to automatically colorize black and white videos. In addition, machines are also being taught to restore old or damaged films as well. And check this out. To help families refinance their homes. To invest in things like high-tech manufacturing, clean energy, and the infrastructure that creates good new jobs. You can see why it's so important that people understand the potential applications for deep learning before someone tries to use deep learning to deceive others for nefarious purposes. Future Finally, let's look at other future deep learning applications beyond the basic input to output mappings that we've already seen. It's important to note that what we've seen so far is just the beginning of deep learning technologies. Deep learning can be applied to a variety of other inputs, and produce a variety of other outputs. Plus, we can combine multiple types of inputs and produce multiple types of outputs as well. This leads to a whole new realm of possible applications for deep learning in the future. For example, self-driving cars connect a variety of sensor inputs with a set of vehicle control outputs to drive an automobile without any human intervention. This same deep learning technology can also be applied to other types of vehicles like semis, buses, boats, airplanes and drones as well.","title":"Applications"},{"location":"applications/#tables","text":"Let's start simple by looking at applications that use tabular data as both input and output. Tabular data are categorical and numeric values stored in columns and rows of the table. There are roughly four general categories of tasks that we can perform on tabular data with deep learning. Classification is where we attempt to make a decision or a prediction involving two or more categories or outcomes. For example, deciding whether to accept or reject a loan based on data from a customer's financial history. Regression is where we attempt to predict a numeric outcome based on one or more input variables. For example, trying to predict the sale price of a house based on the features of the house compared to the sale price of similar houses. Clustering is where we attempt to group similar objects together based on similarities in their data. For example, grouping customers into marketing segments based on their income, age, gender, number of children, etc. Anomaly Detection is where we find observations in the data that are different from the normal data. For example, detecting an unusual spike in the number of negative comments about a product that we've just released. While all four of these tasks can be performed on tabular data using deep neural networks, oftentimes deep learning is overkill for these types of tasks. This is typically the case when the tabular dataset is small, or the function we are attempting to model is relatively simple. These types of problems are generally solved more effectively using traditional machine learning tools, for example,decision tree classifiers, support vector machines, k-means clustering, or shallow, rather than deep neural networks. However, there are cases where the tabular datasets are large enough, and the function that we are attempting to model is complex enough that a deep neural network makes sense. For example, Stanford University announced that it had created a deep neural network to predict whether patients admitted to a hospital will die within the next year or not. The deep neural network was able to predict patient mortality with approximately 90% accuracy. This training algorithm used over 13,000 columns and over 220,000 rows of tabular data as input in order to reach this level of prediction accuracy.","title":"Tables"},{"location":"applications/#text","text":"Now let's learn about deep learning applications for textual data; that is, bodies of text contained in documents. With deep learning, we can map the text and the document as an input to a variety of outputs. Examples : we can perform document classification . This is where we attempt to predict which category a document belongs to. i.e, predicting whether the topic of a news article is technology, sports, or entertainment. Natural language processing has also received tremendous benefits from deep learning as well. For example, deep neural networks can now understand the grammar, syntax, and meaning of sentences. Sentiment analysis is where we take a body of text as input, and determine the author's emotions behind the words. In the example below, we can see words and phrases that either have a positive sentiment in blue, neutral sentiment in gray, or a negative sentiment in red. We can also use deep learning to generate text as an output from a variety of inputs as well. For example, in the last module, we saw how recurrent neural networks can be used to predict the next letter someone is about to type based on the previous letters that they've already typed. However, we can also use recurrent neural networks to predict which word someone is likely to type based on all of the other words that they've previously typed. In addition, we can also use recurrent neural networks for language translation . This is where we translate words and phrases from one language into another language. You may have noticed how much better software based language translation has gotten in recent years. This is all thanks to advances in deep recurrent neural networks. We can train a recurrent neural network to generate headlines and summaries for bodies of text based on the content of the document. The computer generated headlines you see here are almost indistinguishable from the headlines that a human would create to summarize a story.","title":"Text"},{"location":"applications/#images","text":"With deep learning, we can map images as an input to a variety of outputs. For example: we can perform tasks like object recognition where we detect and provide labels for objects in an image. Gender classification, where we determine the gender of a subject in an image. Age regression, where we predict how old a person is in a photo. Emotion detection, where we predict the emotional state portrayed on a person's face. We can also perform more advanced image analysis tasks by combining a convolutional neural network with a deconvolutional neural network, an architecture known as a convolutional encoder decoder . This allows us to perform tasks like pixel wise image segmentation, where we extract pixels corresponding to specific objects in an image. For example, all of the pixels of the car in the center of this image have been color coded yellow. We can also combine a CNN and an RNN to perform image captioning . Image captioning is where we generate a description of what is happening in an image. As you can see from these three examples, the deep learning algorithm is able to provide a caption to each image that is almost indistinguishable from the caption a human would provide. However, we can also use deep learning to generate images as an output using a variety of different types of input. For example, super resolution convolutional neural networks have been applied to images to attempt to enhance their image resolution. As we can see, given an 8x8 array of pixels, the CNN is able to produce a 32x32 image that closely approximates the true image. We can also use generative adversarial networks to clean up the noise in an image as well. In fact, entire sections of this image can be missing, and the GAN will fill in the missing pixels, given the information surrounding the missing section. We can also use generative adversarial networks to modify existing images with new predictions. For example, we can take an image of yours truly and use a GAN to generate a prediction of what I'll look like in 20 years. In fact, this deep learning application is so good that it tricked the age detection algorithms into thinking that I was exactly 20 years older. We can also use deep learning to generate entirely new images from scratch. And how about these fake celebrities? Each of these images was created by a generative adversarial network. They may feel like real celebrities since the GAN was trained using more than 200,000 high-resolution images of real celebrities; however, not a single one of these synthetic celebrities is a real person. (Working) Finally, we're rapidly nearing a point in time where we can synthesize new images based on a description of what we want the image to look like. For example, all of these images were generated by a generative adversarial network using the descriptions above. It won't be long before we can just tell a computer what kind of image we need for our document or presentation, and it will synthesize an entirely new image to match our description.","title":"Images"},{"location":"applications/#audio","text":"With deep learning, we can map audio as an input to a wide variety of outputs. For example, we can use combinations of CNNs and RNNs to classify the source of a sound. This allows computers to recognize the song that you're listening to based on the music it's hearing. It also allows computers to recognize animals, vehicles, and environmental noises by sound alone. In addition, we can also use combinations of CNNs and RNNs to perform speech to text translation at near human level accuracy. If you've noticed how much better your smart phone has gotten at interpreting your commands in the past few years, once again, you can thank deep learning for the recent improvements. We can also use combinations of CNN and RNN encoder/decoder networks to perform real-time speech translation. For example, I can speak a sentence in one language, and in near real-time, the listener will hear what I said in their native language. But we can also use deep learning to generate audio as an output, given a variety of inputs. For example, using a type of deep neural network architecture known as a dilated causal convolutional neural network, we can have deep learning speech synthesis that sounds almost indistinguishable from actual human speech. Finally, here's an experimental application that's currently in development by Princeton University and Adobe. It allows you to feed in your audio recordings, it converts your audio into a transcript, and then it allows you to edit the text just like a text editor. Then after you've made your changes to the text, it synthesizes new audio that sounds almost identical to your own voice. While this specific application might be a few years away yet.","title":"Audio"},{"location":"applications/#video","text":"With deep learning, we can map video as an input to a variety of types of outputs. For example, Tasks like object detection, where we classify an object as it's moving from frame to frame; facial recognition, where we recognize a person's face in a moving video; and outcome prediction, where we analyze a few frames of vide to attempt to predict what will happen next. Plus, this is also leading to entirely new applications like real-time sign language translation and computer-based lip reading. In fact, real-time sign language translation is rapidly approaching human level proficiency, and Google's LipNet is currently better than the average human lip reader. However, even more impressive is that we can now use deep learning to generate video as an output from a variety of inputs. For example, using a combination of convolutional encoder/decoder networks, we can now teach a computer to automatically colorize black and white videos. In addition, machines are also being taught to restore old or damaged films as well. And check this out. To help families refinance their homes. To invest in things like high-tech manufacturing, clean energy, and the infrastructure that creates good new jobs. You can see why it's so important that people understand the potential applications for deep learning before someone tries to use deep learning to deceive others for nefarious purposes.","title":"Video"},{"location":"applications/#future","text":"Finally, let's look at other future deep learning applications beyond the basic input to output mappings that we've already seen. It's important to note that what we've seen so far is just the beginning of deep learning technologies. Deep learning can be applied to a variety of other inputs, and produce a variety of other outputs. Plus, we can combine multiple types of inputs and produce multiple types of outputs as well. This leads to a whole new realm of possible applications for deep learning in the future. For example, self-driving cars connect a variety of sensor inputs with a set of vehicle control outputs to drive an automobile without any human intervention. This same deep learning technology can also be applied to other types of vehicles like semis, buses, boats, airplanes and drones as well.","title":"Future"},{"location":"big_data/","text":"Big Data Hadoop Hadoop is an open source software platform for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. Ambari Ambari is an Apache project supported by Hortonworks. It offers a web based GUI (Graphical User Interface) with wizard scripts for setting up clusters with most of the standard components. Ambari provisions, manages and monitors all the clusters of Hadoop jobs. MapReduce MapReduce distributes the processing of data on your cluster. Divides your data up intro partitions that are MAPPED (transformed) and REDUCED (aggregated) by mapper and reducer functions you define. MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure (or method) - which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method - which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. In computer science, marshalling is the process of transforming the memory representation of an object to a data format suitable for storage or transmission, and it is typically used when data must be moved between different parts of a computer program or from one program to another. Marshalling is similar to serialization and is used to communicate to remote objects with an object, in this case a serialized object. HDFS (Hadoop Distributed File System) The HDFS, distributed under Apache license offers a basic framework for splitting up data collections between multiple nodes. In HDFS, the large files are broken into blocks, where several nodes hold all of the blocks from a file. The file system is designed in a way to mix fault tolerance with high throughput. The blocks of HDFS are loaded to maintain steady streaming. They are not usually cached to minimize latency. Pig When the data stored is visible to Hadoop, Apache Pig dives into the data and runs the code that is written in its own language, called Pig Latin. Pig Latin is filled with abstractions for handling the data. Pig comes with standard functions for common tasks like averaging data, working with dates, or to find differences between strings. Pig also allows the user to write languages of their own, called UDF (User Defined Function), when the standard functions fall short. Hive If you are already fluent with SQL, then you can leverage Hadoop using Hive. Hive was developed by some folks at Facebook. Apache Hive regulates the process of extracting bits from all the files in HBase. It supports analysis of large datasets stored in Hadoop\u2019s HDFS and compatible file systems. It also provides an SQL like language called HSQL (HiveSQL) that gets into the files and extracts the required snippets for the code. TEZ A Framework for YARN-based, Data Processing Applications in Hadoop. Apache Tez is an extensible framework for building high performance batch and interactive data processing applications, coordinated by YARN in Apache Hadoop. Tez improves the MapReduce paradigm by dramatically improving its speed, while maintaining MapReduce\u2019s ability to scale to petabytes of data. Important Hadoop ecosystem projects like Apache Hive and Apache Pig use Apache Tez. Spark Spark is the next generation that pretty much works like Hadoop that processes data cached in the memory. Its objective is to make data analysis fast to run and write with a general execution model. This can optimize arbitrary operator graphs and support in-memory computing, which lets it query data faster than disk-based engines like Hadoop. It also supports streaming data, run ML algorithms on clusters. HBase HBase is a column-oriented database management system that runs on top of HDFS. HBase applications are written in Java, very much like the MapReduce application. It comprises a set of tables, where each table contains rows and columns like a traditional database. When the data falls into the big table, HBase will store the data, search it and automatically share the table across multiple nodes so that MapReduce jobs can run it locally. HBase offers a limited guarantee for some local changes. The changes that happen in a single row can succeed or fail at the same time. Storm A system for processing streaming data in real time. Apache Storm adds reliable real-time data processing capabilities to Enterprise Hadoop. Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations. NoSQL Some Hadoop clusters integrate with NoSQL data stores that come with their own mechanisms for storing data across a cluster of nodes. This allows them to store and retrieve data with all the features of the NoSQL database, after which Hadoop can be used to schedule data analysis jobs on the same cluster. Mahout Mahout is designed to implement a great number of algorithms, classifications and filtering of data analysis to Hadoop cluster. Many of the standard algorithms like K-means, Dirichelet, parallel pattern and Bayesian classifications are ready to run on the data with a Hadoop style Map and reduce. Lucene/Solr Lucene, written in Java integrates easily with Hadoop and is a natural companion for Hadoop. It is a tool meant for indexing large blocks of unstructured text. Lucene handles the indexing, while Hadoop handles the distributed queries across the cluster. Lucene-Hadoop features are rapidly evolving as new projects are being developed. Avro Avro is a serialization system that bundles the data together with a schema for understanding it. Each packet comes with a JSON data structure. JSON explains how the data can be parsed. The header of JSON specifies the structure for the data, where the need to write extra tags in the data to mark the fields can be avoided. The output is considerably more compact than the traditional formats like XML. Oozie A job can be simplified by breaking it into steps. On breaking the project in to multiple Hadoop jobs, Oozie starts processing them in the right sequence. It manages the workflow as specified by DAG (Directed Acyclic Graph) and there is no need for timely monitor. Sqoop Apache Sqoop is specially designed to transfer bulk data efficiently from the traditional databases into Hive or HBase. It can also be used to extract data from Hadoop and export it to external structured data-stores like relational databases and enterprise data warehouses. Sqoop is a command line tool, mapping between the tables and the data storage layer, translating the tables into a configurable combination of HDFS, HBase or Hive. Flume Gathering all the data is equal to storing and analyzing it. Apache Flume dispatches \u2018special agents\u2019 to gather information that will be stored in HDFS. The information gathered can be log files, Twitter API, or website scraps. These data can be chained and subjected to analyses. Kafka Apache Kafka is an open-source stream-processing software. It aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a \"massively scalable pub/sub message queue designed as a distributed transaction log\". Zookeeper Zookeeper is a centralized service that maintains, configures information, gives a name and provides distributed synchronization across a cluster. It imposes a file system-like hierarchy on the cluster and stores all of the metadata for the machines, so we can synchronize the work of the various machines.","title":"Big Data"},{"location":"big_data/#big-data","text":"","title":"Big Data"},{"location":"big_data/#hadoop","text":"Hadoop is an open source software platform for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.","title":"Hadoop"},{"location":"big_data/#ambari","text":"Ambari is an Apache project supported by Hortonworks. It offers a web based GUI (Graphical User Interface) with wizard scripts for setting up clusters with most of the standard components. Ambari provisions, manages and monitors all the clusters of Hadoop jobs.","title":"Ambari"},{"location":"big_data/#mapreduce","text":"MapReduce distributes the processing of data on your cluster. Divides your data up intro partitions that are MAPPED (transformed) and REDUCED (aggregated) by mapper and reducer functions you define. MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure (or method) - which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method - which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. In computer science, marshalling is the process of transforming the memory representation of an object to a data format suitable for storage or transmission, and it is typically used when data must be moved between different parts of a computer program or from one program to another. Marshalling is similar to serialization and is used to communicate to remote objects with an object, in this case a serialized object.","title":"MapReduce"},{"location":"big_data/#hdfs-hadoop-distributed-file-system","text":"The HDFS, distributed under Apache license offers a basic framework for splitting up data collections between multiple nodes. In HDFS, the large files are broken into blocks, where several nodes hold all of the blocks from a file. The file system is designed in a way to mix fault tolerance with high throughput. The blocks of HDFS are loaded to maintain steady streaming. They are not usually cached to minimize latency.","title":"HDFS (Hadoop Distributed File System)"},{"location":"big_data/#pig","text":"When the data stored is visible to Hadoop, Apache Pig dives into the data and runs the code that is written in its own language, called Pig Latin. Pig Latin is filled with abstractions for handling the data. Pig comes with standard functions for common tasks like averaging data, working with dates, or to find differences between strings. Pig also allows the user to write languages of their own, called UDF (User Defined Function), when the standard functions fall short.","title":"Pig"},{"location":"big_data/#hive","text":"If you are already fluent with SQL, then you can leverage Hadoop using Hive. Hive was developed by some folks at Facebook. Apache Hive regulates the process of extracting bits from all the files in HBase. It supports analysis of large datasets stored in Hadoop\u2019s HDFS and compatible file systems. It also provides an SQL like language called HSQL (HiveSQL) that gets into the files and extracts the required snippets for the code.","title":"Hive"},{"location":"big_data/#tez","text":"A Framework for YARN-based, Data Processing Applications in Hadoop. Apache Tez is an extensible framework for building high performance batch and interactive data processing applications, coordinated by YARN in Apache Hadoop. Tez improves the MapReduce paradigm by dramatically improving its speed, while maintaining MapReduce\u2019s ability to scale to petabytes of data. Important Hadoop ecosystem projects like Apache Hive and Apache Pig use Apache Tez.","title":"TEZ"},{"location":"big_data/#spark","text":"Spark is the next generation that pretty much works like Hadoop that processes data cached in the memory. Its objective is to make data analysis fast to run and write with a general execution model. This can optimize arbitrary operator graphs and support in-memory computing, which lets it query data faster than disk-based engines like Hadoop. It also supports streaming data, run ML algorithms on clusters.","title":"Spark"},{"location":"big_data/#hbase","text":"HBase is a column-oriented database management system that runs on top of HDFS. HBase applications are written in Java, very much like the MapReduce application. It comprises a set of tables, where each table contains rows and columns like a traditional database. When the data falls into the big table, HBase will store the data, search it and automatically share the table across multiple nodes so that MapReduce jobs can run it locally. HBase offers a limited guarantee for some local changes. The changes that happen in a single row can succeed or fail at the same time.","title":"HBase"},{"location":"big_data/#storm","text":"A system for processing streaming data in real time. Apache Storm adds reliable real-time data processing capabilities to Enterprise Hadoop. Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations.","title":"Storm"},{"location":"big_data/#nosql","text":"Some Hadoop clusters integrate with NoSQL data stores that come with their own mechanisms for storing data across a cluster of nodes. This allows them to store and retrieve data with all the features of the NoSQL database, after which Hadoop can be used to schedule data analysis jobs on the same cluster.","title":"NoSQL"},{"location":"big_data/#mahout","text":"Mahout is designed to implement a great number of algorithms, classifications and filtering of data analysis to Hadoop cluster. Many of the standard algorithms like K-means, Dirichelet, parallel pattern and Bayesian classifications are ready to run on the data with a Hadoop style Map and reduce.","title":"Mahout"},{"location":"big_data/#lucenesolr","text":"Lucene, written in Java integrates easily with Hadoop and is a natural companion for Hadoop. It is a tool meant for indexing large blocks of unstructured text. Lucene handles the indexing, while Hadoop handles the distributed queries across the cluster. Lucene-Hadoop features are rapidly evolving as new projects are being developed.","title":"Lucene/Solr"},{"location":"big_data/#avro","text":"Avro is a serialization system that bundles the data together with a schema for understanding it. Each packet comes with a JSON data structure. JSON explains how the data can be parsed. The header of JSON specifies the structure for the data, where the need to write extra tags in the data to mark the fields can be avoided. The output is considerably more compact than the traditional formats like XML.","title":"Avro"},{"location":"big_data/#oozie","text":"A job can be simplified by breaking it into steps. On breaking the project in to multiple Hadoop jobs, Oozie starts processing them in the right sequence. It manages the workflow as specified by DAG (Directed Acyclic Graph) and there is no need for timely monitor.","title":"Oozie"},{"location":"big_data/#sqoop","text":"Apache Sqoop is specially designed to transfer bulk data efficiently from the traditional databases into Hive or HBase. It can also be used to extract data from Hadoop and export it to external structured data-stores like relational databases and enterprise data warehouses. Sqoop is a command line tool, mapping between the tables and the data storage layer, translating the tables into a configurable combination of HDFS, HBase or Hive.","title":"Sqoop"},{"location":"big_data/#flume","text":"Gathering all the data is equal to storing and analyzing it. Apache Flume dispatches \u2018special agents\u2019 to gather information that will be stored in HDFS. The information gathered can be log files, Twitter API, or website scraps. These data can be chained and subjected to analyses.","title":"Flume"},{"location":"big_data/#kafka","text":"Apache Kafka is an open-source stream-processing software. It aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a \"massively scalable pub/sub message queue designed as a distributed transaction log\".","title":"Kafka"},{"location":"big_data/#zookeeper","text":"Zookeeper is a centralized service that maintains, configures information, gives a name and provides distributed synchronization across a cluster. It imposes a file system-like hierarchy on the cluster and stores all of the metadata for the machines, so we can synchronize the work of the various machines.","title":"Zookeeper"},{"location":"dap/","text":"This process will help you understand, explore and use your data intelligently so that you make the most of the information you're given. Five steps: Question Wrangle Explore Draw conclusions Communicate. Question The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights. Wrangle Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with. Explore Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data. Conclusions After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately. Communicate Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations","title":"Data Analysis Process"},{"location":"dap/#question","text":"The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights.","title":"Question"},{"location":"dap/#wrangle","text":"Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with.","title":"Wrangle"},{"location":"dap/#explore","text":"Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data.","title":"Explore"},{"location":"dap/#conclusions","text":"After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately.","title":"Conclusions"},{"location":"dap/#communicate","text":"Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations","title":"Communicate"},{"location":"data_science/","text":"Questions Data Science Answers Is it A or B? : Classfication algorithms whether a coupon or discount bring more customer. Is this weird? : Anomaly detection algorithms credit card spending habit, flags unsual behaviour. How much? How many? : Regression algorithms what will my nth quarter sales be. How is this organized? : Clustering algorithms which viewers like same type of movies, which printer model fails same way. What should I do now? : Reinforcement Learning algorithms self driving car, whether to brake or accelerate at yellow light. Is data ready for Data Science Relevant? Connected? Accurate? Enough to work with? New model building workflow Gather data Feature extraction Pick ML framework Spin up AWS EC2 instance Setup machine Launch training job Analyze results When you have a standard computer, and you want to run multiple processes on it, the operating system is what deals with the scheduling - it determines which process gets access to which processor at a given point in time, which process gets access to which parts of memory, decides which processes should be frozen or die in cases where there is a shortage of resources. Use Docker to containerize model development environment, setup code and dependencies on each remote machine. Kubernetes is an open-source container-orchestration system for automating application deployment, scaling and management. These tools allow for easy to use debugging and monitoring. Borg is a Cluster OS Google uses for managing internal workloads. It manages thousands of machines with very good internal network connectivity. When you want to run a \u201cservice\u201d - which is the equivalent of a \u201cdaemon\u201d on a normal machine, something that should stay up all the time - you \u201crun it on Borg\u201d - that is, you tell the Borg cluster scheduler that you want, say, 200 copies of the binary that determines the service. The Borg scheduler identifies machines in the cluster that will have the spare capacity to run your service and sends a request to run the service to the node agents on these machines. Descriptive Statistics Descriptive statistics summarize a given data set, which can be either a representation of the entire or a sample of a population. It is broken down into measure of central tendency (mean, median & mode) and measure of variability (standard deviation, variance & skewness). Inferential Statistics It is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and driving estimates. It is assumed that the observed data set is sampled from a larger population. Recommendation System Collaborative Filtering Collaborative filtering systems recommend items based on how well users prefer those items over others. It's based on crowdsourced user preference data. Two approaches: User based: Systems that recommend items based on similarity between users. eg. customer who are similar to you like x,y,z products so you might like this as well. Item based: Items recommended to you - people who like this product also like x,y,z product. Generate recommendation based on similarity between items with respect to user ratings or purchase history. Content Based Recommenders Content-based recommenders recommend items based on their features and how similar those are to features of other items in a dataset. eg: Pandora uses this for music recommendation. Popularity Based Recommenders Popularity-based recommenders offer a very primitive form of collaborative filtering, where items are recommended to users based on how popular those items are among other users. Rely on purchase history data Used by online news sites Cannot produce personlized results Correlation Based Recommendations Use Pearson's r correlation to recommend an item that is most similar to the item a user has already purchased. Recommend an item based on how well it correlates with other items with respect to use ratings.","title":"Intro"},{"location":"data_science/#questions-data-science-answers","text":"Is it A or B? : Classfication algorithms whether a coupon or discount bring more customer. Is this weird? : Anomaly detection algorithms credit card spending habit, flags unsual behaviour. How much? How many? : Regression algorithms what will my nth quarter sales be. How is this organized? : Clustering algorithms which viewers like same type of movies, which printer model fails same way. What should I do now? : Reinforcement Learning algorithms self driving car, whether to brake or accelerate at yellow light.","title":"Questions Data Science Answers"},{"location":"data_science/#is-data-ready-for-data-science","text":"Relevant? Connected? Accurate? Enough to work with?","title":"Is data ready for Data Science"},{"location":"data_science/#new-model-building-workflow","text":"Gather data Feature extraction Pick ML framework Spin up AWS EC2 instance Setup machine Launch training job Analyze results When you have a standard computer, and you want to run multiple processes on it, the operating system is what deals with the scheduling - it determines which process gets access to which processor at a given point in time, which process gets access to which parts of memory, decides which processes should be frozen or die in cases where there is a shortage of resources. Use Docker to containerize model development environment, setup code and dependencies on each remote machine. Kubernetes is an open-source container-orchestration system for automating application deployment, scaling and management. These tools allow for easy to use debugging and monitoring. Borg is a Cluster OS Google uses for managing internal workloads. It manages thousands of machines with very good internal network connectivity. When you want to run a \u201cservice\u201d - which is the equivalent of a \u201cdaemon\u201d on a normal machine, something that should stay up all the time - you \u201crun it on Borg\u201d - that is, you tell the Borg cluster scheduler that you want, say, 200 copies of the binary that determines the service. The Borg scheduler identifies machines in the cluster that will have the spare capacity to run your service and sends a request to run the service to the node agents on these machines.","title":"New model building workflow"},{"location":"data_science/#descriptive-statistics","text":"Descriptive statistics summarize a given data set, which can be either a representation of the entire or a sample of a population. It is broken down into measure of central tendency (mean, median & mode) and measure of variability (standard deviation, variance & skewness).","title":"Descriptive Statistics"},{"location":"data_science/#inferential-statistics","text":"It is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and driving estimates. It is assumed that the observed data set is sampled from a larger population.","title":"Inferential Statistics"},{"location":"data_science/#recommendation-system","text":"","title":"Recommendation System"},{"location":"data_science/#collaborative-filtering","text":"Collaborative filtering systems recommend items based on how well users prefer those items over others. It's based on crowdsourced user preference data. Two approaches: User based: Systems that recommend items based on similarity between users. eg. customer who are similar to you like x,y,z products so you might like this as well. Item based: Items recommended to you - people who like this product also like x,y,z product. Generate recommendation based on similarity between items with respect to user ratings or purchase history.","title":"Collaborative Filtering"},{"location":"data_science/#content-based-recommenders","text":"Content-based recommenders recommend items based on their features and how similar those are to features of other items in a dataset. eg: Pandora uses this for music recommendation.","title":"Content Based Recommenders"},{"location":"data_science/#popularity-based-recommenders","text":"Popularity-based recommenders offer a very primitive form of collaborative filtering, where items are recommended to users based on how popular those items are among other users. Rely on purchase history data Used by online news sites Cannot produce personlized results","title":"Popularity Based Recommenders"},{"location":"data_science/#correlation-based-recommendations","text":"Use Pearson's r correlation to recommend an item that is most similar to the item a user has already purchased. Recommend an item based on how well it correlates with other items with respect to use ratings.","title":"Correlation Based Recommendations"},{"location":"deep_learning/","text":"Deep Learning Deep Learning is a form of artificial intelligence that uses a type of machine learning called an artificial neural network with multiple hidden layers that learns hierarchical representations of the underlying data in order to make predictions given new data. Deep Learning attempts to model high-level abstractions about data using networks of graphs. It is focused on learning representations in data. Additionally, modeling high-level abstractions about data is very similar to artificial intelligence \u2014 the idea that knowledge can be represented and acted upon intelligently. Artificial Intelligence AI is a field of computer science that attempts to create machines that act rationally in response to their environment. Explicit Programming Encoding Domain Knowledge Statistical Patterns Detection Neural Network It is a machine learning algorihtm based on a very crude approximation of biological neural network in a brain. If we connect a series of artificial neurons in a network, we get an artificial neural network or neural network. Forward Propagation: we use the network with its current parameters to compute a prediction for each example in our training dataset. We use the known correct answer that a human provided to determine if the network made a correct prediction or not. An incorrected prediction, which we refer to as a prediction error, will be used to teach the network to change the weights of its connections to avoid making prediction errors in the future. Backward Propagation: we use the prediction error that we computed in the last step to properly update the weights of the connections between each neuron to help the network make better future prediction. We use a technique called gradient descent to help us decide whether to increase or decrease each individual connection's weights, Training Rate is used to determine how much to increase or decrease the weights during each training step. We repeat this process for each training sample in the training dataset, and then we repeat the whole many times until the weights of the network become stable. Deep Neural Networks It is a neural network with more than one hidden layer. 1. Weight Initialization Training your neural network requires specifying an initial value of the weights. A well chosen initialization can: Speed up the convergence of gradient descent Increase the odds of gradient descent converging to a lower training (and generalization) error Things to remember: Different initializations lead to different results Random initialization is used to break symmetry and make sure different hidden units can learn different things Don't intialize to values that are too large He initialization works well for networks with ReLU activations. 2. Regularization Deep Learning models have so much flexibility and capacity that overfitting can be a serious problem, if the training dataset is not big enough. Sure it does well on the training set, but the learned network doesn't generalize to new examples that it has never seen! What is L2-regularization actually doing L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. What you should remember -- the implications of L2-regularization on: The cost computation: A regularization term is added to the cost The backpropagation function: There are extra terms in the gradients with respect to weight matrices Weights end up smaller (\"weight decay\"): Weights are pushed to smaller values. 3. Dropout Finally, dropout is a widely used regularization technique that is specific to deep learning. It randomly shuts down some neurons in each iteration. Watch these two videos to see what this means! What you should remember about dropout: Dropout is a regularization technique. You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time. Apply dropout both during forward and backward propagation. During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5. Things to remember: Regularization will help you reduce overfitting. Regularization will drive your weights to lower values. L2 regularization and Dropout are two very effective regularization techniques. 4. Gradient Checking Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation). Gradient checking is slow, so we don't run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process. TensorFlow Machine learning frameworks like TensorFlow, PaddlePaddle, Torch, Caffe, Keras, and many others can speed up your machine learning development significantly. All of these frameworks also have a lot of documentation, which you should feel free to read. Programing frameworks can not only shorten your coding time, but sometimes also perform optimizations that speed up your code. Things to remember: Tensorflow is a programming framework used in deep learning The two main object classes in tensorflow are Tensors and Operators. When you code in tensorflow you have to take the following steps: Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...) Create a session Initialize the session Run the session to execute the graph You can execute the graph multiple times as you've seen in model() The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object. Techniques Which allow deep learning to solve a variety of problems: Fully Connected Networks Convolutional Networks Recurrent Networks Generative Adversarial Networks Reinforcement Learning Fully Connected Neural Networks By fully connected, we mean that each neuron in the preceding layer is connected to every neuron in the subsequent layer. By feedforward, we mean that neurons in any preceding layer are only ever connected to the neurons in a subsequent layer. That is, there are no cycles or loops in the connections of the graph of neurons. As we mentioned in the previous module, each neuron in a neural network contains an activation function that changes the output of a neuron given its input. We have several types of activation functions that can change this input to output relationship to make a neuron behave in a variety of ways. Some of the most well-known activation functions: linear function , straight line that essentially multiplies the input by a constant value sigmoid function , s-shaped curve ranging from 0 to 1 hyperbolic tangent or tanH function , s-shaped curve ranging from -1 to +1 rectified linear unit or ReLU function , a piecewise function that outputs a 0 if the input is less than a certain value, or linear multiple if the input is greater than a certain value. The last three activation functions we refer to as non-linear functions because the output is not a linear multiple of the input. Non-linearity is what allows deep neural networks to model complex functions. We can create networks with various inputs, various outputs, various hidden layers, various neurons per hidden layer, and a variety of activation functions. These numerous combinations allow us to create a variety of powerful deep neural networks that can solve a wide array of problems. The more neurons we add to each hidden layer, the wider the network becomes. In addition, the more hidden layers we add, the deeper the network becomes. However, each neuron we add increases the complexity and thus the processing power necessary to train a neural network. This increase in complexity isn't linear in the number of neurons we add, which leads to an explosion in complexity and training time for large neural networks. As a result, there are certain non-fully connected neural network architectures called sparse neural networks that allow us to create deep neural networks without paying the high cost of a large fully connected network. Convolutional Neural Networks(CNN) CNN is a type of deep neural network architecture designed for specific tasks like image classification. CNNs were inspired by the organization of neurons in the visual cortex of the animal brain. As a result, they provide some very interesting features that are useful for processing certain types of data; like images, audio, and video. A CNN is composed of an input layer . However, for basic image processing, this input is typically a two-dimensional array of neurons which correspond to the pixels of an image. So, we'll represent this layer visually as a square instead of a set of circles. A CNN also contains an output layer which is typically a one-dimensional set of output neurons; one neuron for each category of image being classified. So, we'll represent this as a thick, solid line. A CNN also contains one or more hidden layers ; however, unlike a fully connected neural network, CNNs use a combination of sparsely connected convolution layers , which perform image processing on their inputs. In addition, they contain down sampling layers called pooling layers to further reduce the number of neurons necessary in subsequent layers of the network. CNNs typically contain one or more fully connected layers to connect our pooling layer to our output layer. Convolution is a technique that allows us to extract visual features from an image in small chunks. Each neuron in a convolution layer is responsible for a small cluster of neurons in the preceding layer. The bounding box that determines the cluster of neurons is called a filter, also known as a kernel . Conceptually, you can think of a filter moving across the image and performing a mathematical operation on individual regions of the image. It then sends the result to the corresponding neuron in the convolution layer. Filters mathematically modify the input of a convolution to help it detect certain types of features in the image. They can return the unmodified image, blur the image, sharpen the image, detect edges, and more. This is done by multiplying the original image values by a convolution matrix like the four matrices shown. Filters help a CNN detect certain features in an image by performing these transformations. Pooling , also known as subsampling or down sampling, is the next step in a convolutional neural network. Pooling reduces the number of neurons in the previous convolution layer while still retaining the most important information. There are different types of pooling that can be performed. For example, taking the average of each input neuron, the sum of each neuron, or the maximum value. For example , we're performing what is called a 2x2 max pool with a stride of two. When we put all these techniques together, we get an architecture for a deep neural network quite different from our fully connected neural network. First, we take an input image, which is a two-dimensional matrix, typically with three color channels. Next, we use a convolution layer with multiple filters to create a two-dimensional feature matrix as output for each filter. Then, we pool the results to produce a down sample feature matrix for each filter in the convolution layer. Next, we typically repeat the convolution and pooling steps multiple times using previous features as input. Then, we add a few fully connected hidden layers to help classify the image, and finally, we produce our classification prediction in the output layer. We can also reverse this architecture to create what is known as a deconvolutional neural network . These networks perform the inverse of a convolutional network. Rather than taking an image and converting it into a prediction value, these networks take an input value and attempt to produce an image instead. CNN work well for a variety of tasks including image recognition, image processing, image segmentation, video analysis, and natural language processing Recurrent Neural Networks All of the neural networks we've seen so far have been feedforward neural networks. They are called this because data flows only from the input x through one or more hidden neurons, h, to the output y. However, we also have several types of neural network architectures that contain feedback loops. Unlike feedforward neural networks, the recurrent neural network, or RNN, can operate effectively on sequences of data with variable input length. In order to visualize how an RNN works, let's rotate the path through the RNN from its previous left to right orientation to a top to bottom orientation instead. We're just going to focus on a single path through the network from the input x, through the hidden neuron h, to the output neuron y. Now let's unfold this path through the neural network over time. If we imagine this RNN moving through time this first path represents a network in time step one. The hidden node h1 uses the input x1 to reduce output y1. This is exactly what we've already seen with basic feedforward neural networks. Now let's add a second time step. The hidden node at the current time step, h2, uses both the new input x2, and its state from the previous time step, h1, as input to make its new prediction, y2. This means that a recurrent neural network uses knowledge of its previous state as an input for its current prediction, and we can repeat this process for an arbitrary number of steps allowing the network to propagate information via its hidden state through time. This is essentially like giving a neural network a short-term memory . This feature makes RNNs very effective for working with sequences of data that occur over time. For example, time-series data, like changes in stock prices, a sequence of characters, like a stream of characters being typed into a mobile phone, and a sequence of words, like the stream of words contained in the news article. For example , imagine we're creating a recurrent neural network to predict the next letter a person is likely to type based on the previous letters they've already typed. The letter that a user just typed is quite important to predicting the next letter. However, all of the previous letters are also very important to this prediction as well. At the first time step, the user types the letter h, so our network might predict that the next letter is an i based on all of the previous training examples that included the word hi. At the next time step, the user types the letter e, so our network uses both the new letter e plus the state of the first hidden neuron in order to compute our next prediction r. The network predicts this because of the high frequency of occurrences of the word her in our training dataset. Adding the letter l might predict the word help, and adding another l would predict the letter o, which would match the word our user intended to type, which is hello. RNNs work well for applications that involve a sequence of data that changes over time. These applications include natural language processing, speech recognition, language translation, conversation modeling, image captioning, and visual Q&A. Generative Adversarial Networks The GAN is a combination of two deep learning neural networks: a Generator Network, and a Discriminator Network. The Generator Network produces synthetic data, and the Discriminator Network tries to detect if the data that it's seeing is real or synthetic. These two networks are adversaries in the sense that they're both competing to beat one another. The Generator is trying to produce synthetic data indistinguishable from real data, and the Discriminator is trying to become progressively better at detecting fake data. For example , imagine we want to create a neural network that generates synthetic images. First, we'd acquire a library of real-world images that we can use to provide real images for the image detector network. Next, we'd create an Image Generator network to produce synthetic images. This would typically be a deconvolutional neural network, which we discussed earlier in this module. Then we'd create an Image Detector network to detect real images versus fake images. This would typically be a convolutional neural network which we also discussed earlier in this module. At first, the generator would essentially create random noise as it learns how to create images that can fool the detector. In addition, the detector would only have roughly 50/50 accuracy when predicting real versus fake images. However, with each training iteration, the generator gets progressively better at generating real images, and the detector gets progressively better at detecting fake images. If you let these networks compete with one another for long enough, the generator begins producing fake images that approximate real images. Generative Adversarial Networks have gained quite a bit of popularity in recent years. Some of their applications include: image generation, image enhancement, text generation, speech synthesis new drug discovery, and more. Reinforcement Learning Now let's discuss reinforcement learning: our final technique for creating deep neural networks that can solve a variety of problems. Reinforcement learning involves an agent interacting with an environment. The agent is trying to achieve a goal of some kind within the environment. The environment has state, which the agent can observe. The agent has actions that it can take, which modify the state of the environment, and the agent receives reward signals when it achieves a goal of some kind. The objective of the agent is to learn how to interact with its environment in such a way that allows it to achieve its goals. For example , an agent might be a car trying to get its passengers to their destination. The environment would be the world the car is driving in. This would include the road, other cars, pedestrians, and any obstacles on the road. The car can observe the state of its environment; for example, it's position, speed, and direction, the orientation of the road it's driving on, and the location of any obstacles in its path. The car has actions that it can perform to modify the state of the world. For example, the car can accelerate, decelerate, turn left, or turn right. This allows it to change its position relative to the objects in the world. The car would receive reward signals when it achieves a goal of some kind; for example, we could reward the car when it arrives safely at its destination, for each mile it stays on the road, and for each minute it drives at a safe speed. We could also penalize the car when it drives off the road, travels at unsafe speeds, or climbs with an obstacle. The objective for the car is to learn how to drive in this world in such a way that it arrives at its destination. Deep reinforcement learning is the application of reinforcement learning to train deep neural networks. Like our previous deep neural networks, we have an input layer, an output layer, and multiple hidden layers. However, our input is the state of the environment; for example, position, speed, and direction; our output is a series of a possible actions; for example, speed up, slow down, turn left, or turn right. In addition, we're feeding our rewards signal into the network so that we can learn to associate what actions produce positive results given a specific state of the environment. This deep neural network attempts to predict the expected future reward for each action, given the current state of the environment. It then chooses whichever action's predicted to have the highest potential future reward, and performs that action. Some examples of Deep Reinforcement Learning Applications are games, including board games like chess and Go, card games like poker, and 8-bit video games. Autonomous vehicles, like self-driving cars, and autonomous drones. Robotics, including teaching robots how to walk, and teaching robots how to perform manual tasks. Management tasks, including inventory management, resource allocation, and logistics; and financial tasks, including investment decisions, portfolio design, and asset pricing.","title":"Intro"},{"location":"deep_learning/#deep-learning","text":"Deep Learning is a form of artificial intelligence that uses a type of machine learning called an artificial neural network with multiple hidden layers that learns hierarchical representations of the underlying data in order to make predictions given new data. Deep Learning attempts to model high-level abstractions about data using networks of graphs. It is focused on learning representations in data. Additionally, modeling high-level abstractions about data is very similar to artificial intelligence \u2014 the idea that knowledge can be represented and acted upon intelligently.","title":"Deep Learning"},{"location":"deep_learning/#artificial-intelligence","text":"AI is a field of computer science that attempts to create machines that act rationally in response to their environment. Explicit Programming Encoding Domain Knowledge Statistical Patterns Detection","title":"Artificial Intelligence"},{"location":"deep_learning/#neural-network","text":"It is a machine learning algorihtm based on a very crude approximation of biological neural network in a brain. If we connect a series of artificial neurons in a network, we get an artificial neural network or neural network. Forward Propagation: we use the network with its current parameters to compute a prediction for each example in our training dataset. We use the known correct answer that a human provided to determine if the network made a correct prediction or not. An incorrected prediction, which we refer to as a prediction error, will be used to teach the network to change the weights of its connections to avoid making prediction errors in the future. Backward Propagation: we use the prediction error that we computed in the last step to properly update the weights of the connections between each neuron to help the network make better future prediction. We use a technique called gradient descent to help us decide whether to increase or decrease each individual connection's weights, Training Rate is used to determine how much to increase or decrease the weights during each training step. We repeat this process for each training sample in the training dataset, and then we repeat the whole many times until the weights of the network become stable.","title":"Neural Network"},{"location":"deep_learning/#deep-neural-networks","text":"It is a neural network with more than one hidden layer.","title":"Deep Neural Networks"},{"location":"deep_learning/#1-weight-initialization","text":"Training your neural network requires specifying an initial value of the weights. A well chosen initialization can: Speed up the convergence of gradient descent Increase the odds of gradient descent converging to a lower training (and generalization) error Things to remember: Different initializations lead to different results Random initialization is used to break symmetry and make sure different hidden units can learn different things Don't intialize to values that are too large He initialization works well for networks with ReLU activations.","title":"1. Weight Initialization"},{"location":"deep_learning/#2-regularization","text":"Deep Learning models have so much flexibility and capacity that overfitting can be a serious problem, if the training dataset is not big enough. Sure it does well on the training set, but the learned network doesn't generalize to new examples that it has never seen!","title":"2. Regularization"},{"location":"deep_learning/#what-is-l2-regularization-actually-doing","text":"L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. What you should remember -- the implications of L2-regularization on: The cost computation: A regularization term is added to the cost The backpropagation function: There are extra terms in the gradients with respect to weight matrices Weights end up smaller (\"weight decay\"): Weights are pushed to smaller values.","title":"What is L2-regularization actually doing"},{"location":"deep_learning/#3-dropout","text":"Finally, dropout is a widely used regularization technique that is specific to deep learning. It randomly shuts down some neurons in each iteration. Watch these two videos to see what this means! What you should remember about dropout: Dropout is a regularization technique. You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time. Apply dropout both during forward and backward propagation. During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5. Things to remember: Regularization will help you reduce overfitting. Regularization will drive your weights to lower values. L2 regularization and Dropout are two very effective regularization techniques.","title":"3. Dropout"},{"location":"deep_learning/#4-gradient-checking","text":"Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation). Gradient checking is slow, so we don't run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.","title":"4. Gradient Checking"},{"location":"deep_learning/#tensorflow","text":"Machine learning frameworks like TensorFlow, PaddlePaddle, Torch, Caffe, Keras, and many others can speed up your machine learning development significantly. All of these frameworks also have a lot of documentation, which you should feel free to read. Programing frameworks can not only shorten your coding time, but sometimes also perform optimizations that speed up your code. Things to remember: Tensorflow is a programming framework used in deep learning The two main object classes in tensorflow are Tensors and Operators. When you code in tensorflow you have to take the following steps: Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...) Create a session Initialize the session Run the session to execute the graph You can execute the graph multiple times as you've seen in model() The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object.","title":"TensorFlow"},{"location":"deep_learning/#techniques","text":"Which allow deep learning to solve a variety of problems: Fully Connected Networks Convolutional Networks Recurrent Networks Generative Adversarial Networks Reinforcement Learning","title":"Techniques"},{"location":"deep_learning/#fully-connected-neural-networks","text":"By fully connected, we mean that each neuron in the preceding layer is connected to every neuron in the subsequent layer. By feedforward, we mean that neurons in any preceding layer are only ever connected to the neurons in a subsequent layer. That is, there are no cycles or loops in the connections of the graph of neurons. As we mentioned in the previous module, each neuron in a neural network contains an activation function that changes the output of a neuron given its input. We have several types of activation functions that can change this input to output relationship to make a neuron behave in a variety of ways. Some of the most well-known activation functions: linear function , straight line that essentially multiplies the input by a constant value sigmoid function , s-shaped curve ranging from 0 to 1 hyperbolic tangent or tanH function , s-shaped curve ranging from -1 to +1 rectified linear unit or ReLU function , a piecewise function that outputs a 0 if the input is less than a certain value, or linear multiple if the input is greater than a certain value. The last three activation functions we refer to as non-linear functions because the output is not a linear multiple of the input. Non-linearity is what allows deep neural networks to model complex functions. We can create networks with various inputs, various outputs, various hidden layers, various neurons per hidden layer, and a variety of activation functions. These numerous combinations allow us to create a variety of powerful deep neural networks that can solve a wide array of problems. The more neurons we add to each hidden layer, the wider the network becomes. In addition, the more hidden layers we add, the deeper the network becomes. However, each neuron we add increases the complexity and thus the processing power necessary to train a neural network. This increase in complexity isn't linear in the number of neurons we add, which leads to an explosion in complexity and training time for large neural networks. As a result, there are certain non-fully connected neural network architectures called sparse neural networks that allow us to create deep neural networks without paying the high cost of a large fully connected network.","title":"Fully Connected Neural Networks"},{"location":"deep_learning/#convolutional-neural-networkscnn","text":"CNN is a type of deep neural network architecture designed for specific tasks like image classification. CNNs were inspired by the organization of neurons in the visual cortex of the animal brain. As a result, they provide some very interesting features that are useful for processing certain types of data; like images, audio, and video. A CNN is composed of an input layer . However, for basic image processing, this input is typically a two-dimensional array of neurons which correspond to the pixels of an image. So, we'll represent this layer visually as a square instead of a set of circles. A CNN also contains an output layer which is typically a one-dimensional set of output neurons; one neuron for each category of image being classified. So, we'll represent this as a thick, solid line. A CNN also contains one or more hidden layers ; however, unlike a fully connected neural network, CNNs use a combination of sparsely connected convolution layers , which perform image processing on their inputs. In addition, they contain down sampling layers called pooling layers to further reduce the number of neurons necessary in subsequent layers of the network. CNNs typically contain one or more fully connected layers to connect our pooling layer to our output layer. Convolution is a technique that allows us to extract visual features from an image in small chunks. Each neuron in a convolution layer is responsible for a small cluster of neurons in the preceding layer. The bounding box that determines the cluster of neurons is called a filter, also known as a kernel . Conceptually, you can think of a filter moving across the image and performing a mathematical operation on individual regions of the image. It then sends the result to the corresponding neuron in the convolution layer. Filters mathematically modify the input of a convolution to help it detect certain types of features in the image. They can return the unmodified image, blur the image, sharpen the image, detect edges, and more. This is done by multiplying the original image values by a convolution matrix like the four matrices shown. Filters help a CNN detect certain features in an image by performing these transformations. Pooling , also known as subsampling or down sampling, is the next step in a convolutional neural network. Pooling reduces the number of neurons in the previous convolution layer while still retaining the most important information. There are different types of pooling that can be performed. For example, taking the average of each input neuron, the sum of each neuron, or the maximum value. For example , we're performing what is called a 2x2 max pool with a stride of two. When we put all these techniques together, we get an architecture for a deep neural network quite different from our fully connected neural network. First, we take an input image, which is a two-dimensional matrix, typically with three color channels. Next, we use a convolution layer with multiple filters to create a two-dimensional feature matrix as output for each filter. Then, we pool the results to produce a down sample feature matrix for each filter in the convolution layer. Next, we typically repeat the convolution and pooling steps multiple times using previous features as input. Then, we add a few fully connected hidden layers to help classify the image, and finally, we produce our classification prediction in the output layer. We can also reverse this architecture to create what is known as a deconvolutional neural network . These networks perform the inverse of a convolutional network. Rather than taking an image and converting it into a prediction value, these networks take an input value and attempt to produce an image instead. CNN work well for a variety of tasks including image recognition, image processing, image segmentation, video analysis, and natural language processing","title":"Convolutional Neural Networks(CNN)"},{"location":"deep_learning/#recurrent-neural-networks","text":"All of the neural networks we've seen so far have been feedforward neural networks. They are called this because data flows only from the input x through one or more hidden neurons, h, to the output y. However, we also have several types of neural network architectures that contain feedback loops. Unlike feedforward neural networks, the recurrent neural network, or RNN, can operate effectively on sequences of data with variable input length. In order to visualize how an RNN works, let's rotate the path through the RNN from its previous left to right orientation to a top to bottom orientation instead. We're just going to focus on a single path through the network from the input x, through the hidden neuron h, to the output neuron y. Now let's unfold this path through the neural network over time. If we imagine this RNN moving through time this first path represents a network in time step one. The hidden node h1 uses the input x1 to reduce output y1. This is exactly what we've already seen with basic feedforward neural networks. Now let's add a second time step. The hidden node at the current time step, h2, uses both the new input x2, and its state from the previous time step, h1, as input to make its new prediction, y2. This means that a recurrent neural network uses knowledge of its previous state as an input for its current prediction, and we can repeat this process for an arbitrary number of steps allowing the network to propagate information via its hidden state through time. This is essentially like giving a neural network a short-term memory . This feature makes RNNs very effective for working with sequences of data that occur over time. For example, time-series data, like changes in stock prices, a sequence of characters, like a stream of characters being typed into a mobile phone, and a sequence of words, like the stream of words contained in the news article. For example , imagine we're creating a recurrent neural network to predict the next letter a person is likely to type based on the previous letters they've already typed. The letter that a user just typed is quite important to predicting the next letter. However, all of the previous letters are also very important to this prediction as well. At the first time step, the user types the letter h, so our network might predict that the next letter is an i based on all of the previous training examples that included the word hi. At the next time step, the user types the letter e, so our network uses both the new letter e plus the state of the first hidden neuron in order to compute our next prediction r. The network predicts this because of the high frequency of occurrences of the word her in our training dataset. Adding the letter l might predict the word help, and adding another l would predict the letter o, which would match the word our user intended to type, which is hello. RNNs work well for applications that involve a sequence of data that changes over time. These applications include natural language processing, speech recognition, language translation, conversation modeling, image captioning, and visual Q&A.","title":"Recurrent Neural Networks"},{"location":"deep_learning/#generative-adversarial-networks","text":"The GAN is a combination of two deep learning neural networks: a Generator Network, and a Discriminator Network. The Generator Network produces synthetic data, and the Discriminator Network tries to detect if the data that it's seeing is real or synthetic. These two networks are adversaries in the sense that they're both competing to beat one another. The Generator is trying to produce synthetic data indistinguishable from real data, and the Discriminator is trying to become progressively better at detecting fake data. For example , imagine we want to create a neural network that generates synthetic images. First, we'd acquire a library of real-world images that we can use to provide real images for the image detector network. Next, we'd create an Image Generator network to produce synthetic images. This would typically be a deconvolutional neural network, which we discussed earlier in this module. Then we'd create an Image Detector network to detect real images versus fake images. This would typically be a convolutional neural network which we also discussed earlier in this module. At first, the generator would essentially create random noise as it learns how to create images that can fool the detector. In addition, the detector would only have roughly 50/50 accuracy when predicting real versus fake images. However, with each training iteration, the generator gets progressively better at generating real images, and the detector gets progressively better at detecting fake images. If you let these networks compete with one another for long enough, the generator begins producing fake images that approximate real images. Generative Adversarial Networks have gained quite a bit of popularity in recent years. Some of their applications include: image generation, image enhancement, text generation, speech synthesis new drug discovery, and more.","title":"Generative Adversarial Networks"},{"location":"deep_learning/#reinforcement-learning","text":"Now let's discuss reinforcement learning: our final technique for creating deep neural networks that can solve a variety of problems. Reinforcement learning involves an agent interacting with an environment. The agent is trying to achieve a goal of some kind within the environment. The environment has state, which the agent can observe. The agent has actions that it can take, which modify the state of the environment, and the agent receives reward signals when it achieves a goal of some kind. The objective of the agent is to learn how to interact with its environment in such a way that allows it to achieve its goals. For example , an agent might be a car trying to get its passengers to their destination. The environment would be the world the car is driving in. This would include the road, other cars, pedestrians, and any obstacles on the road. The car can observe the state of its environment; for example, it's position, speed, and direction, the orientation of the road it's driving on, and the location of any obstacles in its path. The car has actions that it can perform to modify the state of the world. For example, the car can accelerate, decelerate, turn left, or turn right. This allows it to change its position relative to the objects in the world. The car would receive reward signals when it achieves a goal of some kind; for example, we could reward the car when it arrives safely at its destination, for each mile it stays on the road, and for each minute it drives at a safe speed. We could also penalize the car when it drives off the road, travels at unsafe speeds, or climbs with an obstacle. The objective for the car is to learn how to drive in this world in such a way that it arrives at its destination. Deep reinforcement learning is the application of reinforcement learning to train deep neural networks. Like our previous deep neural networks, we have an input layer, an output layer, and multiple hidden layers. However, our input is the state of the environment; for example, position, speed, and direction; our output is a series of a possible actions; for example, speed up, slow down, turn left, or turn right. In addition, we're feeding our rewards signal into the network so that we can learn to associate what actions produce positive results given a specific state of the environment. This deep neural network attempts to predict the expected future reward for each action, given the current state of the environment. It then chooses whichever action's predicted to have the highest potential future reward, and performs that action. Some examples of Deep Reinforcement Learning Applications are games, including board games like chess and Go, card games like poker, and 8-bit video games. Autonomous vehicles, like self-driving cars, and autonomous drones. Robotics, including teaching robots how to walk, and teaching robots how to perform manual tasks. Management tasks, including inventory management, resource allocation, and logistics; and financial tasks, including investment decisions, portfolio design, and asset pricing.","title":"Reinforcement Learning"},{"location":"dynamic_programming/","text":"The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. An Iterative Method Notes on the Bellman Expectation Equation for state s_1 , we saw that: v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3)) We mentioned that this equation follows directly from the Bellman expectation equation for v_\\pi v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s')) (The Bellman expectation equation for v_\\pi ) In order to see this, we can begin by looking at what the Bellman expectation equation tells us about the value of state s_1 (where we just need to plug in s_1 for state s ). v_\\pi(s_1) = \\sum_{a \\in \\mathcal{A}(s_1)}\\pi(a|s_1)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s_1,a)(r + \\gamma v_\\pi(s')) Then, it's possible to derive the equation for state s_1 by using the following: \\mathcal{A}(s_1=\\{ \\text{down}, \\text{right} \\} (When in state s_1, the agent only has two potential actions: down or right.) \\pi({down}|s_1) = \\pi(\\text{right}|s_1) = \\frac{1}{2} (We are currently examining the policy where the agent goes down with 50% probability and right with 50% probability when in state s_1 .) p(s_3,-3|s_1,\\text{down}) = 1 (\\text{and } p(s',r|s_1,\\text{down}) = 0 \\text{ if } s'\\neq s_3 \\text{ or } r\\neq -3) (If the agent chooses to go down in state s_1 then with 100% probability, the next state is s_3 , and the agent receives a reward of -3.) p(s_2,-1|s_1,\\text{right}) = 1 (\\text{and } p(s',r|s_1,\\text{right}) = 0 \\text{ if } s'\\neq s_2 \\text{ or } r\\neq -1) (If the agent chooses to go right in state s_1 , then with 100% probability, the next state is s_2 , and the agent receives a reward of -1.) \\gamma = 1 (We chose to set the discount rate to 1 in this gridworld example.) Notes on Solving the System of Equations This example serves to illustrate the fact that it is possible to directly solve the system of equations given by the Bellman expectation equation for v_\\pi However, in practice, and especially for much larger Markov decision processes (MDPs), we will instead use an iterative solution approach. You can directly solve the system of equations: v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3)) v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4)) v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4)) v_\\pi(s_4) = 0 Since the equations for v_\\pi(s_2) and v_\\pi(s_3) are identical, we must have that v_\\pi(s_2) = v_\\pi(s_3) Thus, the equations for v_\\pi(s_1) and v_\\pi(s_2) can be changed to: v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_2)) = -2 + v_\\pi(s_2) v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + 0) = 2 + \\frac{1}{2}v_\\pi(s_1) Combining the two latest equations yields v_\\pi(s_1) = -2 + 2 + \\frac{1}{2}v_\\pi(s_1) = \\frac{1}{2}v_\\pi(s_1) which implies v_\\pi(s_1)=0 Furthermore, v_\\pi(s_3) = v_\\pi(s_2) = 2 + \\frac{1}{2}v_\\pi(s_1) = 2 + 0 = 2 Thus, the state-value function is given by: v_\\pi(s_1) = 0 v_\\pi(s_2) = 2 v_\\pi(s_3) = 2 v_\\pi(s_4) = 0 we have discussed how an agent might obtain the state-value function v_\\pi corresponding to a policy \\pi . In the dynamic programming setting, the agent has full knowledge of the Markov decision process (MDP). In this case, it's possible to use the one-step dynamics p(s',r|s,a) of the MDP to obtain a system of equations corresponding to the Bellman expectation equation for v_\\pi . In order to obtain the state-value function, we need only solve the system of equations. While it is always possible to directly solve the system, we will instead use an iterative solution approach. The iterative method begins with an initial guess for the value of each state. In particular, we began by assuming that the value of each state was zero. Then, we looped over the state space and amended the estimate for the state-value function through applying successive update equations. Recall that V denotes the most recent guess for the state-value function, and the update equations are: V(s_1) \\leftarrow \\frac{1}{2}(-1 + V(s_2)) + \\frac{1}{2}(-3 + V(s_3)) V(s_2) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5) V(s_3) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5) The state-value function for the equiprobable random policy is visualized below: Implementation: Iterative Policy Evaluation Note that policy evaluation is guaranteed to converge to the state-value function v_\\pi corresponding to a policy \\pi , as long as v_\\pi(s) is finite for each state s\\in\\mathcal{S} . For a finite Markov decision process (MDP), this is guaranteed as long as either: \\gamma < 1 , or if the agent starts in any state s\\in\\mathcal{S} , it is guaranteed to eventually reach a terminal state if it follows policy \\pi . Action Values Use the simple gridworld to practice converting a state-value function v_\\pi to an action-value function q_\\pi Image below corresponds to the action-value function for the same policy. As an example, consider q_\\pi(s_1, \\text{right}) . This action value can be calculated as q_\\pi(s_1, \\text{right}) = -1 + v_\\pi(s_2) = -1 + 2 = 1 where we just use the fact that we can express the value of the state-action pair s_1, \\text{right} as the sum of two quantities: the immediate reward after moving right and landing on state s_2 the cumulative reward obtained if the agent begins in state s_2 and follows the policy. For More Complex Environments In this simple gridworld example, the environment is deterministic . In other words, after the agent selects an action, the next state and reward are 100% guaranteed and non-random. For deterministic environments, p(s',r|s,a) \\in {0,1} \\text{ for all } s', r, s, a In this case, when the agent is in state s and takes action a , the next state s' and reward r can be predicted with certainty, and we must have q_\\pi(s,a) = r + \\gamma v_\\pi(s') In general, the environment need not be deterministic, and instead may be stochastic . This is the default behavior of the FrozenLake environment; in this case, once the agent selects an action, the next state and reward cannot be predicted with certainty and instead are random draws from a (conditional) probability distribution p(s',r|s,a) In this case, when the agent is in state s and takes action a , the probability of each possible next state s' and reward r is given by p(s',r|s,a) . In this case, we must have q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s')) , where we take the expected value of the sum r + \\gamma v_\\pi(s') Implementation: Estimation of Action Values Write an algorithm that accepts an estimate V of the state-value function v_\\pi along with the one-step dynamics of the MDP p(s',r|s,a) and returns an estimate Q the action-value function q_\\pi Use the one-step dynamics p(s',r|s,a) of MDP to obtain q_\\pi from v_\\pi . Namely, q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s')) holds for all s\\in\\mathcal{S} \\text{ and } a\\in\\mathcal{A}(s) . Policy Improvement Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function v_\\pi for an arbitrary deterministic policy \\pi . For some state s we would like to know whether or not we should change the policy to deterministically choose an action a \\neq \\pi(s) . Implementation It is possible to construct an improved (or equivalent) policy \\pi' , where \\pi'\\geq\\pi For each state s\\in\\mathcal{S} , you need to select the action that maximizes the action-value function estimate. In other words, \\pi'(s)=\\arg\\max_{a\\inA(s)}Q(s,a) for all s\\in\\mathcal{S} . In the event that there is some state s\\in\\mathcal{S} for which \\arg\\max_{a\\in\\mathcal{A}(s)} is not unique, there is some flexibility in how the improved policy \\pi' is constructed. In fact, as long as the policy \\pi' satisfies for each s\\in\\mathcal{S} and a\\in\\mathcal{A}(s) : \\pi'(a|s)=0 \\text{ if a }\\notin \\arg\\max_{a'\\in\\mathcal{A}(s)}Q(s,a') It is an improved policy. In other words, any policy that (for each state) assigns zero probability to the actions that do not maximize the action-value function estimate (for that state) is an improved policy. Policy Iteration This way of finding an optimal policy is called policy iteration . Policy iteration is guaranteed to find the optimal policy for any finite Markov decision process (MDP) in a finite number of iterations. Value Iteration One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to v_\\pi occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called value iteration . In this algorithm, each sweep over the state space effectively performs both policy evaluation and policy improvement. Value iteration is guaranteed to find the optimal policy \\pi_* for any finite MDP. Note that the stopping criterion is satisfied when the difference between successive value function estimates is sufficiently small. In particular, the loop terminates if the difference is less than \\theta for each state. And, the closer we want the final value function estimate to be to the optimal value function, the smaller we need to set the value of \\theta . note that in the case of the FrozenLake environment, values around 1e-8 seem to work reasonably well.","title":"Dynamic Programming"},{"location":"dynamic_programming/#an-iterative-method","text":"Notes on the Bellman Expectation Equation for state s_1 , we saw that: v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3)) We mentioned that this equation follows directly from the Bellman expectation equation for v_\\pi v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s')) (The Bellman expectation equation for v_\\pi ) In order to see this, we can begin by looking at what the Bellman expectation equation tells us about the value of state s_1 (where we just need to plug in s_1 for state s ). v_\\pi(s_1) = \\sum_{a \\in \\mathcal{A}(s_1)}\\pi(a|s_1)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s_1,a)(r + \\gamma v_\\pi(s')) Then, it's possible to derive the equation for state s_1 by using the following: \\mathcal{A}(s_1=\\{ \\text{down}, \\text{right} \\} (When in state s_1, the agent only has two potential actions: down or right.) \\pi({down}|s_1) = \\pi(\\text{right}|s_1) = \\frac{1}{2} (We are currently examining the policy where the agent goes down with 50% probability and right with 50% probability when in state s_1 .) p(s_3,-3|s_1,\\text{down}) = 1 (\\text{and } p(s',r|s_1,\\text{down}) = 0 \\text{ if } s'\\neq s_3 \\text{ or } r\\neq -3) (If the agent chooses to go down in state s_1 then with 100% probability, the next state is s_3 , and the agent receives a reward of -3.) p(s_2,-1|s_1,\\text{right}) = 1 (\\text{and } p(s',r|s_1,\\text{right}) = 0 \\text{ if } s'\\neq s_2 \\text{ or } r\\neq -1) (If the agent chooses to go right in state s_1 , then with 100% probability, the next state is s_2 , and the agent receives a reward of -1.) \\gamma = 1 (We chose to set the discount rate to 1 in this gridworld example.)","title":"An Iterative Method"},{"location":"dynamic_programming/#notes-on-solving-the-system-of-equations","text":"This example serves to illustrate the fact that it is possible to directly solve the system of equations given by the Bellman expectation equation for v_\\pi However, in practice, and especially for much larger Markov decision processes (MDPs), we will instead use an iterative solution approach. You can directly solve the system of equations: v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3)) v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4)) v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4)) v_\\pi(s_4) = 0 Since the equations for v_\\pi(s_2) and v_\\pi(s_3) are identical, we must have that v_\\pi(s_2) = v_\\pi(s_3) Thus, the equations for v_\\pi(s_1) and v_\\pi(s_2) can be changed to: v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_2)) = -2 + v_\\pi(s_2) v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + 0) = 2 + \\frac{1}{2}v_\\pi(s_1) Combining the two latest equations yields v_\\pi(s_1) = -2 + 2 + \\frac{1}{2}v_\\pi(s_1) = \\frac{1}{2}v_\\pi(s_1) which implies v_\\pi(s_1)=0 Furthermore, v_\\pi(s_3) = v_\\pi(s_2) = 2 + \\frac{1}{2}v_\\pi(s_1) = 2 + 0 = 2 Thus, the state-value function is given by: v_\\pi(s_1) = 0 v_\\pi(s_2) = 2 v_\\pi(s_3) = 2 v_\\pi(s_4) = 0 we have discussed how an agent might obtain the state-value function v_\\pi corresponding to a policy \\pi . In the dynamic programming setting, the agent has full knowledge of the Markov decision process (MDP). In this case, it's possible to use the one-step dynamics p(s',r|s,a) of the MDP to obtain a system of equations corresponding to the Bellman expectation equation for v_\\pi . In order to obtain the state-value function, we need only solve the system of equations. While it is always possible to directly solve the system, we will instead use an iterative solution approach. The iterative method begins with an initial guess for the value of each state. In particular, we began by assuming that the value of each state was zero. Then, we looped over the state space and amended the estimate for the state-value function through applying successive update equations. Recall that V denotes the most recent guess for the state-value function, and the update equations are: V(s_1) \\leftarrow \\frac{1}{2}(-1 + V(s_2)) + \\frac{1}{2}(-3 + V(s_3)) V(s_2) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5) V(s_3) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5) The state-value function for the equiprobable random policy is visualized below:","title":"Notes on Solving the System of Equations"},{"location":"dynamic_programming/#implementation-iterative-policy-evaluation","text":"Note that policy evaluation is guaranteed to converge to the state-value function v_\\pi corresponding to a policy \\pi , as long as v_\\pi(s) is finite for each state s\\in\\mathcal{S} . For a finite Markov decision process (MDP), this is guaranteed as long as either: \\gamma < 1 , or if the agent starts in any state s\\in\\mathcal{S} , it is guaranteed to eventually reach a terminal state if it follows policy \\pi .","title":"Implementation: Iterative Policy Evaluation"},{"location":"dynamic_programming/#action-values","text":"Use the simple gridworld to practice converting a state-value function v_\\pi to an action-value function q_\\pi Image below corresponds to the action-value function for the same policy. As an example, consider q_\\pi(s_1, \\text{right}) . This action value can be calculated as q_\\pi(s_1, \\text{right}) = -1 + v_\\pi(s_2) = -1 + 2 = 1 where we just use the fact that we can express the value of the state-action pair s_1, \\text{right} as the sum of two quantities: the immediate reward after moving right and landing on state s_2 the cumulative reward obtained if the agent begins in state s_2 and follows the policy.","title":"Action Values"},{"location":"dynamic_programming/#for-more-complex-environments","text":"In this simple gridworld example, the environment is deterministic . In other words, after the agent selects an action, the next state and reward are 100% guaranteed and non-random. For deterministic environments, p(s',r|s,a) \\in {0,1} \\text{ for all } s', r, s, a In this case, when the agent is in state s and takes action a , the next state s' and reward r can be predicted with certainty, and we must have q_\\pi(s,a) = r + \\gamma v_\\pi(s') In general, the environment need not be deterministic, and instead may be stochastic . This is the default behavior of the FrozenLake environment; in this case, once the agent selects an action, the next state and reward cannot be predicted with certainty and instead are random draws from a (conditional) probability distribution p(s',r|s,a) In this case, when the agent is in state s and takes action a , the probability of each possible next state s' and reward r is given by p(s',r|s,a) . In this case, we must have q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s')) , where we take the expected value of the sum r + \\gamma v_\\pi(s')","title":"For More Complex Environments"},{"location":"dynamic_programming/#implementation-estimation-of-action-values","text":"Write an algorithm that accepts an estimate V of the state-value function v_\\pi along with the one-step dynamics of the MDP p(s',r|s,a) and returns an estimate Q the action-value function q_\\pi Use the one-step dynamics p(s',r|s,a) of MDP to obtain q_\\pi from v_\\pi . Namely, q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s')) holds for all s\\in\\mathcal{S} \\text{ and } a\\in\\mathcal{A}(s) .","title":"Implementation: Estimation of Action Values"},{"location":"dynamic_programming/#policy-improvement","text":"Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function v_\\pi for an arbitrary deterministic policy \\pi . For some state s we would like to know whether or not we should change the policy to deterministically choose an action a \\neq \\pi(s) .","title":"Policy Improvement"},{"location":"dynamic_programming/#implementation","text":"It is possible to construct an improved (or equivalent) policy \\pi' , where \\pi'\\geq\\pi For each state s\\in\\mathcal{S} , you need to select the action that maximizes the action-value function estimate. In other words, \\pi'(s)=\\arg\\max_{a\\inA(s)}Q(s,a) for all s\\in\\mathcal{S} . In the event that there is some state s\\in\\mathcal{S} for which \\arg\\max_{a\\in\\mathcal{A}(s)} is not unique, there is some flexibility in how the improved policy \\pi' is constructed. In fact, as long as the policy \\pi' satisfies for each s\\in\\mathcal{S} and a\\in\\mathcal{A}(s) : \\pi'(a|s)=0 \\text{ if a }\\notin \\arg\\max_{a'\\in\\mathcal{A}(s)}Q(s,a') It is an improved policy. In other words, any policy that (for each state) assigns zero probability to the actions that do not maximize the action-value function estimate (for that state) is an improved policy.","title":"Implementation"},{"location":"dynamic_programming/#policy-iteration","text":"This way of finding an optimal policy is called policy iteration . Policy iteration is guaranteed to find the optimal policy for any finite Markov decision process (MDP) in a finite number of iterations.","title":"Policy Iteration"},{"location":"dynamic_programming/#value-iteration","text":"One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to v_\\pi occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called value iteration . In this algorithm, each sweep over the state space effectively performs both policy evaluation and policy improvement. Value iteration is guaranteed to find the optimal policy \\pi_* for any finite MDP. Note that the stopping criterion is satisfied when the difference between successive value function estimates is sufficiently small. In particular, the loop terminates if the difference is less than \\theta for each state. And, the closer we want the final value function estimate to be to the optimal value function, the smaller we need to set the value of \\theta . note that in the case of the FrozenLake environment, values around 1e-8 seem to work reasonably well.","title":"Value Iteration"},{"location":"libraries/","text":"Libraries Numpy let's you perform mathematical functions on large multi dimensional arrays and matrices efficiently. Pandas is used for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. PyTorch provides a wide range of algorithms for deep learning and is used for applications such as natural language processing (based on Torch). Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano; runs seamlessly on CPU and GPU. Eli5 allows to visualize and debug various Machine Learning models using unified API. It has built-in support for several ML frameworks and provides a way to explain black-box models. Matplotlib is a plotting library that can produce great visualizations often with very few lines of code. Scikit-learn is designed to work with NumPy, SciPy and Pandas, provides toolset for training and evaluation tasks: Data splitting Pre-processing Feature selection Model training Model tuning and offers common interface across algorithms XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI). LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency. Lower memory usage. Better accuracy. Support of parallel and GPU learning. Capable of handling large-scale data.","title":"Libraries"},{"location":"libraries/#libraries","text":"Numpy let's you perform mathematical functions on large multi dimensional arrays and matrices efficiently. Pandas is used for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. PyTorch provides a wide range of algorithms for deep learning and is used for applications such as natural language processing (based on Torch). Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano; runs seamlessly on CPU and GPU. Eli5 allows to visualize and debug various Machine Learning models using unified API. It has built-in support for several ML frameworks and provides a way to explain black-box models. Matplotlib is a plotting library that can produce great visualizations often with very few lines of code. Scikit-learn is designed to work with NumPy, SciPy and Pandas, provides toolset for training and evaluation tasks: Data splitting Pre-processing Feature selection Model training Model tuning and offers common interface across algorithms XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI). LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency. Lower memory usage. Better accuracy. Support of parallel and GPU learning. Capable of handling large-scale data.","title":"Libraries"},{"location":"logistic_regression/","text":"","title":"Logistic Regression"},{"location":"math/","text":"Mean deviation refers to how far away every data point is from the average of those data points. Squared mean deviation simply squares this result. This is the square of the distance of every data point from the mean, and now we can finally calculate the variance of these data points. variance=\\frac{\\sum(x_i-\\overline{x})^2}{n} Variance describe the concentration of data points around the mean, It is a measure of spread/variability of data. It is calculated as sum of the squares of the distances of every individual data point from the mean divided by the total number of data points. The estimate of the variance can be improved by tweaking the denominator of this function. This tweak is called the Bessel's Correction . So instead of using N for all of the data points, we'll simply use N-1 as the denominator. variance=\\frac{\\sum(x_i-\\overline{x})^2}{n-1} Mean and variance succinctly summarize a set of numbers along with Standard deviation , which once again measures how much the numbers jump around. SD = \\sqrt{variance} Mean Square Error (MSE) MSE is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples: MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2 Standard Error(SE) of a statistic is the standard deviation of its sampling distribution or an estimate of that standard deviation. If the parameter or the statistic is the mean, it is called the standard error of the mean (SEM). $\\sigma$=Standard Deviation of population, n=size of sample SE = \\frac{\\sigma}{\\sqrt{n}} Z score (Standard Score) Given an observed value x, the Z score finds the number of Standard deviations x is away from the mean. Z = \\frac{x-\\mu}{\\sigma} T distribution The t-Test is best to use when we do not know the population standard deviation. Instead we use the sample standard deviation. The number of degrees of freedom(df) is the number of values that are free to vary (n-1). S=\\frac{\\sum(x_i-\\overline{x})^2}{n-1} t = \\frac{\\overline{x}-\\mu}{SE} T-tests are also great for testing two sample means (i.e. paired t-tests), we modify the formula to become: SD = \\sqrt{S_1^2 + S_2^2} SE = \\sqrt{\\frac{S_1^2}{n1} + \\frac{S_2^2}{n2}} t = \\frac{(\\overline{x}_2-\\overline{x}_1)-(\\mu_2-\\mu_1)}{\\frac{\\sqrt(s_1^2+s_2^2)}{n}} Cohen's d It measures the effect size of the strength of a phenomenon. Cohen\u2019s d gives us the distance between means in standardized units. $\\overline{x} = \\overline{x_1} - \\overline{x_2}$ d = \\frac{\\overline{x}-\\mu}{S} Margin of Error (ME) ME = t_{critical} * SE Confidence Interval (CI) CI = \\overline{x} \\pm ME Evaluation Metrics Positive Class In binary classification, the two possible classes are labeled as positive and negative. The positive outcome is the thing we're testing for. (Admittedly, we're simultaneously testing for both outcomes, but play along.) For example, the positive class in a medical test might be \"tumor.\" The positive class in an email classifier might be \"spam\". Negative Class In binary classification, one class is termed positive and the other is termed negative. The positive class is the thing we're looking for and the negative class is the other possibility. For example, the negative class in a medical test might be \"not tumor.\" The negative class in an email classifier might be \"not spam\". True Positive (TP) An example in which the model correctly predicted the positive class. For example, the model inferred that a particular email message was spam, and that email message really was spam. True Negative (TN) An example in which the model correctly predicted the negative class. For example, the model inferred that a particular email message was not spam, and that email message really was not spam. true positive rate (TPR) Synonym for recall. That is: True positive rate is the y-axis in an ROC curve. \\text{True Positive Rate} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}} False Negative (FN) An example in which the model mistakenly predicted the negative class. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam. False Positive (FP) An example in which the model mistakenly predicted the positive class. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam. false positive rate (FPR) The x-axis in an ROC curve. The false positive rate is defined as follows: \\text{False Positive Rate} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}} ROC (receiver operating characteristic) Curve: A curve of true positive rate vs. false positive rate at different classification thresholds. Accuracy The fraction of predictions that a classification model got right. In multi-class classification, accuracy is defined as follows: \\text{Accuracy} = \\frac{\\text{Correct Predictions}} {\\text{Total Number Of Examples}} In binary classification, accuracy has the following definition: \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Number Of Examples}} Precision A metric for classification models. Precision identifies the frequency with which a model was correct when predicting the positive class. That is: \\text{Precision} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Positives}} Recall A metric for classification models that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is: \\text{Recall} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}} In medical field it is common to use Sensitivity and Specificity, where Sensitivity is same as Recall while Specificity is as follows: \\text{Specificity} = \\frac{\\text{True Negatives}} {\\text{True Negatives} + \\text{False Positives}} Now, sensitivity and specificity are the rows of this matrix. More specifically, if we label TP: Sick people that we correctly diagnosed as sick. TN: Healthy people that we correctly diagnosed as healthy. FP: Healthy people that we incorrectly diagnosed as sick. FN: Sick people that we incorrectly diagnosed as healthy.","title":"Math"},{"location":"math/#z-score-standard-score","text":"Given an observed value x, the Z score finds the number of Standard deviations x is away from the mean. Z = \\frac{x-\\mu}{\\sigma}","title":"Z score (Standard Score)"},{"location":"math/#t-distribution","text":"The t-Test is best to use when we do not know the population standard deviation. Instead we use the sample standard deviation. The number of degrees of freedom(df) is the number of values that are free to vary (n-1). S=\\frac{\\sum(x_i-\\overline{x})^2}{n-1} t = \\frac{\\overline{x}-\\mu}{SE} T-tests are also great for testing two sample means (i.e. paired t-tests), we modify the formula to become: SD = \\sqrt{S_1^2 + S_2^2} SE = \\sqrt{\\frac{S_1^2}{n1} + \\frac{S_2^2}{n2}} t = \\frac{(\\overline{x}_2-\\overline{x}_1)-(\\mu_2-\\mu_1)}{\\frac{\\sqrt(s_1^2+s_2^2)}{n}}","title":"T distribution"},{"location":"math/#cohens-d","text":"It measures the effect size of the strength of a phenomenon. Cohen\u2019s d gives us the distance between means in standardized units. $\\overline{x} = \\overline{x_1} - \\overline{x_2}$ d = \\frac{\\overline{x}-\\mu}{S}","title":"Cohen's d"},{"location":"math/#margin-of-error-me","text":"ME = t_{critical} * SE","title":"Margin of Error (ME)"},{"location":"math/#confidence-interval-ci","text":"CI = \\overline{x} \\pm ME","title":"Confidence Interval (CI)"},{"location":"math/#evaluation-metrics","text":"Positive Class In binary classification, the two possible classes are labeled as positive and negative. The positive outcome is the thing we're testing for. (Admittedly, we're simultaneously testing for both outcomes, but play along.) For example, the positive class in a medical test might be \"tumor.\" The positive class in an email classifier might be \"spam\". Negative Class In binary classification, one class is termed positive and the other is termed negative. The positive class is the thing we're looking for and the negative class is the other possibility. For example, the negative class in a medical test might be \"not tumor.\" The negative class in an email classifier might be \"not spam\". True Positive (TP) An example in which the model correctly predicted the positive class. For example, the model inferred that a particular email message was spam, and that email message really was spam. True Negative (TN) An example in which the model correctly predicted the negative class. For example, the model inferred that a particular email message was not spam, and that email message really was not spam. true positive rate (TPR) Synonym for recall. That is: True positive rate is the y-axis in an ROC curve. \\text{True Positive Rate} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}} False Negative (FN) An example in which the model mistakenly predicted the negative class. For example, the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam. False Positive (FP) An example in which the model mistakenly predicted the positive class. For example, the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam. false positive rate (FPR) The x-axis in an ROC curve. The false positive rate is defined as follows: \\text{False Positive Rate} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}} ROC (receiver operating characteristic) Curve: A curve of true positive rate vs. false positive rate at different classification thresholds. Accuracy The fraction of predictions that a classification model got right. In multi-class classification, accuracy is defined as follows: \\text{Accuracy} = \\frac{\\text{Correct Predictions}} {\\text{Total Number Of Examples}} In binary classification, accuracy has the following definition: \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Number Of Examples}} Precision A metric for classification models. Precision identifies the frequency with which a model was correct when predicting the positive class. That is: \\text{Precision} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Positives}} Recall A metric for classification models that answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is: \\text{Recall} = \\frac{\\text{True Positives}} {\\text{True Positives} + \\text{False Negatives}} In medical field it is common to use Sensitivity and Specificity, where Sensitivity is same as Recall while Specificity is as follows: \\text{Specificity} = \\frac{\\text{True Negatives}} {\\text{True Negatives} + \\text{False Positives}} Now, sensitivity and specificity are the rows of this matrix. More specifically, if we label TP: Sick people that we correctly diagnosed as sick. TN: Healthy people that we correctly diagnosed as healthy. FP: Healthy people that we incorrectly diagnosed as sick. FN: Sick people that we incorrectly diagnosed as healthy.","title":"Evaluation Metrics"},{"location":"ml_workflow/","text":"An orchestrated and repeatable pattern which systematically transforms and processes information to create prediction solutions. Asking the right question Preparing data Selecting the algorithm Training the model Testing the model Solution Statement Use the Machine Learning Workflow to process and transform Pima Indian data to create a prediction model. This model must predict which peopel are likely to develop diabetes with 70% or greater accuracy Tidy Data Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column each observation is a row each type of observational unit is a table Selecting the algorithm We will use our problem knowledge to help us decide the algorithm to use. we will discuss - The role of the algorithm in machine learning process, select our initial algorithm by utilizing the requirements identified in the solution statement as a guide, and discuss at a high level the characteristics of some specific algorithms. Finally select one algorithm to be our initial algorithm, in machine learning we often cycle through the workflow. In our search to find the best solution, it is likely we will need to train and evaluate multiple algorithms. Let's review how the algorithm is involved in the process. When the training function (often named fit in scikit-learn) is called, the algorithm executes its logic and processes the training data. Using the algorithm's logic, the data in analyzed. This analysis evaluates the data with respect to mathematical model and logic associated with the algorithm. The algorithm uses the results of this analysis to adjust internal parameters to produce a model that has been trained to best fit the features in the training data and produce the associated class result. This best fit is defined by evaluating a function specific to the particular algorithm. The fit parameters are stored and the model is now said to be trained. Later, the trained model is called the prediction function (often named predict in scikit-learn). When this prediction function is called, real data is passed to the trained model. Using only the features in the data, the trained model uses its code and parameter values set during training to evaluate the data's features and predict the class result, diabetes or not for this new data. Decision factors to select our initial algorithm: We will use our solution statement and knowledge of the workflow to help guide us in the evaluation of these factors. what Learning Type they support the Result Type the algorithm predicts the Complexity of the algorithm whether the algorithm is Basic or Enhanced Each algorithm has a set of problems with which it works best. One way to divide them is to look at the type of Learning they support. Reading the statement, we see that our solution is about prediction. Prediction means supervised machine learning, so we can eliminate all algorithms that do not support it. Let's see how Result Type can help. Prediction results can be divided into two categories: Regression (Continuous values) Classification (Discrete values) Based on the Statment, the algorithm must support Binary classification. Since this is our initial algorithm, let's stick to the basic algorithms. Selecting Our Initial Algorithm Candidate Algorithms: Naive Bayes Logistic Regression Decision Tree More complex algoritms use these as building blocks. Naive Bayes Algorithm The Naive Bayes algorithm is based on Bayes' Theorem. This theorem calculates a probability of a diabetes by looking at the likelihood of diabetes based on previous data combined with probability of diabetes on nearby feature values. In other words, so how often does the person having high blood pressure correlate to diabetes? It makes the naive assumption that all of the features we pass in are independent of each other and equally impact the result. This assumption that every featuer is independent to the others allows for fast conversions and therefore requires a small amount of data to train. Logistic Regression Algorithm The Logistic Regression algorithm has a somewhat confusing name. In Statistics, Regression often implies continuous values. But Logistics Regression returns a binary result. The algorithm measures the relationship of each feature and weights them based on their impact on the result. The resultant value is mapped against a curve with two values, one and zero, which is equivalent to diabetes or no diabetes. Decision Tree Algorithm The Decision Tree algorithm can be nicely visualized. The algorithm uses a binary tree structure with each node making a decision based upon the values of the feature. At each node, the feature value causes us to go down one path or another. A lot of data may be required to find the value which defines taking one path or another. As we see decision trees have the advantage of having tools available to produce a picture of the tree. This makes it easy to follow along and visualize how the trained model works. Training the Model Letting specific data teach a machine learning algorithm to create a specific prediction model. Why retrain? Retraining will ensure that our model can take advantage of the new data to make better predictions. And also verify the algorithm can still create a high-performance model with the new data. We will compare our model prediction with actual prediction or actual labels that are associated with training data and use this as feeback to tweak our model parameters, this is the loss function or cost function and its primary purpose is to improve our model parameters and build stronger model. Performance Improvement Options Adjust current algorithm Get more data or improve data Improve training Switch algorithms","title":"Workflow"},{"location":"ml_workflow/#solution-statement","text":"Use the Machine Learning Workflow to process and transform Pima Indian data to create a prediction model. This model must predict which peopel are likely to develop diabetes with 70% or greater accuracy","title":"Solution Statement"},{"location":"ml_workflow/#tidy-data","text":"Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column each observation is a row each type of observational unit is a table","title":"Tidy Data"},{"location":"ml_workflow/#selecting-the-algorithm","text":"We will use our problem knowledge to help us decide the algorithm to use. we will discuss - The role of the algorithm in machine learning process, select our initial algorithm by utilizing the requirements identified in the solution statement as a guide, and discuss at a high level the characteristics of some specific algorithms. Finally select one algorithm to be our initial algorithm, in machine learning we often cycle through the workflow. In our search to find the best solution, it is likely we will need to train and evaluate multiple algorithms. Let's review how the algorithm is involved in the process. When the training function (often named fit in scikit-learn) is called, the algorithm executes its logic and processes the training data. Using the algorithm's logic, the data in analyzed. This analysis evaluates the data with respect to mathematical model and logic associated with the algorithm. The algorithm uses the results of this analysis to adjust internal parameters to produce a model that has been trained to best fit the features in the training data and produce the associated class result. This best fit is defined by evaluating a function specific to the particular algorithm. The fit parameters are stored and the model is now said to be trained. Later, the trained model is called the prediction function (often named predict in scikit-learn). When this prediction function is called, real data is passed to the trained model. Using only the features in the data, the trained model uses its code and parameter values set during training to evaluate the data's features and predict the class result, diabetes or not for this new data. Decision factors to select our initial algorithm: We will use our solution statement and knowledge of the workflow to help guide us in the evaluation of these factors. what Learning Type they support the Result Type the algorithm predicts the Complexity of the algorithm whether the algorithm is Basic or Enhanced Each algorithm has a set of problems with which it works best. One way to divide them is to look at the type of Learning they support. Reading the statement, we see that our solution is about prediction. Prediction means supervised machine learning, so we can eliminate all algorithms that do not support it. Let's see how Result Type can help. Prediction results can be divided into two categories: Regression (Continuous values) Classification (Discrete values) Based on the Statment, the algorithm must support Binary classification. Since this is our initial algorithm, let's stick to the basic algorithms.","title":"Selecting the algorithm"},{"location":"ml_workflow/#selecting-our-initial-algorithm","text":"Candidate Algorithms: Naive Bayes Logistic Regression Decision Tree More complex algoritms use these as building blocks.","title":"Selecting Our Initial Algorithm"},{"location":"ml_workflow/#naive-bayes-algorithm","text":"The Naive Bayes algorithm is based on Bayes' Theorem. This theorem calculates a probability of a diabetes by looking at the likelihood of diabetes based on previous data combined with probability of diabetes on nearby feature values. In other words, so how often does the person having high blood pressure correlate to diabetes? It makes the naive assumption that all of the features we pass in are independent of each other and equally impact the result. This assumption that every featuer is independent to the others allows for fast conversions and therefore requires a small amount of data to train.","title":"Naive Bayes Algorithm"},{"location":"ml_workflow/#logistic-regression-algorithm","text":"The Logistic Regression algorithm has a somewhat confusing name. In Statistics, Regression often implies continuous values. But Logistics Regression returns a binary result. The algorithm measures the relationship of each feature and weights them based on their impact on the result. The resultant value is mapped against a curve with two values, one and zero, which is equivalent to diabetes or no diabetes.","title":"Logistic Regression Algorithm"},{"location":"ml_workflow/#decision-tree-algorithm","text":"The Decision Tree algorithm can be nicely visualized. The algorithm uses a binary tree structure with each node making a decision based upon the values of the feature. At each node, the feature value causes us to go down one path or another. A lot of data may be required to find the value which defines taking one path or another. As we see decision trees have the advantage of having tools available to produce a picture of the tree. This makes it easy to follow along and visualize how the trained model works.","title":"Decision Tree Algorithm"},{"location":"ml_workflow/#training-the-model","text":"Letting specific data teach a machine learning algorithm to create a specific prediction model. Why retrain? Retraining will ensure that our model can take advantage of the new data to make better predictions. And also verify the algorithm can still create a high-performance model with the new data. We will compare our model prediction with actual prediction or actual labels that are associated with training data and use this as feeback to tweak our model parameters, this is the loss function or cost function and its primary purpose is to improve our model parameters and build stronger model.","title":"Training the Model"},{"location":"ml_workflow/#performance-improvement-options","text":"Adjust current algorithm Get more data or improve data Improve training Switch algorithms","title":"Performance Improvement Options"},{"location":"neural_network/","text":"","title":"Neural Network"},{"location":"reinforcement_learning/","text":"Reinforcement Learning Reinforcement learning is learning what to do\u2014how to map situations to actions\u2014so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. A policy defines the learning agent\u2019s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states. A reward signal defines the goal of a reinforcement learning problem. On each time step, the environment sends a single number called the reward to the reinforcement learning agent. The reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Model mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave. Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners. Reinforcement learning uses the formal framework of Markov decision processes to define the interaction between a learning agent and its environment in terms of states, actions, and rewards. We will learn problem formulation with finite Markov Decision Processes and its main ideas including Bellman equations and value functions. Three fundamental classes of methods for solving finite Markov decision problems: Dynamic programming Monte Carlo methods Temporal difference learning The methods also differ in several ways with respect to their efficiency and speed of convergence. Dynamic programming methods are well developed mathematically, but require a complete and accurate model of the environment. Monte Carlo methods don\u2019t require a model and are conceptually simple, but are not well suited for step-by-step incremental computation. Temporal-difference methods require no model and are fully incremental, but are more complex to analyze. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. The learner and decision maker is called the agent . The thing it interacts with, comprising everything outside the agent, is called the environment . Thus RL framework is characterized by an agent learning to interact with its environment . At each time step, the agent receives the environment's state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state. All agents have the goal to maximize expected cumulative reward , or the expected sum of rewards attained over all time steps. The state must include information about all aspects of the past agent\u2013environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property . Episodic vs. Continuing Tasks A task is an instance of the reinforcement learning (RL) problem. Continuing tasks are tasks that continue forever, without end. Episodic tasks are tasks with a well-defined starting and ending point. In this case, we refer to a complete sequence of interaction, from start to finish, as an episode. Episodic tasks come to an end whenever the agent reaches a terminal state. The Reward Hypothesis Reward Hypothesis : All goals can be framed as the maximization of (expected) cumulative reward. Cumulative Reward The return at time step t is G_t := R_{t+1} + R_{t+2} + R_{t+3} + \\ldots The agent selects actions with the goal of maximizing expected (discounted) return. Discounted Return Discounting: The agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. The discounted return at time step t is G_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots G_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\ldots) G_t = R_{t+1} + \\gamma G_{t+1} G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} The discount rate \\gamma is something that you set, to refine the goal that you have the agent. It must satisfy 0 <= \\gamma <= 1 If \\gamma=0 , the agent only cares about the most immediate reward. If \\gamma=1 , the return is not discounted. For larger values of \\gamma , the agent cares more about the distant future. Smaller values of \\gamma result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward. MDPs and One-Step Dynamics The state space \\mathcal{S} is the set of all (nonterminal) states. In episodic tasks, we use \\mathcal{S}^+ to refer to the set of all states, including terminal states. The action space \\mathcal{A} is the set of possible actions. (Alternatively, \\mathcal{A(s)} refers to the set of possible actions available in state s \\in (\\mathcal{S} The one-step dynamics of the environment determine how the environment decides the state and reward at every time step. The dynamics can be defined by specifying p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a) \\text{ for each possible } s', r, s, \\text{and } a A (finite) Markov Decision Process (MDP) is defined by: a (finite) set of states \\mathcal{S} (or \\mathcal{S}^+ , in the case of an episodic task) a (finite) set of actions \\mathcal{A} a set of rewards \\mathcal{R} the one-step dynamics of the environment the discount rate \\gamma \\in [0,1] Bellman Equations In this gridworld example, once the agent selects an action, it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution). In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state. Alexis mentioned that for a general MDP, we have to instead work in terms of an expectation, since it's not often the case that the immediate reward and next state can be predicted with certainty. Indeed, we saw in an earlier lesson that the reward and next state are chosen according to the one-step dynamics of the MDP. In this case, where the reward r and next state s\u2032 are drawn from a (conditional) probability distribution p(s',r|s,a) , the Bellman Expectation Equation (for v_\\pi) expresses the value of any state s in terms of the expected immediate reward and the expected value of the next state: v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s] Calculating the Expectation In the event that the agent's policy \u03c0 is deterministic , the agent selects action \u03c0(s) when in state s , and the Bellman Expectation Equation can be rewritten as the sum over two variables ( s\u2032 and r ): v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s')) In this case, we multiply the sum of the reward and discounted value of the next state (r+\\gamma v_\\pi(s')) by its corresponding probability p(s',r|s,\\pi(s)) and sum over all possibilities to yield the expected value. If the agent's policy \\pi is stochastic , the agent selects action a with probability \\pi(a\u2223s) when in state a , and the Bellman Expectation Equation can be rewritten as the sum over three variables (s\u2032, r, a) : v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s')) In this case, we multiply the sum of the reward and discounted value of the next state (r+\\gamma v_\\pi(s')) by its corresponding probability \u03c0(a\u2223s)p(s\u2032,r\u2223s,a) and sum over all possibilities to yield the expected value. Policies A deterministic policy is a mapping \\pi: \\mathcal{S}\\to\\mathcal{A} . For each state s \\in S , it yields the action (a \\in A) that the agent will choose while in state s . A stochastic policy is a mapping \\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1] . For each state s \\in S and action a \\in A , it yields the probability \\pi(a|s) that the agent chooses action a while in state s . State-Value Functions The state-value function for a policy \\pi is denoted v_\\pi . For each state s \\in S , it yields the expected return if the agent starts in state s and then uses the policy to choose its actions for all time steps. That is, v_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_t | S_t=s] We refer to v_\\pi(s) as the value of state s under policy \\pi . The notation \\mathbb{E}_\\pi[\\cdot] denotes the expected value of a random variable given that the agent follows policy \\pi , and t is any time step. Optimality A policy \\pi\u2032 is defined to be better than or equal to a policy \\pi if and only if v_{\\pi'}(s) \\geq v_\\pi(s) for all s \\in S . An optimal policy \\pi_* satisfies \\pi_*\\ge\\pi for all policies \u03c0. An optimal policy is guaranteed to exist but may not be unique. All optimal policies have the same state-value function v_* called the optimal state-value function . Action-Value Functions The action-value function for a policy \\pi is denoted q_\\pi . For each state s \\in S and action a \\in A , it yields the expected return if the agent starts in state s , takes action a , and then follows the policy for all future time steps. That is, q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a] We refer q_\\pi(s,a) as the value of taking action a in state s under a policy \\pi (or alternatively as the value of the state-action pair s , a ). All optimal policies have the same action-value function q_* , called the optimal action-value function . Optimal Policies Once the agent determines the optimal action-value function q_* , it can quickly obtain an optimal policy \\pi_* by setting: \\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#reinforcement-learning","text":"Reinforcement learning is learning what to do\u2014how to map situations to actions\u2014so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. A policy defines the learning agent\u2019s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states. A reward signal defines the goal of a reinforcement learning problem. On each time step, the environment sends a single number called the reward to the reinforcement learning agent. The reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Model mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave. Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners. Reinforcement learning uses the formal framework of Markov decision processes to define the interaction between a learning agent and its environment in terms of states, actions, and rewards. We will learn problem formulation with finite Markov Decision Processes and its main ideas including Bellman equations and value functions. Three fundamental classes of methods for solving finite Markov decision problems: Dynamic programming Monte Carlo methods Temporal difference learning The methods also differ in several ways with respect to their efficiency and speed of convergence. Dynamic programming methods are well developed mathematically, but require a complete and accurate model of the environment. Monte Carlo methods don\u2019t require a model and are conceptually simple, but are not well suited for step-by-step incremental computation. Temporal-difference methods require no model and are fully incremental, but are more complex to analyze. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. The learner and decision maker is called the agent . The thing it interacts with, comprising everything outside the agent, is called the environment . Thus RL framework is characterized by an agent learning to interact with its environment . At each time step, the agent receives the environment's state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state. All agents have the goal to maximize expected cumulative reward , or the expected sum of rewards attained over all time steps. The state must include information about all aspects of the past agent\u2013environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property .","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#episodic-vs-continuing-tasks","text":"A task is an instance of the reinforcement learning (RL) problem. Continuing tasks are tasks that continue forever, without end. Episodic tasks are tasks with a well-defined starting and ending point. In this case, we refer to a complete sequence of interaction, from start to finish, as an episode. Episodic tasks come to an end whenever the agent reaches a terminal state.","title":"Episodic vs. Continuing Tasks"},{"location":"reinforcement_learning/#the-reward-hypothesis","text":"Reward Hypothesis : All goals can be framed as the maximization of (expected) cumulative reward.","title":"The Reward Hypothesis"},{"location":"reinforcement_learning/#cumulative-reward","text":"The return at time step t is G_t := R_{t+1} + R_{t+2} + R_{t+3} + \\ldots The agent selects actions with the goal of maximizing expected (discounted) return.","title":"Cumulative Reward"},{"location":"reinforcement_learning/#discounted-return","text":"Discounting: The agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. The discounted return at time step t is G_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots G_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\ldots) G_t = R_{t+1} + \\gamma G_{t+1} G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} The discount rate \\gamma is something that you set, to refine the goal that you have the agent. It must satisfy 0 <= \\gamma <= 1 If \\gamma=0 , the agent only cares about the most immediate reward. If \\gamma=1 , the return is not discounted. For larger values of \\gamma , the agent cares more about the distant future. Smaller values of \\gamma result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.","title":"Discounted Return"},{"location":"reinforcement_learning/#mdps-and-one-step-dynamics","text":"The state space \\mathcal{S} is the set of all (nonterminal) states. In episodic tasks, we use \\mathcal{S}^+ to refer to the set of all states, including terminal states. The action space \\mathcal{A} is the set of possible actions. (Alternatively, \\mathcal{A(s)} refers to the set of possible actions available in state s \\in (\\mathcal{S} The one-step dynamics of the environment determine how the environment decides the state and reward at every time step. The dynamics can be defined by specifying p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a) \\text{ for each possible } s', r, s, \\text{and } a A (finite) Markov Decision Process (MDP) is defined by: a (finite) set of states \\mathcal{S} (or \\mathcal{S}^+ , in the case of an episodic task) a (finite) set of actions \\mathcal{A} a set of rewards \\mathcal{R} the one-step dynamics of the environment the discount rate \\gamma \\in [0,1]","title":"MDPs and One-Step Dynamics"},{"location":"reinforcement_learning/#bellman-equations","text":"In this gridworld example, once the agent selects an action, it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution). In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state. Alexis mentioned that for a general MDP, we have to instead work in terms of an expectation, since it's not often the case that the immediate reward and next state can be predicted with certainty. Indeed, we saw in an earlier lesson that the reward and next state are chosen according to the one-step dynamics of the MDP. In this case, where the reward r and next state s\u2032 are drawn from a (conditional) probability distribution p(s',r|s,a) , the Bellman Expectation Equation (for v_\\pi) expresses the value of any state s in terms of the expected immediate reward and the expected value of the next state: v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]","title":"Bellman Equations"},{"location":"reinforcement_learning/#calculating-the-expectation","text":"In the event that the agent's policy \u03c0 is deterministic , the agent selects action \u03c0(s) when in state s , and the Bellman Expectation Equation can be rewritten as the sum over two variables ( s\u2032 and r ): v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s')) In this case, we multiply the sum of the reward and discounted value of the next state (r+\\gamma v_\\pi(s')) by its corresponding probability p(s',r|s,\\pi(s)) and sum over all possibilities to yield the expected value. If the agent's policy \\pi is stochastic , the agent selects action a with probability \\pi(a\u2223s) when in state a , and the Bellman Expectation Equation can be rewritten as the sum over three variables (s\u2032, r, a) : v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s')) In this case, we multiply the sum of the reward and discounted value of the next state (r+\\gamma v_\\pi(s')) by its corresponding probability \u03c0(a\u2223s)p(s\u2032,r\u2223s,a) and sum over all possibilities to yield the expected value.","title":"Calculating the Expectation"},{"location":"reinforcement_learning/#policies","text":"A deterministic policy is a mapping \\pi: \\mathcal{S}\\to\\mathcal{A} . For each state s \\in S , it yields the action (a \\in A) that the agent will choose while in state s . A stochastic policy is a mapping \\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1] . For each state s \\in S and action a \\in A , it yields the probability \\pi(a|s) that the agent chooses action a while in state s .","title":"Policies"},{"location":"reinforcement_learning/#state-value-functions","text":"The state-value function for a policy \\pi is denoted v_\\pi . For each state s \\in S , it yields the expected return if the agent starts in state s and then uses the policy to choose its actions for all time steps. That is, v_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_t | S_t=s] We refer to v_\\pi(s) as the value of state s under policy \\pi . The notation \\mathbb{E}_\\pi[\\cdot] denotes the expected value of a random variable given that the agent follows policy \\pi , and t is any time step.","title":"State-Value Functions"},{"location":"reinforcement_learning/#optimality","text":"A policy \\pi\u2032 is defined to be better than or equal to a policy \\pi if and only if v_{\\pi'}(s) \\geq v_\\pi(s) for all s \\in S . An optimal policy \\pi_* satisfies \\pi_*\\ge\\pi for all policies \u03c0. An optimal policy is guaranteed to exist but may not be unique. All optimal policies have the same state-value function v_* called the optimal state-value function .","title":"Optimality"},{"location":"reinforcement_learning/#action-value-functions","text":"The action-value function for a policy \\pi is denoted q_\\pi . For each state s \\in S and action a \\in A , it yields the expected return if the agent starts in state s , takes action a , and then follows the policy for all future time steps. That is, q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a] We refer q_\\pi(s,a) as the value of taking action a in state s under a policy \\pi (or alternatively as the value of the state-action pair s , a ). All optimal policies have the same action-value function q_* , called the optimal action-value function .","title":"Action-Value Functions"},{"location":"reinforcement_learning/#optimal-policies","text":"Once the agent determines the optimal action-value function q_* , it can quickly obtain an optimal policy \\pi_* by setting: \\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)","title":"Optimal Policies"},{"location":"source/","text":"DS & Model Report Playground NumPy Pandas DataFrame Stroop Effect Practice Problem Data Analysis Process Assessing Cleaning Plotting Conclusion Communicate Case Study Wine Rating Case Study Fuel Economy Fuel Economy - Conclusion Fuel Economy - Visuals Fuel Economy - Merging Data Investigate Dataset Titanic Movies ML Nanodegree Predict House Price Finding Donors Customer Segments Decision Tree Titanic Survival Exploration Naive Bayes Email Spam or Ham Random Forest & Logistic Regression Predict Diabetes Categorical and Numeric Data Text Feature Extraction Image Feature Extraction Lasso & Ridge Regression Vehicle Price SVR Mileage SVM Document Classification Image Classification Gradient Boost Regression / Decision Tree Automobile Price Mean Shift Clustering Titanic Survivor PCA Wine Rating NLTK Getting Started with NLTK Accessing Text Corpora and Lexical Resources Spark Getting Started with Spark 2 London Crime Data Soccer Data Miscellaneous Operations Introducing SparkSQL Airline Data Inferred And Explicit Schemas Windowing Functions Deep Learning Python Basics With Numpy Logistic Regression with a Neural Network mindset Neural Network with one hidden layer Building Deep Neural Network Deep Neural Network Application Gradient Descent XOR Network Student Admission(Keras) IMDB Review(Keras) Convolution Layer Dimension MNIST-Identify Digit Improving Deep Neural Networks Initialization Regularization Gradient_Checking Optimization_methods Tensorflow","title":"Source"},{"location":"tensorflow/","text":"It is an open source library for numerical computation using data flow graphs. Computation Graph The computation graph is the way TensorFlow represents any model in the real world. In the world of TensorFlow, everything can be modeled as a graph. The nodes in a graph are the computations or operators that act upon data, and the edges in a graph is basically the tensors of the data that are transformed by flowing through these nodes. Data within the computation graph are called tensors . These are nothing but n-dimensional matrices used to represent massive data sets. TensorFlow graph is a directed-acyclic graph, every edge in the graph has an arrow, i.e it points towards a certain node - this is the direction in which data flows, these edges point forward towards a result. The computational nodes within this graph depend on other nodes to forward a data before it can perform its computation. One node can send its output to multiple nodes or it can receive inputs from multiple nodes; In such a case - the node can perform its computation only after all its inputs are available. Unrolling the Graph We model cyclical dependencies by unrolling the graph. The output of the last node is fed into a copy of this graph. Instead of feeding the output into the first node of the same graph, you feed it into the first node of this copy. How much you unroll your TensorFlow graph depends on the number of iterations you want to run. Even though TensorFlow models the world as a directed-acyclic graph, when you run multiple iterations, it'll automatically unroll graphs to model cyclic dependencies, which are very common in machine learning. Tensor The cental unit of data in TensorFlow. A tensor consists of a set of primitive values shaped into an array of any number of dimensions. In TensorFlow, data isn\u2019t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. import tensorflow as tf # Create TensorFlow object called hello_constant hello_constant = tf.constant('Hello World!') with tf.Session() as sess: # Run the tf.constant operation in the session output = sess.run(hello_constant) print(output) Session TensorFlow\u2019s api is built around the idea of a computational graph, a way of visualizing a mathematical process. TensorFlow consists of the following two components: a graph protocol buffer a runtime that executes the (distributed) graph These two components are analogous to Python code and the Python interpreter. Just as the Python interpreter is implemented on multiple hardware platforms to run Python code, TensorFlow can run the graph on multiple hardware platforms, including CPU, GPU, and TPU A Quick Look at the tf.estimator API import tensorflow as tf # Set up a linear classifier. classifier = tf.estimator.LinearClassifier(feature_columns) # Train the model on some example data. classifier.train(input_fn=train_input_fn, steps=2000) # Use it to predict. predictions = classifier.predict(input_fn=predict_input_fn) Hyperparameters Steps , which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model's weights once. Batch Size , which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1. total\\,number\\,of\\,trained\\,examples = batch\\,size * steps","title":"TensorFlow"},{"location":"tensorflow/#computation-graph","text":"The computation graph is the way TensorFlow represents any model in the real world. In the world of TensorFlow, everything can be modeled as a graph. The nodes in a graph are the computations or operators that act upon data, and the edges in a graph is basically the tensors of the data that are transformed by flowing through these nodes. Data within the computation graph are called tensors . These are nothing but n-dimensional matrices used to represent massive data sets. TensorFlow graph is a directed-acyclic graph, every edge in the graph has an arrow, i.e it points towards a certain node - this is the direction in which data flows, these edges point forward towards a result. The computational nodes within this graph depend on other nodes to forward a data before it can perform its computation. One node can send its output to multiple nodes or it can receive inputs from multiple nodes; In such a case - the node can perform its computation only after all its inputs are available.","title":"Computation Graph"},{"location":"tensorflow/#unrolling-the-graph","text":"We model cyclical dependencies by unrolling the graph. The output of the last node is fed into a copy of this graph. Instead of feeding the output into the first node of the same graph, you feed it into the first node of this copy. How much you unroll your TensorFlow graph depends on the number of iterations you want to run. Even though TensorFlow models the world as a directed-acyclic graph, when you run multiple iterations, it'll automatically unroll graphs to model cyclic dependencies, which are very common in machine learning.","title":"Unrolling the Graph"},{"location":"tensorflow/#tensor","text":"The cental unit of data in TensorFlow. A tensor consists of a set of primitive values shaped into an array of any number of dimensions. In TensorFlow, data isn\u2019t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. import tensorflow as tf # Create TensorFlow object called hello_constant hello_constant = tf.constant('Hello World!') with tf.Session() as sess: # Run the tf.constant operation in the session output = sess.run(hello_constant) print(output)","title":"Tensor"},{"location":"tensorflow/#session","text":"TensorFlow\u2019s api is built around the idea of a computational graph, a way of visualizing a mathematical process. TensorFlow consists of the following two components: a graph protocol buffer a runtime that executes the (distributed) graph These two components are analogous to Python code and the Python interpreter. Just as the Python interpreter is implemented on multiple hardware platforms to run Python code, TensorFlow can run the graph on multiple hardware platforms, including CPU, GPU, and TPU","title":"Session"},{"location":"tensorflow/#a-quick-look-at-the-tfestimator-api","text":"import tensorflow as tf # Set up a linear classifier. classifier = tf.estimator.LinearClassifier(feature_columns) # Train the model on some example data. classifier.train(input_fn=train_input_fn, steps=2000) # Use it to predict. predictions = classifier.predict(input_fn=predict_input_fn)","title":"A Quick Look at the tf.estimator API"},{"location":"tensorflow/#hyperparameters","text":"Steps , which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model's weights once. Batch Size , which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1. total\\,number\\,of\\,trained\\,examples = batch\\,size * steps","title":"Hyperparameters"},{"location":"terminology/","text":"Model The representation of what an ML system has learned from the training data. Labels A label is the thing we're predicting\u2014the y variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything. Features A feature is an input variable\u2014the x variable in simple linear regression. x_1,x_2,x_3,....x_N synthetic feature: A feature not present among the input features, but created from one or more of them. Kinds of synthetic features include: Bucketing a continuous feature into range bins. Multiplying (or dividing) one feature value by other feature value(s) or by itself. Creating a feature cross. Features created by normalizing or scaling alone are not considered synthetic features. Scaling A commonly used practice in feature engineering to tame a feature's range of values to match the range of other features in the dataset. For example, suppose that you want all floating-point features in the dataset to have a range of 0 to 1. Given a particular feature's range of 0 to 500, you could scale that feature by dividing each value by 500. Weight A coefficient for a feature in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model. Data Set training set: The subset of the dataset used to train a model. test set: The subset of the dataset that you use to test your model after the model has gone through initial vetting by the validation set. validation set: A subset of the dataset\u2014disjoint from the training set\u2014used in validation. temporal data: Data recorded at different points in time. For example, winter coat sales recorded for each day of the year would be temporal data. stationarity: A property of data in a dataset, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December. static model: A model that is trained offline. Examples An example is a particular instance of data, x. We break examples into two categories: labeled examples unlabeled examples A labeled example includes both feature(s) and the label. That is: {features, label}: (x, y) Use labeled examples to train the model. An unlabeled example contains features but not the label. That is: {features, ?}: (x, ?) Once we've trained our model with labeled examples, we use that model to predict the label on unlabeled examples. Models A model defines the relationship between features and label (and are defined by internal parameters, which are learned). For example, a spam detection model might associate certain features strongly with \"spam\". Let's highlight two phases of a model's life: Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. Inference means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y'). For example, during inference, you can predict medianHouseValue for new unlabeled examples. Loss Loss is the penalty for a bad prediction or how far a model's predictions are from its label. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. L1 Loss Loss function based on the absolute value of the difference between the values that a model is predicting and the actual values of the labels. L1 loss is less sensitive to outliers than L2 loss. L2 Loss This function calculates the squares of the difference between a model's predicted value for a labeled example and the actual value of the label. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than L1 loss. (used in linear regression) \\begin{align} L_2\\text{ }Loss & = (observation - prediction)^2 \\\\\\\\ & = (y - y')^2 \\\\\\\\ & = \\sum_{(x,y)\\in D}(y-prediction(x))^2 \\end{align} \\sum \\text{:We're summing over all examples in the training set.} D \\text{: is a data set containing many labeled examples, which are (x,y) pairs.} \\text{ Sometimes useful to average over all examples, } \\text{so divide out by} \\frac{1}{\\|D\\|} Selection Bias Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. The following forms of selection bias exist: coverage bias: The population represented in the dataset does not match the population that the ML model is making predictions about. sampling bias: Data is not collected randomly from the target group. non-response bias (also called participation bias): Users from certain groups opt-out of surveys at different rates than users from other groups. Gradient Gradient The vector of partial derivatives with respect to all of the independent variables. In machine learning, the gradient is the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent. (y - y')^2 The derivative of above with respect to the weights and biases tells us how loss changes for a given example Gradient Descent A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss. Exploding Gradient Problem The tendency for gradients in a deep neural networks (especially recurrent neural networks) to become surprisingly steep (high). Steep gradients result in very large updates to the weights of each node in a deep neural network. Models suffering from the exploding gradient problem become difficult or impossible to train. Gradient clipping can mitigate this problem. Gradient Clipping A commonly used mechanism to mitigate the exploding gradient problem by artificially limiting (clipping) the maximum value of gradients when using gradient descent to train a model. Convergence Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence. Underfitting Producing a model with poor predictive ability because the model hasn't captured the complexity of the training data. Many problems can cause underfitting, including: Training on the wrong set of features. Training for too few epochs or at too low a learning rate. Training with too high a regularization rate. Providing too few hidden layers in a deep neural network. Bias The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). Overfitting The algorithm analyses the data and trains itself to create a high mathematical order model based on the data. y = x_1 + w_2x_2^3 + w_3x_3^8 These high-order terms let this equation define a precise decision boundary between the positive and negative values, but as a result, the training process has created a model that works very well on training data but poorly when asked to predict values based on data it has not trained - this is the class overfit problem and is an issue that must be handled to create machine learning models that work well not only on the training data but also on real-world data. Variance The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). Regularization , Cross validation , Ensemble learning of which dropout is a part, are all ways to mitigate overfitting. We add an additional parameter where if the model coefficients get too complex we add a penalty to the objective function. This is the technique that we use in regression. Regularization: The penalty on a model's complexity. Different kinds of regularization include: L1 regularization L2 regularization Dropout regularization L1 is a type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with L2 regularization. L2 is a type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models. Dropout is a form of regularization useful in training neural networks. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. Regularization rate A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate's influence: \\text{minimize(loss function + }\\lambda\\text{(regularization function))} Raising the regularization rate reduces overfitting but may make the model less accurate. Hyperplane A boundary that separates a space into two subspaces. For example, a line is a hyperplane in two dimensions and a plane is a hyperplane in three dimensions. More typically in machine learning, a hyperplane is the boundary separating a high-dimensional space. Kernel Support Vector Machines use hyperplanes to separate positive classes from negative classes, often in a very high-dimensional space. Parameter A variable of a model that the ML system trains on its own. A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data. Often model parameters are estimated using an optimization algorithm, which is a type of efficient search through possible parameter values. They are required by the model when making predictions. They values define the skill of the model on your problem. They are estimated or learned from data. They are often not set manually by the practitioner. They are often saved as part of the learned model. Parameters are key to machine learning algorithms. They are the part of the model that is learned from historical training data. Statistics: In statistics, you may assume a distribution for a variable, such as a Gaussian distribution. Two parameters of the Gaussian distribution are the mean (mu) and the standard deviation (sigma). This holds in machine learning, where these parameters may be estimated from data and used as part of a predictive model. Programming: In programming, you may pass a parameter to a function. In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data. Some examples of model parameters include: The weights in an artificial neural network. The support vectors in a support vector machine. The coefficients in a linear regression or logistic regression. Hyperparameter Hyperparameters are the configuration settings used to tune how the model is trained and it is external to the model, whose value cannot be estimated from data. They are often used in processes to help estimate model parameters. They are often specified by the practitioner. They can often be set using heuristics. They are often tuned for a given predictive modeling problem. We cannot know the best value for a hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error. When a machine learning algorithm is tuned for a specific problem, such as when you are using a grid search or a random search, then you are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions. Hyperparameters are often referred to as model parameters which can make things confusing. A good rule of thumb to overcome this confusion is as follows: If you have to specify a model parameter manually then it is probably a model hyperparameter. Some examples of model hyperparameters include: The learning rate, number of hidden layers in deep learning models (MXNet, TensorFlow, PyTorch) Number of trees in a forest, Maximum depth per tree in Random Forest (sklearn) Regularization coefficient and weight of the l1 norm term in Elastic Net (sklearn) The C and sigma hyperparameters for support vector machines. The k in k-nearest neighbors. Grid Search is an approach to hyperparameter tuning that will methodically build and evaluate model for each combination of algorithm parameters specified in a grid. Learning Rate A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step. HPO Hyperparameter Optimization is the search for the set of hyperparameters that produces the best model performance. Time Series Analysis A subfield of machine learning and statistics that analyzes temporal data. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data. Keras A popular Python machine learning API. Keras runs on several deep learning frameworks, including TensorFlow, where it is made available as tf.keras.","title":"Terminology"},{"location":"terminology/#model","text":"The representation of what an ML system has learned from the training data.","title":"Model"},{"location":"terminology/#labels","text":"A label is the thing we're predicting\u2014the y variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything.","title":"Labels"},{"location":"terminology/#features","text":"A feature is an input variable\u2014the x variable in simple linear regression. x_1,x_2,x_3,....x_N synthetic feature: A feature not present among the input features, but created from one or more of them. Kinds of synthetic features include: Bucketing a continuous feature into range bins. Multiplying (or dividing) one feature value by other feature value(s) or by itself. Creating a feature cross. Features created by normalizing or scaling alone are not considered synthetic features.","title":"Features"},{"location":"terminology/#scaling","text":"A commonly used practice in feature engineering to tame a feature's range of values to match the range of other features in the dataset. For example, suppose that you want all floating-point features in the dataset to have a range of 0 to 1. Given a particular feature's range of 0 to 500, you could scale that feature by dividing each value by 500.","title":"Scaling"},{"location":"terminology/#weight","text":"A coefficient for a feature in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model.","title":"Weight"},{"location":"terminology/#data-set","text":"training set: The subset of the dataset used to train a model. test set: The subset of the dataset that you use to test your model after the model has gone through initial vetting by the validation set. validation set: A subset of the dataset\u2014disjoint from the training set\u2014used in validation. temporal data: Data recorded at different points in time. For example, winter coat sales recorded for each day of the year would be temporal data. stationarity: A property of data in a dataset, in which the data distribution stays constant across one or more dimensions. Most commonly, that dimension is time, meaning that data exhibiting stationarity doesn't change over time. For example, data that exhibits stationarity doesn't change from September to December. static model: A model that is trained offline.","title":"Data Set"},{"location":"terminology/#examples","text":"An example is a particular instance of data, x. We break examples into two categories: labeled examples unlabeled examples A labeled example includes both feature(s) and the label. That is: {features, label}: (x, y) Use labeled examples to train the model. An unlabeled example contains features but not the label. That is: {features, ?}: (x, ?) Once we've trained our model with labeled examples, we use that model to predict the label on unlabeled examples.","title":"Examples"},{"location":"terminology/#models","text":"A model defines the relationship between features and label (and are defined by internal parameters, which are learned). For example, a spam detection model might associate certain features strongly with \"spam\". Let's highlight two phases of a model's life: Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. Inference means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y'). For example, during inference, you can predict medianHouseValue for new unlabeled examples.","title":"Models"},{"location":"terminology/#loss","text":"Loss is the penalty for a bad prediction or how far a model's predictions are from its label. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. L1 Loss Loss function based on the absolute value of the difference between the values that a model is predicting and the actual values of the labels. L1 loss is less sensitive to outliers than L2 loss. L2 Loss This function calculates the squares of the difference between a model's predicted value for a labeled example and the actual value of the label. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than L1 loss. (used in linear regression) \\begin{align} L_2\\text{ }Loss & = (observation - prediction)^2 \\\\\\\\ & = (y - y')^2 \\\\\\\\ & = \\sum_{(x,y)\\in D}(y-prediction(x))^2 \\end{align} \\sum \\text{:We're summing over all examples in the training set.} D \\text{: is a data set containing many labeled examples, which are (x,y) pairs.} \\text{ Sometimes useful to average over all examples, } \\text{so divide out by} \\frac{1}{\\|D\\|}","title":"Loss"},{"location":"terminology/#selection-bias","text":"Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. The following forms of selection bias exist: coverage bias: The population represented in the dataset does not match the population that the ML model is making predictions about. sampling bias: Data is not collected randomly from the target group. non-response bias (also called participation bias): Users from certain groups opt-out of surveys at different rates than users from other groups.","title":"Selection Bias"},{"location":"terminology/#gradient","text":"Gradient The vector of partial derivatives with respect to all of the independent variables. In machine learning, the gradient is the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent. (y - y')^2 The derivative of above with respect to the weights and biases tells us how loss changes for a given example Gradient Descent A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss. Exploding Gradient Problem The tendency for gradients in a deep neural networks (especially recurrent neural networks) to become surprisingly steep (high). Steep gradients result in very large updates to the weights of each node in a deep neural network. Models suffering from the exploding gradient problem become difficult or impossible to train. Gradient clipping can mitigate this problem. Gradient Clipping A commonly used mechanism to mitigate the exploding gradient problem by artificially limiting (clipping) the maximum value of gradients when using gradient descent to train a model.","title":"Gradient"},{"location":"terminology/#convergence","text":"Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.","title":"Convergence"},{"location":"terminology/#underfitting","text":"Producing a model with poor predictive ability because the model hasn't captured the complexity of the training data. Many problems can cause underfitting, including: Training on the wrong set of features. Training for too few epochs or at too low a learning rate. Training with too high a regularization rate. Providing too few hidden layers in a deep neural network.","title":"Underfitting"},{"location":"terminology/#bias","text":"The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).","title":"Bias"},{"location":"terminology/#overfitting","text":"The algorithm analyses the data and trains itself to create a high mathematical order model based on the data. y = x_1 + w_2x_2^3 + w_3x_3^8 These high-order terms let this equation define a precise decision boundary between the positive and negative values, but as a result, the training process has created a model that works very well on training data but poorly when asked to predict values based on data it has not trained - this is the class overfit problem and is an issue that must be handled to create machine learning models that work well not only on the training data but also on real-world data.","title":"Overfitting"},{"location":"terminology/#variance","text":"The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). Regularization , Cross validation , Ensemble learning of which dropout is a part, are all ways to mitigate overfitting. We add an additional parameter where if the model coefficients get too complex we add a penalty to the objective function. This is the technique that we use in regression. Regularization: The penalty on a model's complexity. Different kinds of regularization include: L1 regularization L2 regularization Dropout regularization L1 is a type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with L2 regularization. L2 is a type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models. Dropout is a form of regularization useful in training neural networks. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. Regularization rate A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate's influence: \\text{minimize(loss function + }\\lambda\\text{(regularization function))} Raising the regularization rate reduces overfitting but may make the model less accurate.","title":"Variance"},{"location":"terminology/#hyperplane","text":"A boundary that separates a space into two subspaces. For example, a line is a hyperplane in two dimensions and a plane is a hyperplane in three dimensions. More typically in machine learning, a hyperplane is the boundary separating a high-dimensional space. Kernel Support Vector Machines use hyperplanes to separate positive classes from negative classes, often in a very high-dimensional space.","title":"Hyperplane"},{"location":"terminology/#parameter","text":"A variable of a model that the ML system trains on its own. A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data. Often model parameters are estimated using an optimization algorithm, which is a type of efficient search through possible parameter values. They are required by the model when making predictions. They values define the skill of the model on your problem. They are estimated or learned from data. They are often not set manually by the practitioner. They are often saved as part of the learned model. Parameters are key to machine learning algorithms. They are the part of the model that is learned from historical training data. Statistics: In statistics, you may assume a distribution for a variable, such as a Gaussian distribution. Two parameters of the Gaussian distribution are the mean (mu) and the standard deviation (sigma). This holds in machine learning, where these parameters may be estimated from data and used as part of a predictive model. Programming: In programming, you may pass a parameter to a function. In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data. Some examples of model parameters include: The weights in an artificial neural network. The support vectors in a support vector machine. The coefficients in a linear regression or logistic regression.","title":"Parameter"},{"location":"terminology/#hyperparameter","text":"Hyperparameters are the configuration settings used to tune how the model is trained and it is external to the model, whose value cannot be estimated from data. They are often used in processes to help estimate model parameters. They are often specified by the practitioner. They can often be set using heuristics. They are often tuned for a given predictive modeling problem. We cannot know the best value for a hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error. When a machine learning algorithm is tuned for a specific problem, such as when you are using a grid search or a random search, then you are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions. Hyperparameters are often referred to as model parameters which can make things confusing. A good rule of thumb to overcome this confusion is as follows: If you have to specify a model parameter manually then it is probably a model hyperparameter. Some examples of model hyperparameters include: The learning rate, number of hidden layers in deep learning models (MXNet, TensorFlow, PyTorch) Number of trees in a forest, Maximum depth per tree in Random Forest (sklearn) Regularization coefficient and weight of the l1 norm term in Elastic Net (sklearn) The C and sigma hyperparameters for support vector machines. The k in k-nearest neighbors. Grid Search is an approach to hyperparameter tuning that will methodically build and evaluate model for each combination of algorithm parameters specified in a grid. Learning Rate A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.","title":"Hyperparameter"},{"location":"terminology/#hpo","text":"Hyperparameter Optimization is the search for the set of hyperparameters that produces the best model performance.","title":"HPO"},{"location":"terminology/#time-series-analysis","text":"A subfield of machine learning and statistics that analyzes temporal data. Many types of machine learning problems require time series analysis, including classification, clustering, forecasting, and anomaly detection. For example, you could use time series analysis to forecast the future sales of winter coats by month based on historical sales data.","title":"Time Series Analysis"},{"location":"terminology/#keras","text":"A popular Python machine learning API. Keras runs on several deep learning frameworks, including TensorFlow, where it is made available as tf.keras.","title":"Keras"}]}